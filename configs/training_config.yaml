# configs/training_config.yaml
# Configuration template for model training

# Experiment settings
experiment:
  name: "deepfake_detection_vit"  # Experiment name
  description: "Training ViT model for deepfake detection"
  tags: ["deepfake", "vit", "transformer"]
  seed: 42  # Random seed for reproducibility
  output_dir: "experiments/vit_celebdf"  # Output directory for experiment

# Data settings
data:
  img_size: 224  # Image size
  faceforensics_root: "/path/to/datasets/FaceForensics"  # Path to FaceForensics++ dataset
  celebdf_root: "/path/to/datasets/CelebDF"  # Path to Celeb-DF dataset
  methods:  # Manipulation methods to include (for FaceForensics++)
    - "Deepfakes"
    - "Face2Face"
    - "FaceSwap"
    - "NeuralTextures"
  dataset: "celebdf"  # Dataset to use: faceforensics, celebdf, combined
  train_split: 0.7  # Percentage of data for training
  val_split: 0.15  # Percentage of data for validation
  test_split: 0.15  # Percentage of data for testing

# Model settings
model:
  type: "vit"  # Model type: vit, deit, swin
  ckpt_path: null  # Path to checkpoint for fine-tuning (null for training from scratch)
  params:  # Model parameters
    img_size: 224
    patch_size: 16
    in_channels: 3
    num_classes: 1
    embed_dim: 768
    depth: 12
    num_heads: 12
    mlp_ratio: 4.0
    dropout: 0.1
    attn_dropout: 0.0

# Training settings
training:
  batch_size: 32  # Batch size for training
  num_workers: 4  # Number of data loading workers
  epochs: 50  # Number of training epochs
  optimizer:
    type: "adam"  # Optimizer type: sgd, adam, adamw
    lr: 1.0e-4  # Learning rate
    weight_decay: 1.0e-5  # Weight decay
    momentum: 0.9  # Momentum (for SGD)
    beta1: 0.9  # Beta1 (for Adam/AdamW)
    beta2: 0.999  # Beta2 (for Adam/AdamW)
  scheduler:
    type: "cosine"  # Scheduler type: step, multistep, cosine
    step_size: 10  # Step size (for step scheduler)
    gamma: 0.1  # Gamma (for step and multistep scheduler)
    milestones: [15, 30, 45]  # Milestones (for multistep scheduler)
    warmup_epochs: 5  # Warmup epochs (for cosine scheduler)
    min_lr: 1.0e-6  # Minimum learning rate (for cosine scheduler)
  early_stopping:
    patience: 10  # Number of epochs to wait before early stopping
    min_delta: 0.001  # Minimum change in monitored value to qualify as improvement
    monitor: "val_loss"  # Metric to monitor for early stopping
  loss:
    type: "bce"  # Loss function type: bce, focal, contrastive, triplet
    focal_alpha: 0.25  # Alpha for focal loss
    focal_gamma: 2.0  # Gamma for focal loss
    margin: 1.0  # Margin for contrastive and triplet loss
  grad_clip: 1.0  # Gradient clipping value (null for no clipping)
  mixed_precision: true  # Whether to use mixed precision training
  use_amp: true  # Whether to use PyTorch's Automatic Mixed Precision (AMP)

# Augmentation settings
augmentation:
  horizontal_flip: true  # Whether to use horizontal flip
  rotate: 10  # Rotation angle range
  brightness: 0.2  # Brightness adjustment range
  contrast: 0.2  # Contrast adjustment range
  saturation: 0.2  # Saturation adjustment range
  hue: 0.1  # Hue adjustment range
  jpeg_quality: [70, 90]  # JPEG quality range for simulating compression
  blur: true  # Whether to apply blur
  noise: true  # Whether to add noise

# Evaluation settings
evaluation:
  batch_size: 64  # Batch size for evaluation
  num_workers: 4  # Number of data loading workers
  metrics: ["accuracy", "auc", "eer", "f1", "precision", "recall"]  # Evaluation metrics
  cross_dataset: true  # Whether to perform cross-dataset evaluation

# Logging settings
logging:
  tensorboard: true  # Whether to use TensorBoard
  log_interval: 50  # Logging interval (in batches)
  save_dir: "logs"  # Log directory
  checkpoint_interval: 5  # Checkpoint interval (in epochs)
  save_best_only: true  # Whether to save only the best model
  best_metric: "val_loss"  # Metric to determine the best model
  best_mode: "min"  # Mode for determining the best metric (min/max)
  num_checkpoints: 3  # Number of checkpoints to keep