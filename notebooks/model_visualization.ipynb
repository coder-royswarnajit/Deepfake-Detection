{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff7d3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Deepfake Detection Model Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides visualizations and interpretability tools for the deepfake detection models. It includes:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Model architecture visualization\\n\",\n",
    "    \"2. Attention map visualization\\n\",\n",
    "    \"3. Grad-CAM analysis\\n\",\n",
    "    \"4. Feature visualization\\n\",\n",
    "    \"5. Comparison of model behaviors\\n\",\n",
    "    \"\\n\",\n",
    "    \"These visualizations help understand how the models are making decisions and what features they are focusing on.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import necessary libraries\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"import torch.nn.functional as F\\n\",\n",
    "    \"import torchvision.transforms as transforms\\n\",\n",
    "    \"from torch.utils.data import DataLoader\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add parent directory to path to enable imports from project\\n\",\n",
    "    \"sys.path.append(os.path.abspath('..'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import project modules\\n\",\n",
    "    \"from models.vit.model import ViT\\n\",\n",
    "    \"from models.deit.model import DeiT\\n\",\n",
    "    \"from models.swin.model import SwinTransformer\\n\",\n",
    "    \"from models.model_zoo.model_factory import create_model\\n\",\n",
    "    \"from data.datasets.faceforensics import FaceForensicsDataset\\n\",\n",
    "    \"from data.datasets.celebdf import CelebDFDataset\\n\",\n",
    "    \"from evaluation.visualization.attention_maps import visualize_attention_maps\\n\",\n",
    "    \"from evaluation.visualization.grad_cam import visualize_grad_cam\\n\",\n",
    "    \"from evaluation.visualization.feature_visualization import visualize_features\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plot style\\n\",\n",
    "    \"plt.style.use('fivethirtyeight')\\n\",\n",
    "    \"sns.set(style=\\\"whitegrid\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Pretrained Models\\n\",\n",
    "    \"\\n\",\n",
    "    \"Load pretrained models for visualization. You need to have trained models saved in checkpoint format.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Configure paths - update these to your checkpoint paths\\n\",\n",
    "    \"CHECKPOINT_DIR = \\\"../trained_models\\\"\\n\",\n",
    "    \"VIT_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"vit_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"DEIT_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"deit_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"SWIN_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"swin_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check if checkpoints exist\\n\",\n",
    "    \"vit_exists = os.path.exists(VIT_CHECKPOINT)\\n\",\n",
    "    \"deit_exists = os.path.exists(DEIT_CHECKPOINT)\\n\",\n",
    "    \"swin_exists = os.path.exists(SWIN_CHECKPOINT)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"ViT checkpoint exists: {vit_exists}\\\")\\n\",\n",
    "    \"print(f\\\"DeiT checkpoint exists: {deit_exists}\\\")\\n\",\n",
    "    \"print(f\\\"Swin checkpoint exists: {swin_exists}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set device\\n\",\n",
    "    \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"print(f\\\"Using device: {device}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def load_checkpoint(model, checkpoint_path, device):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load model from checkpoint\\\"\\\"\\\"\\n\",\n",
    "    \"    if not os.path.exists(checkpoint_path):\\n\",\n",
    "    \"        print(f\\\"Checkpoint not found at {checkpoint_path}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        checkpoint = torch.load(checkpoint_path, map_location=device)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Different checkpoint formats\\n\",\n",
    "    \"        if 'model' in checkpoint:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint['model'])\\n\",\n",
    "    \"        elif 'model_state_dict' in checkpoint:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint['model_state_dict'])\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        model = model.to(device)\\n\",\n",
    "    \"        model.eval()  # Set to evaluation mode\\n\",\n",
    "    \"        print(f\\\"Model loaded successfully from {checkpoint_path}\\\")\\n\",\n",
    "    \"        return model\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error loading checkpoint: {e}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize models\\n\",\n",
    "    \"models = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load ViT model\\n\",\n",
    "    \"if vit_exists:\\n\",\n",
    "    \"    vit_model = ViT(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=16,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=768,\\n\",\n",
    "    \"        depth=12,\\n\",\n",
    "    \"        num_heads=12\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    vit_model = load_checkpoint(vit_model, VIT_CHECKPOINT, device)\\n\",\n",
    "    \"    if vit_model is not None:\\n\",\n",
    "    \"        models['vit'] = vit_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load DeiT model\\n\",\n",
    "    \"if deit_exists:\\n\",\n",
    "    \"    deit_model = DeiT(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=16,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=768,\\n\",\n",
    "    \"        depth=12,\\n\",\n",
    "    \"        num_heads=12,\\n\",\n",
    "    \"        distillation=True\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    deit_model = load_checkpoint(deit_model, DEIT_CHECKPOINT, device)\\n\",\n",
    "    \"    if deit_model is not None:\\n\",\n",
    "    \"        models['deit'] = deit_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load Swin model\\n\",\n",
    "    \"if swin_exists:\\n\",\n",
    "    \"    swin_model = SwinTransformer(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=4,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=96,\\n\",\n",
    "    \"        depths=[2, 2, 6, 2],\\n\",\n",
    "    \"        num_heads=[3, 6, 12, 24],\\n\",\n",
    "    \"        window_size=7\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    swin_model = load_checkpoint(swin_model, SWIN_CHECKPOINT, device)\\n\",\n",
    "    \"    if swin_model is not None:\\n\",\n",
    "    \"        models['swin'] = swin_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Loaded {len(models)} models: {list(models.keys())}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Load Sample Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Load some sample images to visualize the model behavior.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Configure dataset paths - update these to your local paths\\n\",\n",
    "    \"FACEFORENSICS_ROOT = \\\"/path/to/datasets/FaceForensics\\\"\\n\",\n",
    "    \"CELEBDF_ROOT = \\\"/path/to/datasets/CelebDF\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Choose one dataset to use for visualization\\n\",\n",
    "    \"DATASET_ROOT = CELEBDF_ROOT  # Change to the dataset you want to use\\n\",\n",
    "    \"DATASET_NAME = \\\"celebdf\\\"     # Change to match your dataset (\\\"faceforensics\\\" or \\\"celebdf\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check if directory exists\\n\",\n",
    "    \"dataset_exists = os.path.exists(DATASET_ROOT)\\n\",\n",
    "    \"print(f\\\"Dataset path exists: {dataset_exists}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define transform\\n\",\n",
    "    \"transform = transforms.Compose([\\n\",\n",
    "    \"    transforms.Resize((224, 224)),\\n\",\n",
    "    \"    transforms.ToTensor(),\\n\",\n",
    "    \"    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load dataset if it exists\\n\",\n",
    "    \"if dataset_exists:\\n\",\n",
    "    \"    if DATASET_NAME == \\\"faceforensics\\\":\\n\",\n",
    "    \"        dataset = FaceForensicsDataset(\\n\",\n",
    "    \"            root=DATASET_ROOT,\\n\",\n",
    "    \"            split=\\\"test\\\",  # Use test split for visualization\\n\",\n",
    "    \"            img_size=224,\\n\",\n",
    "    \"            transform=transform\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    elif DATASET_NAME == \\\"celebdf\\\":\\n\",\n",
    "    \"        dataset = CelebDFDataset(\\n\",\n",
    "    \"            root=DATASET_ROOT,\\n\",\n",
    "    \"            split=\\\"test\\\",  # Use test split for visualization\\n\",\n",
    "    \"            img_size=224,\\n\",\n",
    "    \"            transform=transform\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"Unknown dataset name: {DATASET_NAME}\\\")\\n\",\n",
    "    \"        dataset = None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    if dataset is not None:\\n\",\n",
    "    \"        print(f\\\"Dataset loaded with {len(dataset)} samples\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Dataset path not found. Cannot load samples.\\\")\\n\",\n",
    "    \"    dataset = None\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def get_sample_batch(dataset, batch_size=4):\\n\",\n",
    "    \"    \\\"\\\"\\\"Get a sample batch with equal number of real and fake samples\\\"\\\"\\\"\\n\",\n",
    "    \"    if dataset is None:\\n\",\n",
    "    \"        return None, None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    dataloader = DataLoader(dataset, batch_size=batch_size*10, shuffle=True)\\n\",\n",
    "    \"    images, labels = next(iter(dataloader))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Separate real and fake\\n\",\n",
    "    \"    real_imgs = [img for img, label in zip(images, labels) if label == 0]\\n\",\n",
    "    \"    fake_imgs = [img for img, label in zip(images, labels) if label == 1]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Make sure we have enough samples\\n\",\n",
    "    \"    half_batch = batch_size // 2\\n\",\n",
    "    \"    if len(real_imgs) < half_batch or len(fake_imgs) < half_batch:\\n\",\n",
    "    \"        print(\\\"Not enough samples of each class. Getting more...\\\")\\n\",\n",
    "    \"        return get_sample_batch(dataset, batch_size)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get equal number of each\\n\",\n",
    "    \"    real_imgs = real_imgs[:half_batch]\\n\",\n",
    "    \"    fake_imgs = fake_imgs[:half_batch]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Combine\\n\",\n",
    "    \"    sample_imgs = real_imgs + fake_imgs\\n\",\n",
    "    \"    sample_labels = [0] * half_batch + [1] * half_batch\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return torch.stack(sample_imgs), torch.tensor(sample_labels)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get sample batch if dataset exists\\n\",\n",
    "    \"if dataset is not None:\\n\",\n",
    "    \"    sample_images, sample_labels = get_sample_batch(dataset, batch_size=8)\\n\",\n",
    "    \"    print(f\\\"Sample batch shape: {sample_images.shape}\\\")\\n\",\n",
    "    \"    print(f\\\"Sample labels: {sample_labels}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize samples\\n\",\n",
    "    \"    plt.figure(figsize=(15, 8))\\n\",\n",
    "    \"    for i in range(len(sample_images)):\\n\",\n",
    "    \"        plt.subplot(2, 4, i+1)\\n\",\n",
    "    \"        img = sample_images[i].permute(1, 2, 0).numpy()\\n\",\n",
    "    \"        # Denormalize for visualization\\n\",\n",
    "    \"        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\\n\",\n",
    "    \"        img = np.clip(img, 0, 1)\\n\",\n",
    "    \"        plt.imshow(img)\\n\",\n",
    "    \"        plt.title(\\\"Real\\\" if sample_labels[i] == 0 else \\\"Fake\\\")\\n\",\n",
    "    \"        plt.axis('off')\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"No dataset available. Skipping sample visualization.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Model Architecture Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"Visualize the architecture of the models.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def count_parameters(model):\\n\",\n",
    "    \"    \\\"\\\"\\\"Count number of trainable parameters\\\"\\\"\\\"\\n\",\n",
    "    \"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def print_model_summary(model, name):\\n\",\n",
    "    \"    \\\"\\\"\\\"Print model summary\\\"\\\"\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\n{name} Model Summary:\\\")\\n\",\n",
    "    \"    print(f\\\"Total parameters: {count_parameters(model):,}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Print main component information\\n\",\n",
    "    \"    if name.lower() == 'vit':\\n\",\n",
    "    \"        print(f\\\"Image size: {model.img_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Patch size: {model.patch_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of patches: {model.num_patches}\\\")\\n\",\n",
    "    \"        print(f\\\"Embedding dimension: {model.embed_dim}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of transformer blocks: {len(model.blocks)}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of attention heads: {model.blocks[0].attn.num_heads}\\\")\\n\",\n",
    "    \"    elif name.lower() == 'deit':\\n\",\n",
    "    \"        print(f\\\"Image size: {model.img_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Patch size: {model.patch_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of patches: {model.num_patches}\\\")\\n\",\n",
    "    \"        print(f\\\"Embedding dimension: {model.embed_dim}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of transformer blocks: {len(model.blocks)}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of attention heads: {model.blocks[0].attn.num_heads}\\\")\\n\",\n",
    "    \"        print(f\\\"Using distillation: {model.distillation is not None}\\\")\\n\",\n",
    "    \"    elif name.lower() == 'swin':\\n\",\n",
    "    \"        print(f\\\"Image size: {model.img_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Patch size: {model.patch_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Embedding dimension: {model.embed_dim}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of layers: {model.num_layers}\\\")\\n\",\n",
    "    \"        print(f\\\"Depths: {model.depths}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize model architectures\\n\",\n",
    "    \"for name, model in models.items():\\n\",\n",
    "    \"    print_model_summary(model, name)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Try to generate a visual diagram of the model architecture\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    from torchviz import make_dot\\n\",\n",
    "    \"    from torch.autograd import Variable\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Function to create model diagram\\n\",\n",
    "    \"    def visualize_model_graph(model, name):\\n\",\n",
    "    \"        # Create a sample input\\n\",\n",
    "    \"        x = Variable(torch.randn(1, 3, 224, 224)).to(device)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Generate output\\n\",\n",
    "    \"        y = model(x)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create dot graph\\n\",\n",
    "    \"        dot = make_dot(y, params=dict(list(model.named_parameters())))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Save and display\\n\",\n",
    "    \"        dot.format = 'png'\\n\",\n",
    "    \"        dot.render(f\\\"{name}_architecture\\\", cleanup=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Display\\n\",\n",
    "    \"        from IPython.display import Image\\n\",\n",
    "    \"        return Image(filename=f\\\"{name}_architecture.png\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize each model\\n\",\n",
    "    \"    for name, model in models.items():\\n\",\n",
    "    \"        print(f\\\"\\\\nGenerating architecture diagram for {name}...\\\")\\n\",\n",
    "    \"        display(visualize_model_graph(model, name))\\n\",\n",
    "    \"except ImportError:\\n\",\n",
    "    \"    print(\\\"torchviz not installed. Install with: pip install torchviz\\\")\\n\",\n",
    "    \"    print(\\\"Also requires graphviz to be installed. See: https://graphviz.org/download/\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Attention Map Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"Visualize attention maps from transformer-based models.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def get_attention_maps(model, img, model_type):\\n\",\n",
    "    \"    \\\"\\\"\\\"Get attention maps for the given model and image\\\"\\\"\\\"\\n\",\n",
    "    \"    # Move image to device\\n\",\n",
    "    \"    img = img.unsqueeze(0).to(device)\\n\",\n",
    "    \"    attention_maps = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Hook function to retrieve attention maps\\n\",\n",
    "    \"    def hook_fn(module, input, output):\\n\",\n",
    "    \"        if model_type in ['vit', 'deit']:\\n\",\n",
    "    \"            # For ViT/DeiT, attention is of shape (batch_size, num_heads, seq_len, seq_len)\\n\",\n",
    "    \"            attention_maps.append(output.detach().cpu())\\n\",\n",
    "    \"        elif model_type == 'swin':\\n\",\n",
    "    \"            # For Swin, attention structure is different\\n\",\n",
    "    \"            # This is a simplified approach - may need adjustment\\n\",\n",
    "    \"            attention_maps.append(output.detach().cpu())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Register hooks\\n\",\n",
    "    \"    hooks = []\\n\",\n",
    "    \"    if model_type in ['vit', 'deit']:\\n\",\n",
    "    \"        for block in model.blocks:\\n\",\n",
    "    \"            hooks.append(block.attn.register_forward_hook(hook_fn))\\n\",\n",
    "    \"    elif model_type == 'swin':\\n\",\n",
    "    \"        # For Swin, we need to find the window attention modules\\n\",\n",
    "    \"        for layer in model.layers:\\n\",\n",
    "    \"            for block in layer.blocks:\\n\",\n",
    "    \"                hooks.append(block.attn.register_forward_hook(hook_fn))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Forward pass\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        model(img)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Remove hooks\\n\",\n",
    "    \"    for hook in hooks:\\n\",\n",
    "    \"        hook.remove()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return attention_maps\\n\",\n",
    "    \"\\n\",\n",
    "    \"def visualize_attention(model, img, model_type, layer_idx=-1, head_idx=0):\\n\",\n",
    "    \"    \\\"\\\"\\\"Visualize attention map for a specific layer and head\\\"\\\"\\\"\\n\",\n",
    "    \"    attention_maps = get_attention_maps(model, img, model_type)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not attention_maps:\\n\",\n",
    "    \"        print(\\\"No attention maps retrieved.\\\")\\n\",\n",
    "    \"        return\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get attention map for the specified layer\\n\",\n",
    "    \"    if layer_idx < 0:\\n\",\n",
    "    \"        layer_idx = len(attention_maps) + layer_idx\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if layer_idx >= len(attention_maps):\\n\",\n",
    "    \"        print(f\\\"Layer index {layer_idx} out of range. Max: {len(attention_maps)-1}\\\")\\n\",\n",
    "    \"        return\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    attention = attention_maps[layer_idx][0]  # Get the first batch\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # For ViT/DeiT\\n\",\n",
    "    \"    if model_type in ['vit', 'deit']:\\n\",\n",
    "    \"        num_heads = attention.shape[0]\\n\",\n",
    "    \"        if head_idx >= num_heads:\\n\",\n",
    "    \"            print(f\\\"Head index {head_idx} out of range. Max: {num_heads-1}\\\")\\n\",\n",
    "    \"            return\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Get attention for specific head\\n\",\n",
    "    \"        attn_map = attention[head_idx].numpy()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # The first row corresponds to the [CLS] token's attention to all other tokens\\n\",\n",
    "    \"        cls_attn = attn_map[0, 1:]  # Skip the attention to [CLS] itself\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Reshape to image size for visualization\\n\",\n",
    "    \"        size = int(np.sqrt(len(cls_attn)))\\n\",\n",
    "    \"        cls_attn = cls_attn.reshape(size, size)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Visualize\\n\",\n",
    "    \"        plt.figure(figsize=(12, 5))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.subplot(1, 2, 1)\\n\",\n",
    "    \"        # Denormalize image for visualization\\n\",\n",
    "    \"        img_np = img.permute(1, 2, 0).numpy()\\n\",\n",
    "    \"        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\\n\",\n",
    "    \"        img_np = np.clip(img_np, 0, 1)\\n\",\n",
    "    \"        plt.imshow(img_np)\\n\",\n",
    "    \"        plt.title(\\\"Input Image\\\")\\n\",\n",
    "    \"        plt.axis('off')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.subplot(1, 2, 2)\\n\",\n",
    "    \"        plt.imshow(cls_attn, cmap='viridis')\\n\",\n",
    "    \"        plt.title(f\\\"Layer {layer_idx}, Head {head_idx} - CLS Token Attention\\\")\\n\",\n",
    "    \"        plt.colorbar(format='%.2f')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return cls_attn\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"Attention visualization for {model_type} is not implemented.\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize attention maps if we have sample images and transformer models\\n\",\n",
    "    \"if dataset is not None and len(models) > 0:\\n\",\n",
    "    \"    # Get a sample image\\n\",\n",
    "    \"    sample_idx = 0  # Choose an index from the sample batch\\n\",\n",
    "    \"    sample_img = sample_images[sample_idx]\\n\",\n",
    "    \"    sample_label = sample_labels[sample_idx]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Visualizing attention for a {'real' if sample_label == 0 else 'fake'} sample\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize for each transformer model\\n\",\n",
    "    \"    for name, model in models.items():\\n\",\n",
    "    \"        if name in ['vit', 'deit']:  # Currently implemented for ViT and DeiT\\n\",\n",
    "    \"            print(f\\\"\\\\nAttention maps for {name.upper()}:\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Visualize last layer attention\\n\",\n",
    "    \"            print(\\\"Last layer attention:\\\")\\n\",\n",
    "    \"            visualize_attention(model, sample_img, name, layer_idx=-1, head_idx=0)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Visualize middle layer attention\\n\",\n",
    "    \"            middle_layer = len(model.blocks) // 2\\n\",\n",
    "    \"            print(f\\\"Middle layer ({middle_layer}) attention:\\\")\\n\",\n",
    "    \"            visualize_attention(model, sample_img, name, layer_idx=middle_layer, head_idx=0)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Visualize first layer attention\\n\",\n",
    "    \"            print(\\\"First layer attention:\\\")\\n\",\n",
    "    \"            visualize_attention(model, sample_img, name, layer_idx=0, head_idx=0)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Cannot visualize attention maps without sample images and transformer models.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Grad-CAM Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"Use Grad-CAM to visualize the regions of the image that are important for the model's decision.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"class GradCAM:\\n\",\n",
    "    \"    \\\"\\\"\\\"Grad-CAM implementation for CNN-based models\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __init__(self, model, target_layer):\\n\",\n",
    "    \"        self.model = model\\n\",\n",
    "    \"        self.target_layer = target_layer\\n\",\n",
    "    \"        self.gradients = None\\n\",\n",
    "    \"        self.activations = None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Register hooks\\n\",\n",
    "    \"        self.register_hooks()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def register_hooks(self):\\n\",\n",
    "    \"        def forward_hook(module, input, output):\\n\",\n",
    "    \"            self.activations = output.detach()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        def backward_hook(module, grad_input, grad_output):\\n\",\n",
    "    \"            self.gradients = grad_output[0].detach()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Register hooks\\n\",\n",
    "    \"        self.target_layer.register_forward_hook(forward_hook)\\n\",\n",
    "    \"        self.target_layer.register_backward_hook(backward_hook)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __call__(self, x, class_idx=None):\\n\",\n",
    "    \"        # Forward pass\\n\",\n",
    "    \"        b, c, h, w = x.size()\\n\",\n",
    "    \"        logits = self.model(x)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # If class_idx is None, use the model's prediction\\n\",\n",
    "    \"        if class_idx is None:\\n\",\n",
    "    \"            if logits.dim() > 1:\\n\",\n",
    "    \"                class_idx = torch.argmax(logits, dim=1).item()\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                class_idx = (logits > 0.5).long().item()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Backward pass\\n\",\n",
    "    \"        self.model.zero_grad()\\n\",\n",
    "    \"        if logits.dim() > 1:\\n\",\n",
    "    \"            target = torch.zeros_like(logits)\\n\",\n",
    "    \"            target[0, class_idx] = 1\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            target = torch.ones_like(logits) if class_idx == 1 else torch.zeros_like(logits)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        logits.backward(gradient=target, retain_graph=True)\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d6612",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_activation_statistics(statistics, labels):\n",
    "    \\\"\\\"\\\"Visualize activation statistics\\\"\\\"\\\"\n",
    "    # Convert labels to class names\n",
    "    class_names = ['Real', 'Fake']\n",
    "    label_names = [class_names[int(label)] for label in labels]\n",
    "    \n",
    "    # For each layer\n",
    "    for layer_name, layer_stats in statistics.items():\n",
    "        print(f\\\"\\\\nActivation Statistics for {layer_name}:\\\")\\n\",\n",
    "        \n",
    "        # For each statistic\n",
    "        for stat_name, stat_values in layer_stats.items():\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Calculate feature-wise statistics\n",
    "            feature_means = {}\n",
    "            for i, label in enumerate(np.unique(labels)):\n",
    "                class_name = class_names[int(label)]\n",
    "                class_indices = labels == label\n",
    "                feature_means[class_name] = np.mean(stat_values[class_indices], axis=0)\n",
    "            \n",
    "            # Number of features to show\n",
    "            num_features = min(20, stat_values.shape[1])\n",
    "            \n",
    "            # Plot feature-wise statistics\n",
    "            x = np.arange(num_features)\n",
    "            width = 0.35\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(15, 6))\n",
    "            rects1 = ax.bar(x - width/2, feature_means['Real'][:num_features], width, label='Real')\n",
    "            rects2 = ax.bar(x + width/2, feature_means['Fake'][:num_features], width, label='Fake')\n",
    "            \n",
    "            ax.set_xlabel('Feature Index')\n",
    "            ax.set_ylabel(f'Mean {stat_name}')\n",
    "            ax.set_title(f'Feature-wise Mean {stat_name} - {layer_name}')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels([str(i) for i in range(num_features)])\n",
    "            ax.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Plot overall distribution\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.boxplot(x='Class', y='Value', data=pd.DataFrame({\n",
    "                'Class': label_names,\n",
    "                'Value': np.mean(stat_values, axis=1)\n",
    "            }))\n",
    "            plt.title(f'Distribution of Mean {stat_name} - {layer_name}')\n",
    "            plt.show()\n",
    "\n",
    "# Function to visualize feature representations\n",
    "def visualize_feature_space(model, dataloader, method='tsne'):\n",
    "    \\\"\\\"\\\"Visualize feature space using dimensionality reduction\\\"\\\"\\\"\n",
    "    # Dictionary to store features\n",
    "    features_dict = {\n",
    "        'embeddings': [],\n",
    "        'labels': []\n",
    "    }\n",
    "    \n",
    "    # Extract features\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(dataloader, desc=\\\"Extracting features\\\"):\n",
    "            # Move to device\n",
    "            imgs = imgs.to(device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            if hasattr(model, 'extract_features'):\n",
    "                # Use extract_features method if available\n",
    "                features = model.extract_features(imgs)\n",
    "            elif isinstance(model, (ViT, DeiT)):\n",
    "                # For ViT/DeiT, use the output of forward_features\n",
    "                features = model.forward_features(imgs)\n",
    "                if isinstance(features, tuple):\n",
    "                    features = features[0]  # For DeiT, take the class token features\n",
    "            elif isinstance(model, SwinTransformer):\n",
    "                # For Swin, use the output of forward_features\n",
    "                features = model.forward_features(imgs)\n",
    "            else:\n",
    "                # Fall back to a simple forward pass and assumedef get_activation_statistics(model, dataloader, layer_name=None):\n",
    "    \\\"\\\"\\\"Get activation statistics for a specific layer\\\"\\\"\\\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Dictionary to store activations\n",
    "    activations = {}\n",
    "    \n",
    "    # Hook function to get activations\n",
    "    def hook_fn(name):\n",
    "        def hook(module, input, output):\n",
    "            activations[name] = output.detach().clone()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    hooks = []\n",
    "    if layer_name is None:\n",
    "        # Register hooks for multiple interesting layers\n",
    "        if hasattr(model, 'blocks'):  # ViT/DeiT\n",
    "            hooks.append(model.blocks[0].register_forward_hook(hook_fn('first_block')))\n",
    "            hooks.append(model.blocks[-1].register_forward_hook(hook_fn('last_block')))\n",
    "        elif hasattr(model, 'layers'):  # Swin\n",
    "            hooks.append(model.layers[0].register_forward_hook(hook_fn('first_layer')))\n",
    "            hooks.append(model.layers[-1].register_forward_hook(hook_fn('last_layer')))\n",
    "    else:\n",
    "        # Register hook for the specific layer\n",
    "        layer = get_layer_by_name(model, layer_name)\n",
    "        if layer is not None:\n",
    "            hooks.append(layer.register_forward_hook(hook_fn(layer_name)))\n",
    "    \n",
    "    # Dictionary to store statistics\n",
    "    statistics = {}\n",
    "    \n",
    "    # List to store labels\n",
    "    labels = []\n",
    "    \n",
    "    # Process batches\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(dataloader, desc=\\\"Collecting activations\\\"):\n",
    "            # Move to device\n",
    "            imgs = imgs.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            model(imgs)\n",
    "            \n",
    "            # Store labels\n",
    "            labels.extend(lbls.cpu().numpy())\n",
    "            \n",
    "            # Calculate statistics\n",
    "            for name, activation in activations.items():\n",
    "                # Initialize statistics for this layer if not exists\n",
    "                if name not in statistics:\n",
    "                    statistics[name] = {'mean': [], 'std': [], 'min': [], 'max': []}\n",
    "                \n",
    "                # Calculate batch statistics\n",
    "                if activation.dim() > 2:  # For 3D+ activations, get stats over spatial dimensions\n",
    "                    batch_mean = activation.mean(dim=[2, 3]).cpu().numpy()\n",
    "                    batch_std = activation.std(dim=[2, 3]).cpu().numpy()\n",
    "                    batch_min = activation.min(dim=2)[0].min(dim=2)[0].cpu().numpy()\n",
    "                    batch_max = activation.max(dim=2)[0].max(dim=2)[0].cpu().numpy()\n",
    "                else:  # For 2D activations (e.g., transformer outputs)\n",
    "                    batch_mean = activation.mean(dim=1).cpu().numpy()\n",
    "                    batch_std = activation.std(dim=1).cpu().numpy()\n",
    "                    batch_min = activation.min(dim=1)[0].cpu().numpy()\n",
    "                    batch_max = activation.max(dim=1)[0].cpu().numpy()\n",
    "                \n",
    "                # Store batch statistics\n",
    "                statistics[name]['mean'].extend(batch_mean)\n",
    "                statistics[name]['std'].extend(batch_std)\n",
    "                statistics[name]['min'].extend(batch_min)\n",
    "                statistics[name]['max'].extend(batch_max)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    for name in statistics:\n",
    "        for stat_name in statistics[name]:\n",
    "            statistics[name][stat_name] = np.array(statistics[name][stat_name])\n",
    "    \n",
    "    return statistics, np.array(labels)\n",
    "\n",
    "def get_layer_by_name(model, name):\n",
    "    \\\"\\\"\\\"Get layer by name\\\"\\\"\\\"\\n\",\n",
    "    if name == 'first_block' and hasattr(model, 'blocks'):\n",
    "        return model.blocks[0]\n",
    "    elif name == 'last_block' and hasattr(model, 'blocks'):\n",
    "        return model.blocks[-1]\n",
    "    elif name == 'first_layer' and hasattr(model, 'layers'):\n",
    "        return model.layers[0]\n",
    "    elif name == 'last_layer' and hasattr(model, 'layers'):\n",
    "        return model.layers[-1]\n",
    "    else:\n",
    "        print(f\\\"Unknown layer name: {name}\\\")\n",
    "        return Nonedef find_target_layer(model, model_type):\n",
    "    \\\"\\\"\\\"Find the target layer for Grad-CAM based on model type\\\"\\\"\\\"\n",
    "    if model_type == 'vit':\n",
    "        # For ViT, we use the output of the last transformer block\n",
    "        return model.blocks[-1]\n",
    "    elif model_type == 'deit':\n",
    "        # For DeiT, we use the output of the last transformer block\n",
    "        return model.blocks[-1]\n",
    "    elif model_type == 'swin':\n",
    "        # For Swin, we use the output of the last layer\n",
    "        return model.layers[-1]\n",
    "    else:\n",
    "        raise ValueError(f\\\"Unknown model type: {model_type}\\\")\n",
    "\n",
    "# Apply Grad-CAM to our sample images\n",
    "if dataset is not None and len(models) > 0:\n",
    "    print(\\\"\\\\n\\\\nGrad-CAM Visualization:\\\")\n",
    "    \n",
    "    # Try for different samples (real and fake)\n",
    "    for i in range(min(4, len(sample_images))):\n",
    "        sample_img = sample_images[i]\n",
    "        sample_label = sample_labels[i]\n",
    "        \n",
    "        print(f\\\"\\\\nGrad-CAM for {'real' if sample_label == 0 else 'fake'} sample {i+1}:\\\")\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\\\"\\\\n{name.upper()} model:\\\")\n",
    "            try:\n",
    "                # Find target layer\n",
    "                target_layer = find_target_layer(model, name)\n",
    "                \n",
    "                # Visualize Grad-CAM\n",
    "                visualize_gradcam(model, sample_img, target_layer, class_idx=sample_label)\n",
    "            except Exception as e:\n",
    "                print(f\\\"Error applying Grad-CAM to {name} model: {e}\\\")\n",
    "else:\n",
    "    print(\\\"Cannot visualize Grad-CAM without sample images and models.\\\"){\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Deepfake Detection Model Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides visualizations and interpretability tools for the deepfake detection models. It includes:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Model architecture visualization\\n\",\n",
    "    \"2. Attention map visualization\\n\",\n",
    "    \"3. Grad-CAM analysis\\n\",\n",
    "    \"4. Feature visualization\\n\",\n",
    "    \"5. Comparison of model behaviors\\n\",\n",
    "    \"\\n\",\n",
    "    \"These visualizations help understand how the models are making decisions and what features they are focusing on.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import necessary libraries\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"import torch.nn.functional as F\\n\",\n",
    "    \"import torchvision.transforms as transforms\\n\",\n",
    "    \"from torch.utils.data import DataLoader\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add parent directory to path to enable imports from project\\n\",\n",
    "    \"sys.path.append(os.path.abspath('..'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import project modules\\n\",\n",
    "    \"from models.vit.model import ViT\\n\",\n",
    "    \"from models.deit.model import DeiT\\n\",\n",
    "    \"from models.swin.model import SwinTransformer\\n\",\n",
    "    \"from models.model_zoo.model_factory import create_model\\n\",\n",
    "    \"from data.datasets.faceforensics import FaceForensicsDataset\\n\",\n",
    "    \"from data.datasets.celebdf import CelebDFDataset\\n\",\n",
    "    \"from evaluation.visualization.attention_maps import visualize_attention_maps\\n\",\n",
    "    \"from evaluation.visualization.grad_cam import visualize_grad_cam\\n\",\n",
    "    \"from evaluation.visualization.feature_visualization import visualize_features\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plot style\\n\",\n",
    "    \"plt.style.use('fivethirtyeight')\\n\",\n",
    "    \"sns.set(style=\\\"whitegrid\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Pretrained Models\\n\",\n",
    "    \"\\n\",\n",
    "    \"Load pretrained models for visualization. You need to have trained models saved in checkpoint format.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Configure paths - update these to your checkpoint paths\\n\",\n",
    "    \"CHECKPOINT_DIR = \\\"../trained_models\\\"\\n\",\n",
    "    \"VIT_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"vit_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"DEIT_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"deit_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"SWIN_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"swin_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check if checkpoints exist\\n\",\n",
    "    \"vit_exists = os.path.exists(VIT_CHECKPOINT)\\n\",\n",
    "    \"deit_exists = os.path.exists(DEIT_CHECKPOINT)\\n\",\n",
    "    \"swin_exists = os.path.exists(SWIN_CHECKPOINT)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"ViT checkpoint exists: {vit_exists}\\\")\\n\",\n",
    "    \"print(f\\\"DeiT checkpoint exists: {deit_exists}\\\")\\n\",\n",
    "    \"print(f\\\"Swin checkpoint exists: {swin_exists}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set device\\n\",\n",
    "    \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"print(f\\\"Using device: {device}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def load_checkpoint(model, checkpoint_path, device):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load model from checkpoint\\\"\\\"\\\"\\n\",\n",
    "    \"    if not os.path.exists(checkpoint_path):\\n\",\n",
    "    \"        print(f\\\"Checkpoint not found at {checkpoint_path}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        checkpoint = torch.load(checkpoint_path, map_location=device)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Different checkpoint formats\\n\",\n",
    "    \"        if 'model' in checkpoint:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint['model'])\\n\",\n",
    "    \"        elif 'model_state_dict' in checkpoint:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint['model_state_dict'])\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        model = model.to(device)\\n\",\n",
    "    \"        model.eval()  # Set to evaluation mode\\n\",\n",
    "    \"        print(f\\\"Model loaded successfully from {checkpoint_path}\\\")\\n\",\n",
    "    \"        return model\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error loading checkpoint: {e}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize models\\n\",\n",
    "    \"models = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load ViT model\\n\",\n",
    "    \"if vit_exists:\\n\",\n",
    "    \"    vit_model = ViT(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=16,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=768,\\n\",\n",
    "    \"        depth=12,\\n\",\n",
    "    \"        num_heads=12\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    vit_model = load_checkpoint(vit_model, VIT_CHECKPOINT, device)\\n\",\n",
    "    \"    if vit_model is not None:\\n\",\n",
    "    \"        models['vit'] = vit_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load DeiT model\\n\",\n",
    "    \"if deit_exists:\\n\",\n",
    "    \"    deit_model = DeiT(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=16,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=768,\\n\",\n",
    "    \"        depth=12,\\n\",\n",
    "    \"        num_heads=12,\\n\",\n",
    "    \"        distillation=True\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    deit_model = load_checkpoint(deit_model, DEIT_CHECKPOINT, device)\\n\",\n",
    "    \"    if deit_model is not None:\\n\",\n",
    "    \"        models['deit'] = deit_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load Swin model\\n\",\n",
    "    \"if swin_exists:\\n\",\n",
    "    \"    swin_model = SwinTransformer(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=4,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=96,\\n\",\n",
    "    \"        depths=[2, 2, 6, 2],\\n\",\n",
    "    \"        num_heads=[3, 6, 12, 24],\\n\",\n",
    "    \"        window_size=7\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    swin_model = load_checkpoint(swin_model, SWIN_CHECKPOINT, device)\\n\",\n",
    "    \"    if swin_model is not None:\\n\",\n",
    "    \"        models['swin'] = swin_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Loaded {len(models)} models: {list(models.keys())}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Load Sample Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Load some sample images to visualize the model behavior.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Configure dataset paths - update these to your local paths\\n\",\n",
    "    \"FACEFORENSICS_ROOT = \\\"/path/to/datasets/FaceForensics\\\"\\n\",\n",
    "    \"CELEBDF_ROOT = \\\"/path/to/datasets/CelebDF\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Choose one dataset to use for visualization\\n\",\n",
    "    \"DATASET_ROOT = CELEBDF_ROOT  # Change to the dataset you want to use\\n\",\n",
    "    \"DATASET_NAME = \\\"celebdf\\\"     # Change to match your dataset (\\\"faceforensics\\\" or \\\"celebdf\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check if directory exists\\n\",\n",
    "    \"dataset_exists = os.path.exists(DATASET_ROOT)\\n\",\n",
    "    \"print(f\\\"Dataset path exists: {dataset_exists}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define transform\\n\",\n",
    "    \"transform = transforms.Compose([\\n\",\n",
    "    \"    transforms.Resize((224, 224)),\\n\",\n",
    "    \"    transforms.ToTensor(),\\n\",\n",
    "    \"    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load dataset if it exists\\n\",\n",
    "    \"if dataset_exists:\\n\",\n",
    "    \"    if DATASET_NAME == \\\"faceforensics\\\":\\n\",\n",
    "    \"        dataset = FaceForensicsDataset(\\n\",\n",
    "    \"            root=DATASET_ROOT,\\n\",\n",
    "    \"            split=\\\"test\\\",  # Use test split for visualization\\n\",\n",
    "    \"            img_size=224,\\n\",\n",
    "    \"            transform=transform\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    elif DATASET_NAME == \\\"celebdf\\\":\\n\",\n",
    "    \"        dataset = CelebDFDataset(\\n\",\n",
    "    \"            root=DATASET_ROOT,\\n\",\n",
    "    \"            split=\\\"test\\\",  # Use test split for visualization\\n\",\n",
    "    \"            img_size=224,\\n\",\n",
    "    \"            transform=transform\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"Unknown dataset name: {DATASET_NAME}\\\")\\n\",\n",
    "    \"        dataset = None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    if dataset is not None:\\n\",\n",
    "    \"        print(f\\\"Dataset loaded with {len(dataset)} samples\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Dataset path not found. Cannot load samples.\\\")\\n\",\n",
    "    \"    dataset = None\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def get_sample_batch(dataset, batch_size=4):\\n\",\n",
    "    \"    \\\"\\\"\\\"Get a sample batch with equal number of real and fake samples\\\"\\\"\\\"\\n\",\n",
    "    \"    if dataset is None:\\n\",\n",
    "    \"        return None, None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    dataloader = DataLoader(dataset, batch_size=batch_size*10, shuffle=True)\\n\",\n",
    "    \"    images, labels = next(iter(dataloader))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Separate real and fake\\n\",\n",
    "    \"    real_imgs = [img for img, label in zip(images, labels) if label == 0]\\n\",\n",
    "    \"    fake_imgs = [img for img, label in zip(images, labels) if label == 1]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Make sure we have enough samples\\n\",\n",
    "    \"    half_batch = batch_size // 2\\n\",\n",
    "    \"    if len(real_imgs) < half_batch or len(fake_imgs) < half_batch:\\n\",\n",
    "    \"        print(\\\"Not enough samples of each class. Getting more...\\\")\\n\",\n",
    "    \"        return get_sample_batch(dataset, batch_size)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get equal number of each\\n\",\n",
    "    \"    real_imgs = real_imgs[:half_batch]\\n\",\n",
    "    \"    fake_imgs = fake_imgs[:half_batch]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Combine\\n\",\n",
    "    \"    sample_imgs = real_imgs + fake_imgs\\n\",\n",
    "    \"    sample_labels = [0] * half_batch + [1] * half_batch\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return torch.stack(sample_imgs), torch.tensor(sample_labels)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get sample batch if dataset exists\\n\",\n",
    "    \"if dataset is not None:\\n\",\n",
    "    \"    sample_images, sample_labels = get_sample_batch(dataset, batch_size=8)\\n\",\n",
    "    \"    print(f\\\"Sample batch shape: {sample_images.shape}\\\")\\n\",\n",
    "    \"    print(f\\\"Sample labels: {sample_labels}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize samples\\n\",\n",
    "    \"    plt.figure(figsize=(15, 8))\\n\",\n",
    "    \"    for i in range(len(sample_images)):\\n\",\n",
    "    \"        plt.subplot(2, 4, i+1)\\n\",\n",
    "    \"        img = sample_images[i].permute(1, 2, 0).numpy()\\n\",\n",
    "    \"        # Denormalize for visualization\\n\",\n",
    "    \"        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\\n\",\n",
    "    \"        img = np.clip(img, 0, 1)\\n\",\n",
    "    \"        plt.imshow(img)\\n\",\n",
    "    \"        plt.title(\\\"Real\\\" if sample_labels[i] == 0 else \\\"Fake\\\")\\n\",\n",
    "    \"        plt.axis('off')\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"No dataset available. Skipping sample visualization.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Model Architecture Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"Visualize the architecture of the models.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def count_parameters(model):\\n\",\n",
    "    \"    \\\"\\\"\\\"Count number of trainable parameters\\\"\\\"\\\"\\n\",\n",
    "    \"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def print_model_summary(model, name):\\n\",\n",
    "    \"    \\\"\\\"\\\"Print model summary\\\"\\\"\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\n{name} Model Summary:\\\")\\n\",\n",
    "    \"    print(f\\\"Total parameters: {count_parameters(model):,}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Print main component information\\n\",\n",
    "    \"    if name.lower() == 'vit':\\n\",\n",
    "    \"        print(f\\\"Image size: {model.img_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Patch size: {model.patch_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of patches: {model.num_patches}\\\")\\n\",\n",
    "    \"        print(f\\\"Embedding dimension: {model.embed_dim}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of transformer blocks: {len(model.blocks)}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of attention heads: {model.blocks[0].attn.num_heads}\\\")\\n\",\n",
    "    \"    elif name.lower() == 'deit':\\n\",\n",
    "    \"        print(f\\\"Image size: {model.img_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Patch size: {model.patch_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of patches: {model.num_patches}\\\")\\n\",\n",
    "    \"        print(f\\\"Embedding dimension: {model.embed_dim}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of transformer blocks: {len(model.blocks)}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of attention heads: {model.blocks[0].attn.num_heads}\\\")\\n\",\n",
    "    \"        print(f\\\"Using distillation: {model.distillation is not None}\\\")\\n\",\n",
    "    \"    elif name.lower() == 'swin':\\n\",\n",
    "    \"        print(f\\\"Image size: {model.img_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Patch size: {model.patch_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Embedding dimension: {model.embed_dim}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of layers: {model.num_layers}\\\")\\n\",\n",
    "    \"        print(f\\\"Depths: {model.depths}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize model architectures\\n\",\n",
    "    \"for name, model in models.items():\\n\",\n",
    "    \"    print_model_summary(model, name)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Try to generate a visual diagram of the model architecture\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    from torchviz import make_dot\\n\",\n",
    "    \"    from torch.autograd import Variable\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Function to create model diagram\\n\",\n",
    "    \"    def visualize_model_graph(model, name):\\n\",\n",
    "    \"        # Create a sample input\\n\",\n",
    "    \"        x = Variable(torch.randn(1, 3, 224, 224)).to(device)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Generate output\\n\",\n",
    "    \"        y = model(x)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create dot graph\\n\",\n",
    "    \"        dot = make_dot(y, params=dict(list(model.named_parameters())))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Save and display\\n\",\n",
    "    \"        dot.format = 'png'\\n\",\n",
    "    \"        dot.render(f\\\"{name}_architecture\\\", cleanup=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Display\\n\",\n",
    "    \"        from IPython.display import Image\\n\",\n",
    "    \"        return Image(filename=f\\\"{name}_architecture.png\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize each model\\n\",\n",
    "    \"    for name, model in models.items():\\n\",\n",
    "    \"        print(f\\\"\\\\nGenerating architecture diagram for {name}...\\\")\\n\",\n",
    "    \"        display(visualize_model_graph(model, name))\\n\",\n",
    "    \"except ImportError:\\n\",\n",
    "    \"    print(\\\"torchviz not installed. Install with: pip install torchviz\\\")\\n\",\n",
    "    \"    print(\\\"Also requires graphviz to be installed. See: https://graphviz.org/download/\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Attention Map Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"Visualize attention maps from transformer-based models.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def get_attention_maps(model, img, model_type):\\n\",\n",
    "    \"    \\\"\\\"\\\"Get attention maps for the given model and image\\\"\\\"\\\"\\n\",\n",
    "    \"    # Move image to device\\n\",\n",
    "    \"    img = img.unsqueeze(0).to(device)\\n\",\n",
    "    \"    attention_maps = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Hook function to retrieve attention maps\\n\",\n",
    "    \"    def hook_fn(module, input, output):\\n\",\n",
    "    \"        if model_type in ['vit', 'deit']:\\n\",\n",
    "    \"            # For ViT/DeiT, attention is of shape (batch_size, num_heads, seq_len, seq_len)\\n\",\n",
    "    \"            attention_maps.append(output.detach().cpu())\\n\",\n",
    "    \"        elif model_type == 'swin':\\n\",\n",
    "    \"            # For Swin, attention structure is different\\n\",\n",
    "    \"            # This is a simplified approach - may need adjustment\\n\",\n",
    "    \"            attention_maps.append(output.detach().cpu())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Register hooks\\n\",\n",
    "    \"    hooks = []\\n\",\n",
    "    \"    if model_type in ['vit', 'deit']:\\n\",\n",
    "    \"        for block in model.blocks:\\n\",\n",
    "    \"            hooks.append(block.attn.register_forward_hook(hook_fn))\\n\",\n",
    "    \"    elif model_type == 'swin':\\n\",\n",
    "    \"        # For Swin, we need to find the window attention modules\\n\",\n",
    "    \"        for layer in model.layers:\\n\",\n",
    "    \"            for block in layer.blocks:\\n\",\n",
    "    \"                hooks.append(block.attn.register_forward_hook(hook_fn))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Forward pass\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        model(img)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Remove hooks\\n\",\n",
    "    \"    for hook in hooks:\\n\",\n",
    "    \"        hook.remove()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return attention_maps\\n\",\n",
    "    \"\\n\",\n",
    "    \"def visualize_attention(model, img, model_type, layer_idx=-1, head_idx=0):\\n\",\n",
    "    \"    \\\"\\\"\\\"Visualize attention map for a specific layer and head\\\"\\\"\\\"\\n\",\n",
    "    \"    attention_maps = get_attention_maps(model, img, model_type)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not attention_maps:\\n\",\n",
    "    \"        print(\\\"No attention maps retrieved.\\\")\\n\",\n",
    "    \"        return\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get attention map for the specified layer\\n\",\n",
    "    \"    if layer_idx < 0:\\n\",\n",
    "    \"        layer_idx = len(attention_maps) + layer_idx\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if layer_idx >= len(attention_maps):\\n\",\n",
    "    \"        print(f\\\"Layer index {layer_idx} out of range. Max: {len(attention_maps)-1}\\\")\\n\",\n",
    "    \"        return\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    attention = attention_maps[layer_idx][0]  # Get the first batch\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # For ViT/DeiT\\n\",\n",
    "    \"    if model_type in ['vit', 'deit']:\\n\",\n",
    "    \"        num_heads = attention.shape[0]\\n\",\n",
    "    \"        if head_idx >= num_heads:\\n\",\n",
    "    \"            print(f\\\"Head index {head_idx} out of range. Max: {num_heads-1}\\\")\\n\",\n",
    "    \"            return\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Get attention for specific head\\n\",\n",
    "    \"        attn_map = attention[head_idx].numpy()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # The first row corresponds to the [CLS] token's attention to all other tokens\\n\",\n",
    "    \"        cls_attn = attn_map[0, 1:]  # Skip the attention to [CLS] itself\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Reshape to image size for visualization\\n\",\n",
    "    \"        size = int(np.sqrt(len(cls_attn)))\\n\",\n",
    "    \"        cls_attn = cls_attn.reshape(size, size)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Visualize\\n\",\n",
    "    \"        plt.figure(figsize=(12, 5))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.subplot(1, 2, 1)\\n\",\n",
    "    \"        # Denormalize image for visualization\\n\",\n",
    "    \"        img_np = img.permute(1, 2, 0).numpy()\\n\",\n",
    "    \"        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\\n\",\n",
    "    \"        img_np = np.clip(img_np, 0, 1)\\n\",\n",
    "    \"        plt.imshow(img_np)\\n\",\n",
    "    \"        plt.title(\\\"Input Image\\\")\\n\",\n",
    "    \"        plt.axis('off')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.subplot(1, 2, 2)\\n\",\n",
    "    \"        plt.imshow(cls_attn, cmap='viridis')\\n\",\n",
    "    \"        plt.title(f\\\"Layer {layer_idx}, Head {head_idx} - CLS Token Attention\\\")\\n\",\n",
    "    \"        plt.colorbar(format='%.2f')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return cls_attn\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"Attention visualization for {model_type} is not implemented.\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize attention maps if we have sample images and transformer models\\n\",\n",
    "    \"if dataset is not None and len(models) > 0:\\n\",\n",
    "    \"    # Get a sample image\\n\",\n",
    "    \"    sample_idx = 0  # Choose an index from the sample batch\\n\",\n",
    "    \"    sample_img = sample_images[sample_idx]\\n\",\n",
    "    \"    sample_label = sample_labels[sample_idx]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Visualizing attention for a {'real' if sample_label == 0 else 'fake'} sample\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize for each transformer model\\n\",\n",
    "    \"    for name, model in models.items():\\n\",\n",
    "    \"        if name in ['vit', 'deit']:  # Currently implemented for ViT and DeiT\\n\",\n",
    "    \"            print(f\\\"\\\\nAttention maps for {name.upper()}:\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Visualize last layer attention\\n\",\n",
    "    \"            print(\\\"Last layer attention:\\\")\\n\",\n",
    "    \"            visualize_attention(model, sample_img, name, layer_idx=-1, head_idx=0)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Visualize middle layer attention\\n\",\n",
    "    \"            middle_layer = len(model.blocks) // 2\\n\",\n",
    "    \"            print(f\\\"Middle layer ({middle_layer}) attention:\\\")\\n\",\n",
    "    \"            visualize_attention(model, sample_img, name, layer_idx=middle_layer, head_idx=0)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Visualize first layer attention\\n\",\n",
    "    \"            print(\\\"First layer attention:\\\")\\n\",\n",
    "    \"            visualize_attention(model, sample_img, name, layer_idx=0, head_idx=0)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Cannot visualize attention maps without sample images and transformer models.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Grad-CAM Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"Use Grad-CAM to visualize the regions of the image that are important for the model's decision.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"class GradCAM:\\n\",\n",
    "    \"    \\\"\\\"\\\"Grad-CAM implementation for CNN-based models\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __init__(self, model, target_layer):\\n\",\n",
    "    \"        self.model = model\\n\",\n",
    "    \"        self.target_layer = target_layer\\n\",\n",
    "    \"        self.gradients = None\\n\",\n",
    "    \"        self.activations = None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Register hooks\\n\",\n",
    "    \"        self.register_hooks()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def register_hooks(self):\\n\",\n",
    "    \"        def forward_hook(module, input, output):\\n\",\n",
    "    \"            self.activations = output.detach()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        def backward_hook(module, grad_input, grad_output):\\n\",\n",
    "    \"            self.gradients = grad_output[0].detach()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Register hooks\\n\",\n",
    "    \"        self.target_layer.register_forward_hook(forward_hook)\\n\",\n",
    "    \"        self.target_layer.register_backward_hook(backward_hook)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __call__(self, x, class_idx=None):\\n\",\n",
    "    \"        # Forward pass\\n\",\n",
    "    \"        b, c, h, w = x.size()\\n\",\n",
    "    \"        logits = self.model(x)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # If class_idx is None, use the model's prediction\\n\",\n",
    "    \"        if class_idx is None:\\n\",\n",
    "    \"            if logits.dim() > 1:\\n\",\n",
    "    \"                class_idx = torch.argmax(logits, dim=1).item()\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                class_idx = (logits > 0.5).long().item()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Backward pass\\n\",\n",
    "    \"        self.model.zero_grad()\\n\",\n",
    "    \"        if logits.dim() > 1:\\n\",\n",
    "    \"            target = torch.zeros_like(logits)\\n\",\n",
    "    \"            target[0, class_idx] = 1\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            target = torch.ones_like(logits) if class_idx == 1 else torch.zeros_like(logits)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        logits.backward(gradient=target, retain_graph=True)\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffe806",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 7. Summary and Insights\n",
    "\n",
    "def generate_insights_summary(models):\n",
    "    \\\"\\\"\\\"Generate a summary of insights from model visualization\\\"\\\"\\\"\n",
    "    print(\"\\n\\nSummary of Insights from Model Visualization:\\n\")\n",
    "    \n",
    "    # 1. Model Architecture Insights\n",
    "    print(\"1. Model Architecture Insights:\")\n",
    "    for name, model in models.items():\n",
    "        print(f\"  - {name.upper()} model has {count_parameters(model):,} trainable parameters\")\n",
    "        \n",
    "        if name == 'vit':\n",
    "            print(f\"    * Uses a transformer architecture with {len(model.blocks)} blocks\")\n",
    "            print(f\"    * Uses {model.blocks[0].attn.num_heads} attention heads\")\n",
    "            print(f\"    * Processes images as {model.num_patches} patches of size {model.patch_size}x{model.patch_size}\")\n",
    "        elif name == 'deit':\n",
    "            print(f\"    * Similar to ViT but with {len(model.blocks)} blocks\")\n",
    "            print(f\"    * Uses {model.blocks[0].attn.num_heads} attention heads\")\n",
    "            print(f\"    * Uses distillation token: {model.distillation is not None}\")\n",
    "        elif name == 'swin':\n",
    "            print(f\"    * Uses hierarchical structure with {model.num_layers} layers\")\n",
    "            print(f\"    * Uses shifted windows for efficient attention\")\n",
    "    \n",
    "    # 2. Attention Mechanism Insights\n",
    "    print(\"\\n2. Attention Mechanism Insights:\")\n",
    "    for name in models:\n",
    "        if name in ['vit', 'deit']:\n",
    "            print(f\"  - {name.upper()} model:\")\n",
    "            print(\"    * Class token attention shows which image regions are important for classification\")\n",
    "            print(\"    * Early layers capture low-level features, while later layers focus on semantic regions\")\n",
    "            print(\"    * Different attention heads specialize in different aspects of the image\")\n",
    "    \n",
    "    # 3. Grad-CAM Insights\n",
    "    print(\"\\n3. Grad-CAM Insights:\")\n",
    "    print(\"  - Highlights regions that influence the model's decision\")\n",
    "    print(\"  - For real faces, models often focus on natural facial features\")\n",
    "    print(\"  - For fake faces, models often focus on inconsistencies or artifacts\")\n",
    "    \n",
    "    # 4. Feature Space Insights\n",
    "    print(\"\\n4. Feature Space Insights:\")\n",
    "    print(\"  - Models learn to separate real and fake samples in the feature space\")\n",
    "    print(\"  - The degree of separation indicates the model's confidence\")\n",
    "    print(\"  - Samples close to the decision boundary are more challenging to classify\")\n",
    "    \n",
    "    # 5. Model Comparison Insights\n",
    "    if len(models) > 1:\n",
    "        print(\"\\n5. Model Comparison Insights:\")\n",
    "        print(\"  - Different models may focus on different aspects of the images\")\n",
    "        print(\"  - Agreement between models suggests more reliable predictions\")\n",
    "        print(\"  - Ensemble methods could improve overall performance by combining strengths\")\n",
    "    \n",
    "    # 6. Challenging Cases\n",
    "    print(\"\\n6. Challenging Cases:\")\n",
    "    print(\"  - High-quality deepfakes may fool models by preserving natural facial features\")\n",
    "    print(\"  - Poor quality real images may be misclassified due to compression artifacts\")\n",
    "    print(\"  - Models may struggle with unusual lighting, poses, or facial expressions\")\n",
    "    \n",
    "    # 7. Practical Recommendations\n",
    "    print(\"\\n7. Practical Recommendations:\")\n",
    "    print(\"  - Use multiple models for more reliable detection\")\n",
    "    print(\"  - Consider model confidence in the decision-making process\")\n",
    "    print(\"  - Further training on challenging examples could improve performance\")\n",
    "    print(\"  - Model interpretability tools can help understand and debug failures\")\n",
    "\n",
    "# Generate insights summary if we have models\n",
    "if len(models) > 0:\n",
    "    generate_insights_summary(models)\n",
    "\n",
    "print(\"\\n\\nModel Visualization Notebook Complete!\")\n",
    "print(\"You can use the above visualizations and analyses to better understand how the deepfake detection models work.\")\n",
    "print(\"This understanding can help improve model performance, interpretability, and trust in the system.\")\n",
    "# Model interpretability analysis\n",
    "if dataset is not None and len(models) > 0:\n",
    "    print(\"\\n\\nDetailed Model Interpretability Analysis:\")\n",
    "    \n",
    "    # Get a few interesting samples\n",
    "    # Try to find real and fake samples\n",
    "    real_samples = []\n",
    "    fake_samples = []\n",
    "    \n",
    "    for i in range(len(sample_images)):\n",
    "        if sample_labels[i] == 0 and len(real_samples) < 2:\n",
    "            real_samples.append((sample_images[i], i))\n",
    "        elif sample_labels[i] == 1 and len(fake_samples) < 2:\n",
    "            fake_samples.append((sample_images[i], i))\n",
    "        \n",
    "        if len(real_samples) >= 2 and len(fake_samples) >= 2:\n",
    "            break\n",
    "    \n",
    "    # Combine samples\n",
    "    selected_samples = real_samples + fake_samples\n",
    "    \n",
    "    # Analyze each sample with each model\n",
    "    for img, idx in selected_samples:\n",
    "        label = sample_labels[idx]\n",
    "        label_name = \"Real\" if label == 0 else \"Fake\"\n",
    "        print(f\"\\n\\nAnalyzing {label_name} sample (index {idx}):\")\n",
    "        \n",
    "        # Show image\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        img_np = np.clip(img_np, 0, 1)\n",
    "        plt.imshow(img_np)\n",
    "        plt.title(f\"Sample Image (Ground Truth: {label_name})\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyze with each model\n",
    "        for name, model in models.items():\n",
    "            analyze_model_interpretability(model, img, name)\n",
    "else:\n",
    "    print(\"Cannot perform interpretability analysis without sample images and models.\")## 6. Model Interpretability Analysis\n",
    "\n",
    "def analyze_model_interpretability(model, img, model_type):\n",
    "    \\\"\\\"\\\"Comprehensive interpretability analysis for a single image\\\"\\\"\\\"\n",
    "    print(f\"\\nInterpretability Analysis for {model_type.upper()} model:\")\n",
    "    \n",
    "    # Move image to device\n",
    "    img_tensor = img.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get model prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        \n",
    "        # Handle different output formats\n",
    "        if output.dim() > 1 and output.shape[1] > 1:  # Multi-class\n",
    "            prob = torch.softmax(output, dim=1)[0, 1].item()  # Probability of fake class\n",
    "            pred = output.argmax(dim=1).item()\n",
    "        else:  # Binary\n",
    "            prob = torch.sigmoid(output).item()\n",
    "            pred = int(prob > 0.5)\n",
    "    \n",
    "    pred_class = \"Fake\" if pred == 1 else \"Real\"\n",
    "    print(f\"Model prediction: {pred_class} with confidence {prob:.4f}\")\n",
    "    \n",
    "    # 1. Attention Map Visualization\n",
    "    if model_type in ['vit', 'deit']:\n",
    "        print(\"\\n1. Attention Map Visualization:\")\n",
    "        visualize_attention(model, img, model_type, layer_idx=-1, head_idx=0)\n",
    "    \n",
    "    # 2. Grad-CAM Visualization\n",
    "    print(\"\\n2. Grad-CAM Visualization:\")\n",
    "    try:\n",
    "        target_layer = find_target_layer(model, model_type)\n",
    "        visualize_gradcam(model, img, target_layer, class_idx=pred)\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying Grad-CAM: {e}\")\n",
    "    \n",
    "    # 3. Feature Importance Analysis\n",
    "    print(\"\\n3. Feature Importance Analysis:\")\n",
    "    if model_type in ['vit', 'deit']:\n",
    "        # For transformer models, analyze attention to different patches\n",
    "        attention_maps = get_attention_maps(model, img, model_type)\n",
    "        \n",
    "        if attention_maps:\n",
    "            # Get attention from last layer\n",
    "            attention = attention_maps[-1][0]  # Get the first batch\n",
    "            \n",
    "            # Average across heads\n",
    "            avg_attention = attention.mean(dim=0)\n",
    "            \n",
    "            # Get CLS token attention to patches\n",
    "            cls_attention = avg_attention[0, 1:]\n",
    "            \n",
    "            # Reshape to image grid\n",
    "            size = int(np.sqrt(len(cls_attention)))\n",
    "            attn_grid = cls_attention.reshape(size, size).numpy()\n",
    "            \n",
    "            # Visualize as heatmap\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(attn_grid, cmap='viridis')\n",
    "            plt.colorbar(format='%.2f')\n",
    "            plt.title(f\"{model_type.upper()} Patch Importance\")\n",
    "            plt.show()\n",
    "            \n",
    "            # Identify most important patches\n",
    "            top_k = 5\n",
    "            flat_indices = np.argsort(cls_attention.numpy())[-top_k:]\n",
    "            row_indices = flat_indices // size\n",
    "            col_indices = flat_indices % size\n",
    "            \n",
    "            print(f\"Top {top_k} important patches:\")\n",
    "            for i, (row, col) in enumerate(zip(row_indices, col_indices)):\n",
    "                print(f\"  {i+1}. Patch at position ({row}, {col}) with importance {cls_attention[row*size + col]:.4f}\")\n",
    "    \n",
    "    return pred, prob# Compare model predictions\n",
    "if dataset is not None and len(models) > 1:\n",
    "    print(\"\\n\\nComparing Model Predictions:\")\n",
    "    \n",
    "    # Create a dataloader for testing\n",
    "    test_dataset = torch.utils.data.Subset(dataset, \n",
    "                                         indices=np.random.choice(len(dataset), \n",
    "                                                               size=min(300, len(dataset)), \n",
    "                                                               replace=False))\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Compare predictions\n",
    "    predictions, probabilities, labels = compare_model_predictions(models, test_dataloader)\n",
    "    \n",
    "    # Analyze difficult examples\n",
    "    print(\"\\nAnalyzing Difficult Examples:\")\n",
    "    \n",
    "    # Find examples where models disagree\n",
    "    disagreement = np.zeros(len(labels), dtype=bool)\n",
    "    model_names = list(models.keys())\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i+1, len(model_names)):\n",
    "            disagreement |= (predictions[model_names[i]] != predictions[model_names[j]])\n",
    "    \n",
    "    # Find examples where all models are wrong\n",
    "    all_wrong = np.ones(len(labels), dtype=bool)\n",
    "    for name in models:\n",
    "        all_wrong &= (predictions[name] != labels)\n",
    "    \n",
    "    # Print statistics\n",
    "    n_disagreement = disagreement.sum()\n",
    "    n_all_wrong = all_wrong.sum()\n",
    "    print(f\"Models disagree on {n_disagreement} examples ({n_disagreement/len(labels)*100:.1f}%)\")\n",
    "    print(f\"All models are wrong on {n_all_wrong} examples ({n_all_wrong/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    # Get indices of disagreement and all-wrong examples\n",
    "    disagreement_indices = np.where(disagreement)[0]\n",
    "    all_wrong_indices = np.where(all_wrong)[0]\n",
    "    \n",
    "    # Print confusion matrix for each model\n",
    "    print(\"\\nConfusion Matrices:\")\n",
    "    for name in models:\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        cm = confusion_matrix(labels, predictions[name])\n",
    "        print(f\"\\n{name.upper()} Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'{name.upper()} Confusion Matrix')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Cannot compare model predictions without dataset or with less than 2 models.\")def compare_model_predictions(models, dataloader):\n",
    "    \\\"\\\"\\\"Compare predictions from different models\\\"\\\"\\\"\n",
    "    # Dictionary to store predictions\n",
    "    predictions = {name: [] for name in models}\n",
    "    probabilities = {name: [] for name in models}\n",
    "    \n",
    "    # List to store ground truth labels\n",
    "    labels = []\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    for imgs, lbls in tqdm(dataloader, desc=\"Getting predictions\"):\n",
    "        # Move images to device\n",
    "        imgs = imgs.to(device)\n",
    "        \n",
    "        # Store ground truth labels\n",
    "        labels.extend(lbls.numpy())\n",
    "        \n",
    "        # Get predictions from each model\n",
    "        for name, model in models.items():\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(imgs)\n",
    "                \n",
    "                # Handle different output formats\n",
    "                if outputs.dim() > 1 and outputs.shape[1] > 1:  # Multi-class\n",
    "                    probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()  # Probability of fake class\n",
    "                    preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "                else:  # Binary\n",
    "                    probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "                    preds = (probs > 0.5).astype(int)\n",
    "                \n",
    "                # Store predictions and probabilities\n",
    "                predictions[name].extend(preds)\n",
    "                probabilities[name].extend(probs)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    labels = np.array(labels)\n",
    "    for name in models:\n",
    "        predictions[name] = np.array(predictions[name])\n",
    "        probabilities[name] = np.array(probabilities[name])\n",
    "    \n",
    "    # Compare predictions\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot distribution of predictions\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for name in models:\n",
    "        sns.kdeplot(probabilities[name], label=name.upper())\n",
    "    plt.xlabel('Probability of Fake')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of Predicted Probabilities')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot prediction agreement\n",
    "    plt.subplot(2, 2, 2)\n",
    "    if len(models) > 1:\n",
    "        model_names = list(models.keys())\n",
    "        agreement_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "        \n",
    "        for i, name1 in enumerate(model_names):\n",
    "            for j, name2 in enumerate(model_names):\n",
    "                if i == j:\n",
    "                    agreement_matrix[i, j] = 1.0\n",
    "                else:\n",
    "                    agreement = np.mean(predictions[name1] == predictions[name2])\n",
    "                    agreement_matrix[i, j] = agreement\n",
    "        \n",
    "        sns.heatmap(agreement_matrix, annot=True, fmt='.2f', \n",
    "                   xticklabels=[name.upper() for name in model_names],\n",
    "                   yticklabels=[name.upper() for name in model_names],\n",
    "                   cmap='YlGnBu')\n",
    "        plt.title('Model Prediction Agreement')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Need at least 2 models to compare', \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Plot accuracy comparison\n",
    "    plt.subplot(2, 2, 3)\n",
    "    accuracies = {}\n",
    "    for name in models:\n",
    "        accuracies[name] = np.mean(predictions[name] == labels)\n",
    "    \n",
    "    plt.bar(range(len(accuracies)), list(accuracies.values()), tick_label=[name.upper() for name in accuracies])\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.ylim(0.5, 1.0)  # Start from 0.5 for better visual comparison\n",
    "    \n",
    "    # For each bar, add the accuracy value on top\n",
    "    for i, v in enumerate(accuracies.values()):\n",
    "        plt.text(i, v + 0.01, f'{v:.2f}', ha='center')\n",
    "    \n",
    "    # Plot per-class accuracy\n",
    "    plt.subplot(2, 2, 4)\n",
    "    per_class_acc = {}\n",
    "    for name in models:\n",
    "        real_acc = np.mean((predictions[name] == labels)[labels == 0])\n",
    "        fake_acc = np.mean((predictions[name] == labels)[labels == 1])\n",
    "        per_class_acc[name] = [real_acc, fake_acc]\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    df = pd.DataFrame(per_class_acc, index=['Real', 'Fake']).T\n",
    "    df.plot(kind='bar', ax=plt.gca())\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Per-class Accuracy Comparison')\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    plt.legend(title='Class')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return predictions, probabilities, labels# Feature space visualization\n",
    "if dataset is not None and len(models) > 0:\n",
    "    print(\"\\n\\nFeature Space Visualization:\")\n",
    "    \n",
    "    # Create a smaller dataloader for visualization\n",
    "    vis_dataset = torch.utils.data.Subset(dataset, \n",
    "                                         indices=np.random.choice(len(dataset), \n",
    "                                                               size=min(200, len(dataset)), \n",
    "                                                               replace=False))\n",
    "    vis_dataloader = DataLoader(vis_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Visualize feature space for each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nVisualizing feature space for {name.upper()} model:\")\n",
    "        try:\n",
    "            visualize_feature_space(model, vis_dataloader, method='tsne')\n",
    "        except Exception as e:\n",
    "            print(f\"Error visualizing feature space for {name} model: {e}\")\n",
    "else:\n",
    "    print(\"Cannot visualize feature space without dataset and models.\")def visualize_activation_statistics(statistics, labels):\n",
    "    \\\"\\\"\\\"Visualize activation statistics\\\"\\\"\\\"\n",
    "    # Convert labels to class names\n",
    "    class_names = ['Real', 'Fake']\n",
    "    label_names = [class_names[int(label)] for label in labels]\n",
    "    \n",
    "    # For each layer\n",
    "    for layer_name, layer_stats in statistics.items():\n",
    "        print(f\\\"\\\\nActivation Statistics for {layer_name}:\\\")\\n\",\n",
    "        \n",
    "        # For each statistic\n",
    "        for stat_name, stat_values in layer_stats.items():\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Calculate feature-wise statistics\n",
    "            feature_means = {}\n",
    "            for i, label in enumerate(np.unique(labels)):\n",
    "                class_name = class_names[int(label)]\n",
    "                class_indices = labels == label\n",
    "                feature_means[class_name] = np.mean(stat_values[class_indices], axis=0)\n",
    "            \n",
    "            # Number of features to show\n",
    "            num_features = min(20, stat_values.shape[1])\n",
    "            \n",
    "            # Plot feature-wise statistics\n",
    "            x = np.arange(num_features)\n",
    "            width = 0.35\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(15, 6))\n",
    "            rects1 = ax.bar(x - width/2, feature_means['Real'][:num_features], width, label='Real')\n",
    "            rects2 = ax.bar(x + width/2, feature_means['Fake'][:num_features], width, label='Fake')\n",
    "            \n",
    "            ax.set_xlabel('Feature Index')\n",
    "            ax.set_ylabel(f'Mean {stat_name}')\n",
    "            ax.set_title(f'Feature-wise Mean {stat_name} - {layer_name}')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels([str(i) for i in range(num_features)])\n",
    "            ax.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Plot overall distribution\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.boxplot(x='Class', y='Value', data=pd.DataFrame({\n",
    "                'Class': label_names,\n",
    "                'Value': np.mean(stat_values, axis=1)\n",
    "            }))\n",
    "            plt.title(f'Distribution of Mean {stat_name} - {layer_name}')\n",
    "            plt.show()\n",
    "\n",
    "# Function to visualize feature representations\n",
    "def visualize_feature_space(model, dataloader, method='tsne'):\n",
    "    \\\"\\\"\\\"Visualize feature space using dimensionality reduction\\\"\\\"\\\"\n",
    "    # Dictionary to store features\n",
    "    features_dict = {\n",
    "        'embeddings': [],\n",
    "        'labels': []\n",
    "    }\n",
    "    \n",
    "    # Extract features\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            # Move to device\n",
    "            imgs = imgs.to(device)\n",
    "            \n",
    "            # Get embeddings\n",
    "            if hasattr(model, 'extract_features'):\n",
    "                # Use extract_features method if available\n",
    "                features = model.extract_features(imgs)\n",
    "            elif isinstance(model, (ViT, DeiT)):\n",
    "                # For ViT/DeiT, use the output of forward_features\n",
    "                features = model.forward_features(imgs)\n",
    "                if isinstance(features, tuple):\n",
    "                    features = features[0]  # For DeiT, take the class token features\n",
    "            elif isinstance(model, SwinTransformer):\n",
    "                # For Swin, use the output of forward_features\n",
    "                features = model.forward_features(imgs)\n",
    "            else:\n",
    "                # Fall back to a simple forward pass and assume the model has a feature extractor\n",
    "                print(\"Using generic feature extraction - this might not work as expected\")\n",
    "                features = model(imgs)\n",
    "            \n",
    "            # Store features and labels\n",
    "            features_dict['embeddings'].append(features.cpu().numpy())\n",
    "            features_dict['labels'].append(lbls.numpy())\n",
    "    \n",
    "    # Concatenate features and labels\n",
    "    embeddings = np.concatenate(features_dict['embeddings'], axis=0)\n",
    "    labels = np.concatenate(features_dict['labels'], axis=0)\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    if method.lower() == 'tsne':\n",
    "        try:\n",
    "            from sklearn.manifold import TSNE\n",
    "            print(\"Applying t-SNE dimensionality reduction...\")\n",
    "            reduced_features = TSNE(n_components=2, random_state=42).fit_transform(embeddings)\n",
    "        except ImportError:\n",
    "            print(\"sklearn not installed. Install with: pip install scikit-learn\")\n",
    "            return\n",
    "    elif method.lower() == 'pca':\n",
    "        try:\n",
    "            from sklearn.decomposition import PCA\n",
    "            print(\"Applying PCA dimensionality reduction...\")\n",
    "            reduced_features = PCA(n_components=2, random_state=42).fit_transform(embeddings)\n",
    "        except ImportError:\n",
    "            print(\"sklearn not installed. Install with: pip install scikit-learn\")\n",
    "            return\n",
    "    else:\n",
    "        print(f\"Unknown dimensionality reduction method: {method}\")\n",
    "        return\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], \n",
    "                         c=labels, cmap='viridis', alpha=0.8, s=30)\n",
    "    \n",
    "    # Add legend\n",
    "    legend1 = plt.legend(*scatter.legend_elements(),\n",
    "                        loc=\"upper right\", title=\"Classes\")\n",
    "    plt.gca().add_artist(legend1)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(f\"Feature Space Visualization using {method.upper()}\")\n",
    "    plt.xlabel(f\"{method.upper()} Component 1\")\n",
    "    plt.ylabel(f\"{method.upper()} Component 2\")\n",
    "    \n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return reduced_features, labelsdef get_activation_statistics(model, dataloader, layer_name=None):\n",
    "    \\\"\\\"\\\"Get activation statistics for a specific layer\\\"\\\"\\\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Dictionary to store activations\n",
    "    activations = {}\n",
    "    \n",
    "    # Hook function to get activations\n",
    "    def hook_fn(name):\n",
    "        def hook(module, input, output):\n",
    "            activations[name] = output.detach().clone()\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    hooks = []\n",
    "    if layer_name is None:\n",
    "        # Register hooks for multiple interesting layers\n",
    "        if hasattr(model, 'blocks'):  # ViT/DeiT\n",
    "            hooks.append(model.blocks[0].register_forward_hook(hook_fn('first_block')))\n",
    "            hooks.append(model.blocks[-1].register_forward_hook(hook_fn('last_block')))\n",
    "        elif hasattr(model, 'layers'):  # Swin\n",
    "            hooks.append(model.layers[0].register_forward_hook(hook_fn('first_layer')))\n",
    "            hooks.append(model.layers[-1].register_forward_hook(hook_fn('last_layer')))\n",
    "    else:\n",
    "        # Register hook for the specific layer\n",
    "        layer = get_layer_by_name(model, layer_name)\n",
    "        if layer is not None:\n",
    "            hooks.append(layer.register_forward_hook(hook_fn(layer_name)))\n",
    "    \n",
    "    # Dictionary to store statistics\n",
    "    statistics = {}\n",
    "    \n",
    "    # List to store labels\n",
    "    labels = []\n",
    "    \n",
    "    # Process batches\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(dataloader, desc=\\\"Collecting activations\\\"):\n",
    "            # Move to device\n",
    "            imgs = imgs.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            model(imgs)\n",
    "            \n",
    "            # Store labels\n",
    "            labels.extend(lbls.cpu().numpy())\n",
    "            \n",
    "            # Calculate statistics\n",
    "            for name, activation in activations.items():\n",
    "                # Initialize statistics for this layer if not exists\n",
    "                if name not in statistics:\n",
    "                    statistics[name] = {'mean': [], 'std': [], 'min': [], 'max': []}\n",
    "                \n",
    "                # Calculate batch statistics\n",
    "                if activation.dim() > 2:  # For 3D+ activations, get stats over spatial dimensions\n",
    "                    batch_mean = activation.mean(dim=[2, 3]).cpu().numpy()\n",
    "                    batch_std = activation.std(dim=[2, 3]).cpu().numpy()\n",
    "                    batch_min = activation.min(dim=2)[0].min(dim=2)[0].cpu().numpy()\n",
    "                    batch_max = activation.max(dim=2)[0].max(dim=2)[0].cpu().numpy()\n",
    "                else:  # For 2D activations (e.g., transformer outputs)\n",
    "                    batch_mean = activation.mean(dim=1).cpu().numpy()\n",
    "                    batch_std = activation.std(dim=1).cpu().numpy()\n",
    "                    batch_min = activation.min(dim=1)[0].cpu().numpy()\n",
    "                    batch_max = activation.max(dim=1)[0].cpu().numpy()\n",
    "                \n",
    "                # Store batch statistics\n",
    "                statistics[name]['mean'].extend(batch_mean)\n",
    "                statistics[name]['std'].extend(batch_std)\n",
    "                statistics[name]['min'].extend(batch_min)\n",
    "                statistics[name]['max'].extend(batch_max)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    for name in statistics:\n",
    "        for stat_name in statistics[name]:\n",
    "            statistics[name][stat_name] = np.array(statistics[name][stat_name])\n",
    "    \n",
    "    return statistics, np.array(labels)\n",
    "\n",
    "def get_layer_by_name(model, name):\n",
    "    \\\"\\\"\\\"Get layer by name\\\"\\\"\\\"\\n\",\n",
    "    if name == 'first_block' and hasattr(model, 'blocks'):\n",
    "        return model.blocks[0]\n",
    "    elif name == 'last_block' and hasattr(model, 'blocks'):\n",
    "        return model.blocks[-1]\n",
    "    elif name == 'first_layer' and hasattr(model, 'layers'):\n",
    "        return model.layers[0]\n",
    "    elif name == 'last_layer' and hasattr(model, 'layers'):\n",
    "        return model.layers[-1]\n",
    "    else:\n",
    "        print(f\\\"Unknown layer name: {name}\\\")\n",
    "        return Nonedef find_target_layer(model, model_type):\n",
    "    \\\"\\\"\\\"Find the target layer for Grad-CAM based on model type\\\"\\\"\\\"\n",
    "    if model_type == 'vit':\n",
    "        # For ViT, we use the output of the last transformer block\n",
    "        return model.blocks[-1]\n",
    "    elif model_type == 'deit':\n",
    "        # For DeiT, we use the output of the last transformer block\n",
    "        return model.blocks[-1]\n",
    "    elif model_type == 'swin':\n",
    "        # For Swin, we use the output of the last layer\n",
    "        return model.layers[-1]\n",
    "    else:\n",
    "        raise ValueError(f\\\"Unknown model type: {model_type}\\\")\n",
    "\n",
    "# Apply Grad-CAM to our sample images\n",
    "if dataset is not None and len(models) > 0:\n",
    "    print(\\\"\\\\n\\\\nGrad-CAM Visualization:\\\")\n",
    "    \n",
    "    # Try for different samples (real and fake)\n",
    "    for i in range(min(4, len(sample_images))):\n",
    "        sample_img = sample_images[i]\n",
    "        sample_label = sample_labels[i]\n",
    "        \n",
    "        print(f\\\"\\\\nGrad-CAM for {'real' if sample_label == 0 else 'fake'} sample {i+1}:\\\")\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\\\"\\\\n{name.upper()} model:\\\")\n",
    "            try:\n",
    "                # Find target layer\n",
    "                target_layer = find_target_layer(model, name)\n",
    "                \n",
    "                # Visualize Grad-CAM\n",
    "                visualize_gradcam(model, sample_img, target_layer, class_idx=sample_label)\n",
    "            except Exception as e:\n",
    "                print(f\\\"Error applying Grad-CAM to {name} model: {e}\\\")\n",
    "else:\n",
    "    print(\\\"Cannot visualize Grad-CAM without sample images and models.\\\"){\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Deepfake Detection Model Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides visualizations and interpretability tools for the deepfake detection models. It includes:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Model architecture visualization\\n\",\n",
    "    \"2. Attention map visualization\\n\",\n",
    "    \"3. Grad-CAM analysis\\n\",\n",
    "    \"4. Feature visualization\\n\",\n",
    "    \"5. Comparison of model behaviors\\n\",\n",
    "    \"\\n\",\n",
    "    \"These visualizations help understand how the models are making decisions and what features they are focusing on.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import necessary libraries\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"import torch.nn.functional as F\\n\",\n",
    "    \"import torchvision.transforms as transforms\\n\",\n",
    "    \"from torch.utils.data import DataLoader\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add parent directory to path to enable imports from project\\n\",\n",
    "    \"sys.path.append(os.path.abspath('..'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import project modules\\n\",\n",
    "    \"from models.vit.model import ViT\\n\",\n",
    "    \"from models.deit.model import DeiT\\n\",\n",
    "    \"from models.swin.model import SwinTransformer\\n\",\n",
    "    \"from models.model_zoo.model_factory import create_model\\n\",\n",
    "    \"from data.datasets.faceforensics import FaceForensicsDataset\\n\",\n",
    "    \"from data.datasets.celebdf import CelebDFDataset\\n\",\n",
    "    \"from evaluation.visualization.attention_maps import visualize_attention_maps\\n\",\n",
    "    \"from evaluation.visualization.grad_cam import visualize_grad_cam\\n\",\n",
    "    \"from evaluation.visualization.feature_visualization import visualize_features\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plot style\\n\",\n",
    "    \"plt.style.use('fivethirtyeight')\\n\",\n",
    "    \"sns.set(style=\\\"whitegrid\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Pretrained Models\\n\",\n",
    "    \"\\n\",\n",
    "    \"Load pretrained models for visualization. You need to have trained models saved in checkpoint format.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Configure paths - update these to your checkpoint paths\\n\",\n",
    "    \"CHECKPOINT_DIR = \\\"../trained_models\\\"\\n\",\n",
    "    \"VIT_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"vit_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"DEIT_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"deit_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"SWIN_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"swin_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check if checkpoints exist\\n\",\n",
    "    \"vit_exists = os.path.exists(VIT_CHECKPOINT)\\n\",\n",
    "    \"deit_exists = os.path.exists(DEIT_CHECKPOINT)\\n\",\n",
    "    \"swin_exists = os.path.exists(SWIN_CHECKPOINT)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"ViT checkpoint exists: {vit_exists}\\\")\\n\",\n",
    "    \"print(f\\\"DeiT checkpoint exists: {deit_exists}\\\")\\n\",\n",
    "    \"print(f\\\"Swin checkpoint exists: {swin_exists}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set device\\n\",\n",
    "    \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"print(f\\\"Using device: {device}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def load_checkpoint(model, checkpoint_path, device):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load model from checkpoint\\\"\\\"\\\"\\n\",\n",
    "    \"    if not os.path.exists(checkpoint_path):\\n\",\n",
    "    \"        print(f\\\"Checkpoint not found at {checkpoint_path}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        checkpoint = torch.load(checkpoint_path, map_location=device)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Different checkpoint formats\\n\",\n",
    "    \"        if 'model' in checkpoint:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint['model'])\\n\",\n",
    "    \"        elif 'model_state_dict' in checkpoint:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint['model_state_dict'])\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        model = model.to(device)\\n\",\n",
    "    \"        model.eval()  # Set to evaluation mode\\n\",\n",
    "    \"        print(f\\\"Model loaded successfully from {checkpoint_path}\\\")\\n\",\n",
    "    \"        return model\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error loading checkpoint: {e}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize models\\n\",\n",
    "    \"models = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load ViT model\\n\",\n",
    "    \"if vit_exists:\\n\",\n",
    "    \"    vit_model = ViT(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=16,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=768,\\n\",\n",
    "    \"        depth=12,\\n\",\n",
    "    \"        num_heads=12\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    vit_model = load_checkpoint(vit_model, VIT_CHECKPOINT, device)\\n\",\n",
    "    \"    if vit_model is not None:\\n\",\n",
    "    \"        models['vit'] = vit_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load DeiT model\\n\",\n",
    "    \"if deit_exists:\\n\",\n",
    "    \"    deit_model = DeiT(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=16,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=768,\\n\",\n",
    "    \"        depth=12,\\n\",\n",
    "    \"        num_heads=12,\\n\",\n",
    "    \"        distillation=True\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    deit_model = load_checkpoint(deit_model, DEIT_CHECKPOINT, device)\\n\",\n",
    "    \"    if deit_model is not None:\\n\",\n",
    "    \"        models['deit'] = deit_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load Swin model\\n\",\n",
    "    \"if swin_exists:\\n\",\n",
    "    \"    swin_model = SwinTransformer(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=4,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=96,\\n\",\n",
    "    \"        depths=[2, 2, 6, 2],\\n\",\n",
    "    \"        num_heads=[3, 6, 12, 24],\\n\",\n",
    "    \"        window_size=7\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    swin_model = load_checkpoint(swin_model, SWIN_CHECKPOINT, device)\\n\",\n",
    "    \"    if swin_model is not None:\\n\",\n",
    "    \"        models['swin'] = swin_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Loaded {len(models)} models: {list(models.keys())}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Load Sample Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Load some sample images to visualize the model behavior.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Configure dataset paths - update these to your local paths\\n\",\n",
    "    \"FACEFORENSICS_ROOT = \\\"/path/to/datasets/FaceForensics\\\"\\n\",\n",
    "    \"CELEBDF_ROOT = \\\"/path/to/datasets/CelebDF\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Choose one dataset to use for visualization\\n\",\n",
    "    \"DATASET_ROOT = CELEBDF_ROOT  # Change to the dataset you want to use\\n\",\n",
    "    \"DATASET_NAME = \\\"celebdf\\\"     # Change to match your dataset (\\\"faceforensics\\\" or \\\"celebdf\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check if directory exists\\n\",\n",
    "    \"dataset_exists = os.path.exists(DATASET_ROOT)\\n\",\n",
    "    \"print(f\\\"Dataset path exists: {dataset_exists}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Define transform\\n\",\n",
    "    \"transform = transforms.Compose([\\n\",\n",
    "    \"    transforms.Resize((224, 224)),\\n\",\n",
    "    \"    transforms.ToTensor(),\\n\",\n",
    "    \"    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load dataset if it exists\\n\",\n",
    "    \"if dataset_exists:\\n\",\n",
    "    \"    if DATASET_NAME == \\\"faceforensics\\\":\\n\",\n",
    "    \"        dataset = FaceForensicsDataset(\\n\",\n",
    "    \"            root=DATASET_ROOT,\\n\",\n",
    "    \"            split=\\\"test\\\",  # Use test split for visualization\\n\",\n",
    "    \"            img_size=224,\\n\",\n",
    "    \"            transform=transform\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    elif DATASET_NAME == \\\"celebdf\\\":\\n\",\n",
    "    \"        dataset = CelebDFDataset(\\n\",\n",
    "    \"            root=DATASET_ROOT,\\n\",\n",
    "    \"            split=\\\"test\\\",  # Use test split for visualization\\n\",\n",
    "    \"            img_size=224,\\n\",\n",
    "    \"            transform=transform\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"Unknown dataset name: {DATASET_NAME}\\\")\\n\",\n",
    "    \"        dataset = None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    if dataset is not None:\\n\",\n",
    "    \"        print(f\\\"Dataset loaded with {len(dataset)} samples\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Dataset path not found. Cannot load samples.\\\")\\n\",\n",
    "    \"    dataset = None\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def get_sample_batch(dataset, batch_size=4):\\n\",\n",
    "    \"    \\\"\\\"\\\"Get a sample batch with equal number of real and fake samples\\\"\\\"\\\"\\n\",\n",
    "    \"    if dataset is None:\\n\",\n",
    "    \"        return None, None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    dataloader = DataLoader(dataset, batch_size=batch_size*10, shuffle=True)\\n\",\n",
    "    \"    images, labels = next(iter(dataloader))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Separate real and fake\\n\",\n",
    "    \"    real_imgs = [img for img, label in zip(images, labels) if label == 0]\\n\",\n",
    "    \"    fake_imgs = [img for img, label in zip(images, labels) if label == 1]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Make sure we have enough samples\\n\",\n",
    "    \"    half_batch = batch_size // 2\\n\",\n",
    "    \"    if len(real_imgs) < half_batch or len(fake_imgs) < half_batch:\\n\",\n",
    "    \"        print(\\\"Not enough samples of each class. Getting more...\\\")\\n\",\n",
    "    \"        return get_sample_batch(dataset, batch_size)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get equal number of each\\n\",\n",
    "    \"    real_imgs = real_imgs[:half_batch]\\n\",\n",
    "    \"    fake_imgs = fake_imgs[:half_batch]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Combine\\n\",\n",
    "    \"    sample_imgs = real_imgs + fake_imgs\\n\",\n",
    "    \"    sample_labels = [0] * half_batch + [1] * half_batch\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return torch.stack(sample_imgs), torch.tensor(sample_labels)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get sample batch if dataset exists\\n\",\n",
    "    \"if dataset is not None:\\n\",\n",
    "    \"    sample_images, sample_labels = get_sample_batch(dataset, batch_size=8)\\n\",\n",
    "    \"    print(f\\\"Sample batch shape: {sample_images.shape}\\\")\\n\",\n",
    "    \"    print(f\\\"Sample labels: {sample_labels}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize samples\\n\",\n",
    "    \"    plt.figure(figsize=(15, 8))\\n\",\n",
    "    \"    for i in range(len(sample_images)):\\n\",\n",
    "    \"        plt.subplot(2, 4, i+1)\\n\",\n",
    "    \"        img = sample_images[i].permute(1, 2, 0).numpy()\\n\",\n",
    "    \"        # Denormalize for visualization\\n\",\n",
    "    \"        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\\n\",\n",
    "    \"        img = np.clip(img, 0, 1)\\n\",\n",
    "    \"        plt.imshow(img)\\n\",\n",
    "    \"        plt.title(\\\"Real\\\" if sample_labels[i] == 0 else \\\"Fake\\\")\\n\",\n",
    "    \"        plt.axis('off')\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"No dataset available. Skipping sample visualization.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Model Architecture Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"Visualize the architecture of the models.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def count_parameters(model):\\n\",\n",
    "    \"    \\\"\\\"\\\"Count number of trainable parameters\\\"\\\"\\\"\\n\",\n",
    "    \"    return sum(p.numel() for p in model.parameters() if p.requires_grad)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def print_model_summary(model, name):\\n\",\n",
    "    \"    \\\"\\\"\\\"Print model summary\\\"\\\"\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\n{name} Model Summary:\\\")\\n\",\n",
    "    \"    print(f\\\"Total parameters: {count_parameters(model):,}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Print main component information\\n\",\n",
    "    \"    if name.lower() == 'vit':\\n\",\n",
    "    \"        print(f\\\"Image size: {model.img_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Patch size: {model.patch_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of patches: {model.num_patches}\\\")\\n\",\n",
    "    \"        print(f\\\"Embedding dimension: {model.embed_dim}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of transformer blocks: {len(model.blocks)}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of attention heads: {model.blocks[0].attn.num_heads}\\\")\\n\",\n",
    "    \"    elif name.lower() == 'deit':\\n\",\n",
    "    \"        print(f\\\"Image size: {model.img_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Patch size: {model.patch_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of patches: {model.num_patches}\\\")\\n\",\n",
    "    \"        print(f\\\"Embedding dimension: {model.embed_dim}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of transformer blocks: {len(model.blocks)}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of attention heads: {model.blocks[0].attn.num_heads}\\\")\\n\",\n",
    "    \"        print(f\\\"Using distillation: {model.distillation is not None}\\\")\\n\",\n",
    "    \"    elif name.lower() == 'swin':\\n\",\n",
    "    \"        print(f\\\"Image size: {model.img_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Patch size: {model.patch_size}\\\")\\n\",\n",
    "    \"        print(f\\\"Embedding dimension: {model.embed_dim}\\\")\\n\",\n",
    "    \"        print(f\\\"Number of layers: {model.num_layers}\\\")\\n\",\n",
    "    \"        print(f\\\"Depths: {model.depths}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize model architectures\\n\",\n",
    "    \"for name, model in models.items():\\n\",\n",
    "    \"    print_model_summary(model, name)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Try to generate a visual diagram of the model architecture\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    from torchviz import make_dot\\n\",\n",
    "    \"    from torch.autograd import Variable\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Function to create model diagram\\n\",\n",
    "    \"    def visualize_model_graph(model, name):\\n\",\n",
    "    \"        # Create a sample input\\n\",\n",
    "    \"        x = Variable(torch.randn(1, 3, 224, 224)).to(device)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Generate output\\n\",\n",
    "    \"        y = model(x)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Create dot graph\\n\",\n",
    "    \"        dot = make_dot(y, params=dict(list(model.named_parameters())))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Save and display\\n\",\n",
    "    \"        dot.format = 'png'\\n\",\n",
    "    \"        dot.render(f\\\"{name}_architecture\\\", cleanup=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Display\\n\",\n",
    "    \"        from IPython.display import Image\\n\",\n",
    "    \"        return Image(filename=f\\\"{name}_architecture.png\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize each model\\n\",\n",
    "    \"    for name, model in models.items():\\n\",\n",
    "    \"        print(f\\\"\\\\nGenerating architecture diagram for {name}...\\\")\\n\",\n",
    "    \"        display(visualize_model_graph(model, name))\\n\",\n",
    "    \"except ImportError:\\n\",\n",
    "    \"    print(\\\"torchviz not installed. Install with: pip install torchviz\\\")\\n\",\n",
    "    \"    print(\\\"Also requires graphviz to be installed. See: https://graphviz.org/download/\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Attention Map Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"Visualize attention maps from transformer-based models.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def get_attention_maps(model, img, model_type):\\n\",\n",
    "    \"    \\\"\\\"\\\"Get attention maps for the given model and image\\\"\\\"\\\"\\n\",\n",
    "    \"    # Move image to device\\n\",\n",
    "    \"    img = img.unsqueeze(0).to(device)\\n\",\n",
    "    \"    attention_maps = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Hook function to retrieve attention maps\\n\",\n",
    "    \"    def hook_fn(module, input, output):\\n\",\n",
    "    \"        if model_type in ['vit', 'deit']:\\n\",\n",
    "    \"            # For ViT/DeiT, attention is of shape (batch_size, num_heads, seq_len, seq_len)\\n\",\n",
    "    \"            attention_maps.append(output.detach().cpu())\\n\",\n",
    "    \"        elif model_type == 'swin':\\n\",\n",
    "    \"            # For Swin, attention structure is different\\n\",\n",
    "    \"            # This is a simplified approach - may need adjustment\\n\",\n",
    "    \"            attention_maps.append(output.detach().cpu())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Register hooks\\n\",\n",
    "    \"    hooks = []\\n\",\n",
    "    \"    if model_type in ['vit', 'deit']:\\n\",\n",
    "    \"        for block in model.blocks:\\n\",\n",
    "    \"            hooks.append(block.attn.register_forward_hook(hook_fn))\\n\",\n",
    "    \"    elif model_type == 'swin':\\n\",\n",
    "    \"        # For Swin, we need to find the window attention modules\\n\",\n",
    "    \"        for layer in model.layers:\\n\",\n",
    "    \"            for block in layer.blocks:\\n\",\n",
    "    \"                hooks.append(block.attn.register_forward_hook(hook_fn))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Forward pass\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        model(img)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Remove hooks\\n\",\n",
    "    \"    for hook in hooks:\\n\",\n",
    "    \"        hook.remove()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return attention_maps\\n\",\n",
    "    \"\\n\",\n",
    "    \"def visualize_attention(model, img, model_type, layer_idx=-1, head_idx=0):\\n\",\n",
    "    \"    \\\"\\\"\\\"Visualize attention map for a specific layer and head\\\"\\\"\\\"\\n\",\n",
    "    \"    attention_maps = get_attention_maps(model, img, model_type)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if not attention_maps:\\n\",\n",
    "    \"        print(\\\"No attention maps retrieved.\\\")\\n\",\n",
    "    \"        return\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get attention map for the specified layer\\n\",\n",
    "    \"    if layer_idx < 0:\\n\",\n",
    "    \"        layer_idx = len(attention_maps) + layer_idx\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if layer_idx >= len(attention_maps):\\n\",\n",
    "    \"        print(f\\\"Layer index {layer_idx} out of range. Max: {len(attention_maps)-1}\\\")\\n\",\n",
    "    \"        return\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    attention = attention_maps[layer_idx][0]  # Get the first batch\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # For ViT/DeiT\\n\",\n",
    "    \"    if model_type in ['vit', 'deit']:\\n\",\n",
    "    \"        num_heads = attention.shape[0]\\n\",\n",
    "    \"        if head_idx >= num_heads:\\n\",\n",
    "    \"            print(f\\\"Head index {head_idx} out of range. Max: {num_heads-1}\\\")\\n\",\n",
    "    \"            return\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Get attention for specific head\\n\",\n",
    "    \"        attn_map = attention[head_idx].numpy()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # The first row corresponds to the [CLS] token's attention to all other tokens\\n\",\n",
    "    \"        cls_attn = attn_map[0, 1:]  # Skip the attention to [CLS] itself\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Reshape to image size for visualization\\n\",\n",
    "    \"        size = int(np.sqrt(len(cls_attn)))\\n\",\n",
    "    \"        cls_attn = cls_attn.reshape(size, size)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Visualize\\n\",\n",
    "    \"        plt.figure(figsize=(12, 5))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.subplot(1, 2, 1)\\n\",\n",
    "    \"        # Denormalize image for visualization\\n\",\n",
    "    \"        img_np = img.permute(1, 2, 0).numpy()\\n\",\n",
    "    \"        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\\n\",\n",
    "    \"        img_np = np.clip(img_np, 0, 1)\\n\",\n",
    "    \"        plt.imshow(img_np)\\n\",\n",
    "    \"        plt.title(\\\"Input Image\\\")\\n\",\n",
    "    \"        plt.axis('off')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.subplot(1, 2, 2)\\n\",\n",
    "    \"        plt.imshow(cls_attn, cmap='viridis')\\n\",\n",
    "    \"        plt.title(f\\\"Layer {layer_idx}, Head {head_idx} - CLS Token Attention\\\")\\n\",\n",
    "    \"        plt.colorbar(format='%.2f')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return cls_attn\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(f\\\"Attention visualization for {model_type} is not implemented.\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize attention maps if we have sample images and transformer models\\n\",\n",
    "    \"if dataset is not None and len(models) > 0:\\n\",\n",
    "    \"    # Get a sample image\\n\",\n",
    "    \"    sample_idx = 0  # Choose an index from the sample batch\\n\",\n",
    "    \"    sample_img = sample_images[sample_idx]\\n\",\n",
    "    \"    sample_label = sample_labels[sample_idx]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Visualizing attention for a {'real' if sample_label == 0 else 'fake'} sample\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize for each transformer model\\n\",\n",
    "    \"    for name, model in models.items():\\n\",\n",
    "    \"        if name in ['vit', 'deit']:  # Currently implemented for ViT and DeiT\\n\",\n",
    "    \"            print(f\\\"\\\\nAttention maps for {name.upper()}:\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Visualize last layer attention\\n\",\n",
    "    \"            print(\\\"Last layer attention:\\\")\\n\",\n",
    "    \"            visualize_attention(model, sample_img, name, layer_idx=-1, head_idx=0)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Visualize middle layer attention\\n\",\n",
    "    \"            middle_layer = len(model.blocks) // 2\\n\",\n",
    "    \"            print(f\\\"Middle layer ({middle_layer}) attention:\\\")\\n\",\n",
    "    \"            visualize_attention(model, sample_img, name, layer_idx=middle_layer, head_idx=0)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Visualize first layer attention\\n\",\n",
    "    \"            print(\\\"First layer attention:\\\")\\n\",\n",
    "    \"            visualize_attention(model, sample_img, name, layer_idx=0, head_idx=0)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Cannot visualize attention maps without sample images and transformer models.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Grad-CAM Visualization\\n\",\n",
    "    \"\\n\",\n",
    "    \"Use Grad-CAM to visualize the regions of the image that are important for the model's decision.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"class GradCAM:\\n\",\n",
    "    \"    \\\"\\\"\\\"Grad-CAM implementation for CNN-based models\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __init__(self, model, target_layer):\\n\",\n",
    "    \"        self.model = model\\n\",\n",
    "    \"        self.target_layer = target_layer\\n\",\n",
    "    \"        self.gradients = None\\n\",\n",
    "    \"        self.activations = None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Register hooks\\n\",\n",
    "    \"        self.register_hooks()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def register_hooks(self):\\n\",\n",
    "    \"        def forward_hook(module, input, output):\\n\",\n",
    "    \"            self.activations = output.detach()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        def backward_hook(module, grad_input, grad_output):\\n\",\n",
    "    \"            self.gradients = grad_output[0].detach()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Register hooks\\n\",\n",
    "    \"        self.target_layer.register_forward_hook(forward_hook)\\n\",\n",
    "    \"        self.target_layer.register_backward_hook(backward_hook)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __call__(self, x, class_idx=None):\\n\",\n",
    "    \"        # Forward pass\\n\",\n",
    "    \"        b, c, h, w = x.size()\\n\",\n",
    "    \"        logits = self.model(x)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # If class_idx is None, use the model's prediction\\n\",\n",
    "    \"        if class_idx is None:\\n\",\n",
    "    \"            if logits.dim() > 1:\\n\",\n",
    "    \"                class_idx = torch.argmax(logits, dim=1).item()\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                class_idx = (logits > 0.5).long().item()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Backward pass\\n\",\n",
    "    \"        self.model.zero_grad()\\n\",\n",
    "    \"        if logits.dim() > 1:\\n\",\n",
    "    \"            target = torch.zeros_like(logits)\\n\",\n",
    "    \"            target[0, class_idx] = 1\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            target = torch.ones_like(logits) if class_idx == 1 else torch.zeros_like(logits)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        logits.backward(gradient=target, retain_graph=True)\\n\","
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
