{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362155a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Deepfake Detection Demo\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides a demonstration of the deepfake detection system. It allows you to:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Load pretrained deepfake detection models\\n\",\n",
    "    \"2. Analyze single images for deepfake detection\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf6259",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Deepfake Detection Demo\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides a demonstration of the deepfake detection system. It allows you to:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Load pretrained deepfake detection models\\n\",\n",
    "    \"2. Analyze single images for deepfake detection\\n\",\n",
    "    \"3. Process videos frame-by-frame for deepfake detection\\n\",\n",
    "    \"4. Visualize model decisions and explain results\\n\",\n",
    "    \"5. Test with your own images or sample images\\n\",\n",
    "    \"\\n\",\n",
    "    \"The demo uses the transformer-based deepfake detection models (ViT, DeiT, and Swin) and includes visualization tools to help understand the decision-making process.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import necessary libraries\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"import argparse\\n\",\n",
    "    \"import ipywidgets as widgets\\n\",\n",
    "    \"from IPython.display import display, clear_output, HTML\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add parent directory to path for importing project modules\\n\",\n",
    "    \"sys.path.append(os.path.abspath('..'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import project modules\\n\",\n",
    "    \"from models.vit.model import ViT\\n\",\n",
    "    \"from models.deit.model import DeiT\\n\",\n",
    "    \"from models.swin.model import SwinTransformer\\n\",\n",
    "    \"from models.model_zoo.model_factory import create_model\\n\",\n",
    "    \"from data.preprocessing.face_extraction import setup_face_detector, extract_faces\\n\",\n",
    "    \"from data.preprocessing.normalization import normalize_face\\n\",\n",
    "    \"from evaluation.visualization.attention_maps import visualize_attention_maps\\n\",\n",
    "    \"from evaluation.visualization.grad_cam import visualize_grad_cam\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set device\\n\",\n",
    "    \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"print(f\\\"Using device: {device}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Pretrained Models\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, let's load the pretrained models for deepfake detection.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Configure paths - update these to your checkpoint paths\\n\",\n",
    "    \"CHECKPOINT_DIR = \\\"../trained_models\\\"\\n\",\n",
    "    \"VIT_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"vit_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"DEIT_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"deit_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"SWIN_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"swin_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check if checkpoints exist\\n\",\n",
    "    \"vit_exists = os.path.exists(VIT_CHECKPOINT)\\n\",\n",
    "    \"deit_exists = os.path.exists(DEIT_CHECKPOINT)\\n\",\n",
    "    \"swin_exists = os.path.exists(SWIN_CHECKPOINT)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"ViT checkpoint exists: {vit_exists}\\\")\\n\",\n",
    "    \"print(f\\\"DeiT checkpoint exists: {deit_exists}\\\")\\n\",\n",
    "    \"print(f\\\"Swin checkpoint exists: {swin_exists}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def load_checkpoint(model, checkpoint_path, device):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load model from checkpoint\\\"\\\"\\\"\\n\",\n",
    "    \"    if not os.path.exists(checkpoint_path):\\n\",\n",
    "    \"        print(f\\\"Checkpoint not found at {checkpoint_path}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        checkpoint = torch.load(checkpoint_path, map_location=device)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Different checkpoint formats\\n\",\n",
    "    \"        if 'model' in checkpoint:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint['model'])\\n\",\n",
    "    \"        elif 'model_state_dict' in checkpoint:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint['model_state_dict'])\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        model = model.to(device)\\n\",\n",
    "    \"        model.eval()  # Set to evaluation mode\\n\",\n",
    "    \"        print(f\\\"Model loaded successfully from {checkpoint_path}\\\")\\n\",\n",
    "    \"        return model\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error loading checkpoint: {e}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize models\\n\",\n",
    "    \"models = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load ViT model\\n\",\n",
    "    \"if vit_exists:\\n\",\n",
    "    \"    vit_model = ViT(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=16,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=768,\\n\",\n",
    "    \"        depth=12,\\n\",\n",
    "    \"        num_heads=12\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    vit_model = load_checkpoint(vit_model, VIT_CHECKPOINT, device)\\n\",\n",
    "    \"    if vit_model is not None:\\n\",\n",
    "    \"        models['vit'] = vit_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load DeiT model\\n\",\n",
    "    \"if deit_exists:\\n\",\n",
    "    \"    deit_model = DeiT(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=16,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=768,\\n\",\n",
    "    \"        depth=12,\\n\",\n",
    "    \"        num_heads=12,\\n\",\n",
    "    \"        distillation=True\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    deit_model = load_checkpoint(deit_model, DEIT_CHECKPOINT, device)\\n\",\n",
    "    \"    if deit_model is not None:\\n\",\n",
    "    \"        models['deit'] = deit_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load Swin model\\n\",\n",
    "    \"if swin_exists:\\n\",\n",
    "    \"    swin_model = SwinTransformer(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=4,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=96,\\n\",\n",
    "    \"        depths=[2, 2, 6, 2],\\n\",\n",
    "    \"        num_heads=[3, 6, 12, 24],\\n\",\n",
    "    \"        window_size=7\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    swin_model = load_checkpoint(swin_model, SWIN_CHECKPOINT, device)\\n\",\n",
    "    \"    if swin_model is not None:\\n\",\n",
    "    \"        models['swin'] = swin_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Loaded {len(models)} models: {list(models.keys())}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Set up Face Detection\\n\",\n",
    "    \"\\n\",\n",
    "    \"We need to set up face detection to extract faces from images and videos.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Set up face detector\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    face_detector = setup_face_detector(device='cpu')\\n\",\n",
    "    \"    print(\\\"Face detector set up successfully.\\\")\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"Error setting up face detector: {e}\\\")\\n\",\n",
    "    \"    print(\\\"Please install the required dependencies: pip install facenet-pytorch opencv-python\\\")\\n\",\n",
    "    \"    face_detector = None\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Image Preprocessing Functions\\n\",\n",
    "    \"\\n\",\n",
    "    \"Define functions for preprocessing images for the deepfake detection models.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def preprocess_image(image_path, face_detector=None, target_size=224):\\n\",\n",
    "    \"    \\\"\\\"\\\"Preprocess an image for deepfake detection\\\"\\\"\\\"\\n\",\n",
    "    \"    # Load image\\n\",\n",
    "    \"    if isinstance(image_path, str):\\n\",\n",
    "    \"        # Load from file\\n\",\n",
    "    \"        if not os.path.exists(image_path):\\n\",\n",
    "    \"            print(f\\\"Image not found at {image_path}\\\")\\n\",\n",
    "    \"            return None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        img = cv2.imread(image_path)\\n\",\n",
    "    \"        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # Assume numpy array\\n\",\n",
    "    \"        img = image_path\\n\",\n",
    "    \"        if img.shape[2] == 3 and img.dtype == np.uint8:\\n\",\n",
    "    \"            # Likely BGR format from OpenCV\\n\",\n",
    "    \"            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Extract face if detector is provided\\n\",\n",
    "    \"    if face_detector is not None:\\n\",\n",
    "    \"        faces = extract_faces(img, face_detector)\\n\",\n",
    "    \"        if not faces:\\n\",\n",
    "    \"            print(\\\"No faces detected in the image\\\")\\n\",\n",
    "    \"            # Just use the whole image if no faces detected\\n\",\n",
    "    \"            faces = [img]\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # Use the whole image if no detector\\n\",\n",
    "    \"        faces = [img]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Process each face\\n\",\n",
    "    \"    processed_faces = []\\n\",\n",
    "    \"    for face in faces:\\n\",\n",
    "    \"        # Resize to target size\\n\",\n",
    "    \"        face_resized = cv2.resize(face, (target_size, target_size))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert to float and normalize\\n\",\n",
    "    \"        face_float = face_resized.astype(np.float32) / 255.0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Normalize with ImageNet mean and std\\n\",\n",
    "    \"        face_normalized = (face_float - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert to tensor and add batch dimension (C, H, W)\\n\",\n",
    "    \"        face_tensor = torch.from_numpy(face_normalized.transpose(2, 0, 1)).float()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        processed_faces.append({\\n\",\n",
    "    \"            'original': face,\\n\",\n",
    "    \"            'resized': face_resized,\\n\",\n",
    "    \"            'tensor': face_tensor\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return processed_faces\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Deepfake Detection Functions\\n\",\n",
    "    \"\\n\",\n",
    "    \"Define functions for performing deepfake detection on images.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def detect_deepfake(face_tensor, models, device):\\n\",\n",
    "    \"    \\\"\\\"\\\"Detect deepfake in a face image\\\"\\\"\\\"\\n\",\n",
    "    \"    # Check if we have models\\n\",\n",
    "    \"    if not models:\\n\",\n",
    "    \"        print(\\\"No models available for detection\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Move tensor to device\\n\",\n",
    "    \"    face_tensor = face_tensor.to(device).unsqueeze(0)  # Add batch dimension\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get predictions from each model\\n\",\n",
    "    \"    results = {}\\n\",\n",
    "    \"    for name, model in models.items():\\n\",\n",
    "    \"        with torch.no_grad():\\n\",\n",
    "    \"            # Forward pass\\n\",\n",
    "    \"            output = model(face_tensor)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Convert to probability\\n\",\n",
    "    \"            prob = torch.sigmoid(output).item()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Store result\\n\",\n",
    "    \"            results[name] = {\\n\",\n",
    "    \"                'probability': prob,\\n\",\n",
    "    \"                'prediction': 'Fake' if prob > 0.5 else 'Real',\\n\",\n",
    "    \"                'confidence': max(prob, 1 - prob)\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate ensemble prediction (simple averaging)\\n\",\n",
    "    \"    if len(results) > 1:\\n\",\n",
    "    \"        ensemble_prob = np.mean([r['probability'] for r in results.values()])\\n\",\n",
    "    \"        results['ensemble'] = {\\n\",\n",
    "    \"            'probability': ensemble_prob,\\n\",\n",
    "    \"            'prediction': 'Fake' if ensemble_prob > 0.5 else 'Real',\\n\",\n",
    "    \"            'confidence': max(ensemble_prob, 1 - ensemble_prob)\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return results\\n\",\n",
    "    \"\\n\",\n",
    "    \"def visualize_detection_results(face_dict, results):\\n\",\n",
    "    \"    \\\"\\\"\\\"Visualize deepfake detection results\\\"\\\"\\\"\\n\",\n",
    "    \"    # Check inputs\\n\",\n",
    "    \"    if face_dict is None or results is None:\\n\",\n",
    "    \"        print(\\\"No face or results to visualize\\\")\\n\",\n",
    "    \"        return\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Set up figure\\n\",\n",
    "    \"    fig = plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot original face\\n\",\n",
    "    \"    plt.subplot(1, 2, 1)\\n\",\n",
    "    \"    plt.imshow(face_dict['original'])\\n\",\n",
    "    \"    plt.title('Input Face')\\n\",\n",
    "    \"    plt.axis('off')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot detection results\\n\",\n",
    "    \"    plt.subplot(1, 2, 2)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create bar chart of fakeness probabilities\\n\",\n",
    "    \"    models = list(results.keys())\\n\",\n",
    "    \"    probs = [results[m]['probability'] for m in models]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Choose color based on prediction (green for real, red for fake)\\n\",\n",
    "    \"    colors = ['green' if results[m]['prediction'] == 'Real' else 'red' for m in models]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    bars = plt.barh(models, probs, color=colors)\\n\",\n",
    "    \"    plt.xlim(0, 1)\\n\",\n",
    "    \"    plt.xlabel('Probability of Fake')\\n\",\n",
    "    \"    plt.axvline(x=0.5, color='black', linestyle='--')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add value labels\\n\",\n",
    "    \"    for i, bar in enumerate(bars):\\n\",\n",
    "    \"        plt.text(probs[i] + 0.01, bar.get_y() + bar.get_height()/2, \\n\",\n",
    "    \"                f\\\"{probs[i]:.2f} ({results[models[i]]['prediction']})\\\", \\n\",\n",
    "    \"                va='center')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.grid(True, linestyle='--', alpha=0.7)\\n\",\n",
    "    \"    plt.title('Deepfake Detection Results')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Print summary\\n\",\n",
    "    \"    print(\\\"\\\\nDetection Summary:\\\")\\n\",\n",
    "    \"    for model, result in results.items():\\n\",\n",
    "    \"        print(f\\\"• {model}: {result['prediction']} with {result['confidence']*100:.1f}% confidence\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Overall verdict (ensemble or only model)\\n\",\n",
    "    \"    if 'ensemble' in results:\\n\",\n",
    "    \"        verdict = results['ensemble']['prediction']\\n\",\n",
    "    \"        confidence = results['ensemble']['confidence']\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # Use the only model available\\n\",\n",
    "    \"        model = list(results.keys())[0]\\n\",\n",
    "    \"        verdict = results[model]['prediction']\\n\",\n",
    "    \"        confidence = results[model]['confidence']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Print overall verdict with confidence level text\\n\",\n",
    "    \"    confidence_level = \\\"high\\\" if confidence > 0.8 else \\\"moderate\\\" if confidence > 0.6 else \\\"low\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\nOverall verdict: Image is {verdict} with {confidence_level} confidence ({confidence*100:.1f}%)\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return fig\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Explanation Functions\\n\",\n",
    "    \"\\n\",\n",
    "    \"Define functions for explaining model decisions.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def find_target_layer(model, model_type):\\n\",\n",
    "    \"    \\\"\\\"\\\"Find the target layer for Grad-CAM based on model type\\\"\\\"\\\"\\n\",\n",
    "    \"    if model_type == 'vit':\\n\",\n",
    "    \"        # For ViT, we use the output of the last transformer block\\n\",\n",
    "    \"        return model.blocks[-1]\\n\",\n",
    "    \"    elif model_type == 'deit':\\n\",\n",
    "    \"        # For DeiT, we use the output of the last transformer block\\n\",\n",
    "    \"        return model.blocks[-1]\\n\",\n",
    "    \"    elif model_type == 'swin':\\n\",\n",
    "    \"        # For Swin, we use the output of the last layer\\n\",\n",
    "    \"        return model.layers[-1]\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        raise ValueError(f\\\"Unknown model type: {model_type}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"class GradCAM:\\n\",\n",
    "    \"    \\\"\\\"\\\"Grad-CAM implementation for transformer models\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __init__(self, model, target_layer):\\n\",\n",
    "    \"        self.model = model\\n\",\n",
    "    \"        self.target_layer = target_layer\\n\",\n",
    "    \"        self.gradients = None\\n\",\n",
    "    \"        self.activations = None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Register hooks\\n\",\n",
    "    \"        self.register_hooks()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def register_hooks(self):\\n\",\n",
    "    \"        def forward_hook(module, input, output):\\n\",\n",
    "    \"            self.activations = output.detach()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        def backward_hook(module, grad_input, grad_output):\\n\",\n",
    "    \"            self.gradients = grad_output[0].detach()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Register hooks\\n\",\n",
    "    \"        self.target_layer.register_forward_hook(forward_hook)\\n\",\n",
    "    \"        self.target_layer.register_backward_hook(backward_hook)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __call__(self, x, class_idx=None):\\n\",\n",
    "    \"        # Forward pass\\n\",\n",
    "    \"        b, c, h, w = x.size()\\n\",\n",
    "    \"        logits = self.model(x)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # If class_idx is None, use the model's prediction\\n\",\n",
    "    \"        if class_idx is None:\\n\",\n",
    "    \"            if logits.dim() > 1 and logits.shape[1] > 1:\\n\",\n",
    "    \"                class_idx = torch.argmax(logits, dim=1).item()\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                class_idx = (logits > 0.5).long().item()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Backward pass\\n\",\n",
    "    \"        self.model.zero_grad()\\n\",\n",
    "    \"        if logits.dim() > 1 and logits.shape[1] > 1:\\n\",\n",
    "    \"            target = torch.zeros_like(logits)\\n\",\n",
    "    \"            target[0, class_idx] = 1\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            target = torch.ones_like(logits) if class_idx == 1 else torch.zeros_like(logits)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        logits.backward(gradient=target, retain_graph=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Special handling for transformer models\\n\",\n",
    "    \"        if hasattr(self.model, 'blocks'):\\n\",\n",
    "    \"            # For ViT/DeiT, reshape activations and gradients to match image size\\n\",\n",
    "    \"            if self.activations.dim() == 3:  # [B, L, D]\\n\",\n",
    "    \"                # Skip class token\\n\",\n",
    "    \"                activations = self.activations[:, 1:, :]\\n\",\n",
    "    \"                gradients = self.gradients[:, 1:, :]\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Calculate patch size\\n\",\n",
    "    \"                patch_size = int(np.sqrt(w // int(np.sqrt(activations.shape[1]))))\\n\",\n",
    "    \"                num_patches = int(np.sqrt(activations.shape[1]))\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Reshape to [B, H, W, D]\\n\",\n",
    "    \"                activations = activations.reshape(b, num_patches, num_patches, -1)\\n\",\n",
    "    \"                gradients = gradients.reshape(b, num_patches, num_patches, -1)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Get weights (average over patch dimensions)\\n\",\n",
    "    \"                weights = gradients.mean(dim=(1, 2)).unsqueeze(-1).unsqueeze(-1)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Compute weighted activation map\\n\",\n",
    "    \"                cam = (weights * activations).sum(dim=3)[0].detach().cpu().numpy()\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Resize to original image size\\n\",\n",
    "    \"                cam = cv2.resize(cam, (w, h))\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                # Fallback for unsupported format\\n\",\n",
    "    \"                cam = np.zeros((h, w), dtype=np.float32)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            # Traditional CNN approach\\n\",\n",
    "    \"            weights = self.gradients.mean(dim=(2, 3))[0]\\n\",\n",
    "    \"            activations = self.activations[0]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Compute weighted activation map\\n\",\n",
    "    \"            cam = torch.zeros(activations.shape[1:], dtype=torch.float32, device=x.device)\\n\",\n",
    "    \"            for i, w in enumerate(weights):\\n\",\n",
    "    \"                cam += w * activations[i]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            cam = cam.detach().cpu().numpy()\\n\",\n",
    "    \"            cam = cv2.resize(cam, (w, h))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Apply ReLU and normalize\\n\",\n",
    "    \"        cam = np.maximum(cam, 0)\\n\",\n",
    "    \"        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return cam\\n\",\n",
    "    \"\\n\",\n",
    "    \"def get_attention_maps(model, img_tensor, model_type):\\n\",\n",
    "    \"    \\\"\\\"\\\"Get attention maps for visualization\\\"\\\"\\\"\\n\",\n",
    "    \"    # Check if model type is supported\\n\",\n",
    "    \"    if model_type not in ['vit', 'deit']:\\n\",\n",
    "    \"        print(f\\\"Attention map visualization not supported for {model_type}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Move image to device\\n\",\n",
    "    \"    img_tensor = img_tensor.to(device).unsqueeze(0)  # Add batch dimension\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Initialize list to store attention maps\\n\",\n",
    "    \"    attention_maps = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Hook function to extract attention maps\\n\",\n",
    "    \"    def attention_hook(module, input, output):\\n\",\n",
    "    \"        attention_maps.append(output.detach().cpu())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Register hooks for attention layers\\n\",\n",
    "    \"    hooks = []\\n\",\n",
    "    \"    for block in model.blocks:\\n\",\n",
    "    \"        hooks.append(block.attn.register_forward_hook(attention_hook))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Forward pass\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        model(img_tensor)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Remove hooks\\n\",\n",
    "    \"    for hook in hooks:\\n\",\n",
    "    \"        hook.remove()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return attention_maps\\n\",\n",
    "    \"\\n\",\n",
    "    \"def explain_prediction(face_dict, model_name, model, device):\\n\",\n",
    "    \"    \\\"\\\"\\\"Explain model prediction using visualization techniques\\\"\\\"\\\"\\n\",\n",
    "    \"    # Get face tensor\\n\",\n",
    "    \"    face_tensor = face_dict['tensor'].to(device).unsqueeze(0)  # Add batch dimension\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get prediction\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        output = model(face_tensor)\\n\",\n",
    "    \"        prob = torch.sigmoid(output).item()\\n\",\n",
    "    \"        pred = 'Fake' if prob > 0.5 else 'Real'\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Set up figure\\n\",\n",
    "    \"    fig = plt.figure(figsize=(15, 10))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. Original image\\n\",\n",
    "    \"    plt.subplot(2, 3, 1)\\n\",\n",
    "    \"    plt.imshow(face_dict['original'])\\n\",\n",
    "    \"    plt.title(f'Original Image\\\\nPrediction: {pred} ({prob:.2f})')\\n\",\n",
    "    \"    plt.axis('off')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 2. Grad-CAM visualization\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        target_layer = find_target_layer(model, model_name)\\n\",\n",
    "    \"        grad_cam = GradCAM(model, target_layer)\\n\",\n",
    "    \"        cam = grad_cam(face_tensor)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert to heatmap\\n\",\n",
    "    \"        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\\n\",\n",
    "    \"        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Resize to match image size\\n\",\n",
    "    \"        heatmap = cv2.resize(heatmap, (face_dict['original'].shape[1], face_dict['original'].shape[0]))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Overlay heatmap on image\\n\",\n",
    "    \"        superimposed = heatmap * 0.4 + face_dict['original'] * 0.6\\n\",\n",
    "    \"        superimposed = np.clip(superimposed, 0, 255).astype(np.uint8)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.subplot(2, 3, 2)\\n\",\n",
    "    \"        plt.imshow(heatmap)\\n\",\n",
    "    \"        plt.title('Grad-CAM Heatmap')\\n\",\n",
    "    \"        plt.axis('off')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.subplot(2, 3, 3)\\n\",\n",
    "    \"        plt.imshow(superimposed)\\n\",\n",
    "    \"        plt.title('Grad-CAM Overlay')\\n\",\n",
    "    \"        plt.axis('off')\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error generating Grad-CAM: {e}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 3. Attention map visualization (for ViT/DeiT)\\n\",\n",
    "    \"    if model_name in ['vit', 'deit']:\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            attention_maps = get_attention_maps(model, face_dict['tensor'], model_name)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if attention_maps and len(attention_maps) > 0:\\n\",\n",
    "    \"                # Use the last attention map\\n\",\n",
    "    \"                attn_map = attention_maps[-1][0]  # First batch\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Average over heads\\n\",\n",
    "    \"                avg_attn = attn_{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Deepfake Detection Demo\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides a demonstration of the deepfake detection system. It allows you to:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Load pretrained deepfake detection models\\n\",\n",
    "    \"2. Analyze single images for deepfake detection\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc16189",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def download_sample_images():\n",
    "    \\\"\\\"\\\"Download sample images for testing\\\"\\\"\\\"\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    from io import BytesIO\n",
    "    \n",
    "    # Create a directory for sample images\n",
    "    os.makedirs('sample_images', exist_ok=True)\n",
    "    \n",
    "    # Sample image URLs (replace these with your own samples or use a dataset)\n",
    "    sample_urls = {\n",
    "        'real_1': 'https://raw.githubusercontent.com/ondyari/FaceForensics/master/dataset/sample_images/original.png',\n",
    "        'fake_1': 'https://raw.githubusercontent.com/ondyari/FaceForensics/master/dataset/sample_images/manipulated.png'\n",
    "    }\n",
    "    \n",
    "    # Download and save samples\n",
    "    for name, url in sample_urls.items():\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                img_path = f'sample_images/{name}.png'\n",
    "                img.save(img_path)\n",
    "                print(f\"Downloaded {name} to {img_path}\")\n",
    "            else:\n",
    "                print(f\"Failed to download {name}: HTTP {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {name}: {e}\")\n",
    "    \n",
    "    print(\"\\nSample images downloaded. You can use these for testing the detection system.\")\n",
    "\n",
    "# Create download button\n",
    "download_button = widgets.Button(\n",
    "    description='Download Samples',\n",
    "    disabled=False,\n",
    "    button_style='info',\n",
    "    tooltip='Click to download sample images',\n",
    "    icon='download'\n",
    ")\n",
    "\n",
    "# Define button callback\n",
    "def on_download_button_clicked(b):\n",
    "    download_sample_images()\n",
    "\n",
    "# Register callback\n",
    "download_button.on_click(on_download_button_clicked)\n",
    "\n",
    "# Display button\n",
    "display(download_button)\n",
    "\n",
    "# Display existing samples if they exist\n",
    "if os.path.exists('sample_images'):\n",
    "    sample_files = [f for f in os.listdir('sample_images') if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if sample_files:\n",
    "        print(\"Existing sample images:\")\n",
    "        for file in sample_files:\n",
    "            print(f\"• {file}\")\n",
    "\n",
    "## 9. Conclusion and Further Resources\n",
    "\n",
    "print(\"\"\"\n",
    "# Conclusion\n",
    "\n",
    "This demo demonstrates the use of transformer-based models for deepfake detection. The system supports:\n",
    "\n",
    "- Single image analysis with multiple models\n",
    "- Video analysis with frame-by-frame detection\n",
    "- Visual explanations using Grad-CAM and attention maps\n",
    "- Ensemble predictions by combining multiple models\n",
    "\n",
    "# Further Resources\n",
    "\n",
    "To learn more about deepfake detection:\n",
    "\n",
    "1. FaceForensics++ dataset: https://github.com/ondyari/FaceForensics\n",
    "2. Celeb-DF dataset: https://github.com/yuezunli/celeb-deepfakeforensics\n",
    "3. Vision Transformers (ViT): https://arxiv.org/abs/2010.11929\n",
    "4. Data-efficient Image Transformers (DeiT): https://arxiv.org/abs/2012.12877\n",
    "5. Swin Transformer: https://arxiv.org/abs/2103.14030\n",
    "6. Grad-CAM: https://arxiv.org/abs/1610.02391\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "- Train models on additional datasets for better generalization\n",
    "- Implement temporal analysis for video detection\n",
    "- Explore multi-modal approaches combining image and audio analysis\n",
    "- Deploy the system as a web service or mobile application\n",
    "\"\"\")\n",
    "## 7. Run Demo\n",
    "\n",
    "# Create tabs for image and video detection\n",
    "tab_titles = ['Image Detection', 'Video Detection']\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = [create_image_detection_interface(), create_video_detection_interface()]\n",
    "\n",
    "# Set tab titles\n",
    "for i, title in enumerate(tab_titles):\n",
    "    tabs.set_title(i, title)\n",
    "\n",
    "# Display tabs\n",
    "display(tabs)\n",
    "\n",
    "print(\"\\nDeepfake Detection Demo is ready to use!\")\n",
    "print(\"1. Upload an image or video file\")\n",
    "print(\"2. Select model(s) to use for detection\")\n",
    "print(\"3. Click 'Run Detection' to analyze the content\")\n",
    "print(\"4. For images, toggle 'Show Explanation' to visualize how the model made its decision\")\n",
    "print(\"\\nNote: Face detection is a required step. If no faces are detected, the system will use the whole image.\")def create_image_detection_interface():\n",
    "    \\\"\\\"\\\"Create a demo interface for image-based deepfake detection\\\"\\\"\\\"\n",
    "    # Create file upload widget\n",
    "    file_upload = widgets.FileUpload(\n",
    "        accept='image/*',\n",
    "        multiple=False,\n",
    "        description='Upload Image:'\n",
    "    )\n",
    "    \n",
    "    # Create model selection widget\n",
    "    model_options = list(models.keys())\n",
    "    if 'ensemble' not in model_options and len(model_options) > 1:\n",
    "        model_options.append('ensemble')\n",
    "    \n",
    "    model_select = widgets.SelectMultiple(\n",
    "        options=model_options,\n",
    "        value=model_options,\n",
    "        description='Select Models:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    # Create explanation toggle\n",
    "    explain_toggle = widgets.Checkbox(\n",
    "        value=False,\n",
    "        description='Show Explanation',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    # Create run button\n",
    "    run_button = widgets.Button(\n",
    "        description='Run Detection',\n",
    "        disabled=False,\n",
    "        button_style='success',\n",
    "        tooltip='Click to run detection',\n",
    "        icon='check'\n",
    "    )\n",
    "    \n",
    "    # Create output widget\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Define run button callback\n",
    "    def on_run_button_clicked(b):\n",
    "        # Clear previous output\n",
    "        output.clear_output()\n",
    "        \n",
    "        with output:\n",
    "            # Check if file is uploaded\n",
    "            if not file_upload.value:\n",
    "                print(\"Please upload an image first\")\n",
    "                return\n",
    "            \n",
    "            # Check if models are selected\n",
    "            if not model_select.value:\n",
    "                print(\"Please select at least one model\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                # Get uploaded file\n",
    "                uploaded_file = list(file_upload.value.values())[0]\n",
    "                content = uploaded_file['content']\n",
    "                \n",
    "                # Convert content to numpy array\n",
    "                img_array = np.frombuffer(content, dtype=np.uint8)\n",
    "                img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                print(f\"Processing image: {uploaded_file['metadata']['name']} ({img.shape[1]}x{img.shape[0]})\")\n",
    "                \n",
    "                # Preprocess image\n",
    "                faces = preprocess_image(img, face_detector)\n",
    "                \n",
    "                if not faces:\n",
    "                    print(\"No faces detected or error in processing image\")\n",
    "                    return\n",
    "                \n",
    "                # For simplicity, use the first face\n",
    "                face = faces[0]\n",
    "                \n",
    "                # Get selected models\n",
    "                selected_models = {name: models[name] for name in model_select.value if name in models}\n",
    "                \n",
    "                # Detect deepfake\n",
    "                results = detect_deepfake(face['tensor'], selected_models, device)\n",
    "                \n",
    "                # Visualize results\n",
    "                visualize_detection_results(face, results)\n",
    "                \n",
    "                # Show explanation if toggled\n",
    "                if explain_toggle.value:\n",
    "                    print(\"\\nGenerating explanation visualizations...\")\n",
    "                    \n",
    "                    for name, model in selected_models.items():\n",
    "                        print(f\"\\nExplanation for {name.upper()} model:\")\n",
    "                        explain_prediction(face, name, model, device)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "    \n",
    "    # Register callback\n",
    "    run_button.on_click(on_run_button_clicked)\n",
    "    \n",
    "    # Arrange widgets\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([file_upload]), \n",
    "        widgets.HBox([model_select, explain_toggle]),\n",
    "        widgets.HBox([run_button]),\n",
    "        output\n",
    "    ])\n",
    "    \n",
    "    return ui\n",
    "\n",
    "def create_video_detection_interface():\n",
    "    \\\"\\\"\\\"Create a demo interface for video-based deepfake detection\\\"\\\"\\\"\n",
    "    # Create file upload widget\n",
    "    file_upload = widgets.FileUpload(\n",
    "        accept='video/*',\n",
    "        multiple=False,\n",
    "        description='Upload Video:'\n",
    "    )\n",
    "    \n",
    "    # Create model selection widget\n",
    "    model_options = list(models.keys())\n",
    "    if 'ensemble' not in model_options and len(model_options) > 1:\n",
    "        model_options.append('ensemble')\n",
    "    \n",
    "    model_select = widgets.Dropdown(\n",
    "        options=model_options,\n",
    "        value=model_options[0] if model_options else None,\n",
    "        description='Model:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    # Create sample rate slider\n",
    "    sample_rate = widgets.IntSlider(\n",
    "        value=30,\n",
    "        min=1,\n",
    "        max=100,\n",
    "        step=1,\n",
    "        description='Sample Rate:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d'\n",
    "    )\n",
    "    \n",
    "    # Create run button\n",
    "    run_button = widgets.Button(\n",
    "        description='Run Detection',\n",
    "        disabled=False,\n",
    "        button_style='success',\n",
    "        tooltip='Click to run detection',\n",
    "        icon='check'\n",
    "    )\n",
    "    \n",
    "    # Create output widget\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Define run button callback\n",
    "    def on_run_button_clicked(b):\n",
    "        # Clear previous output\n",
    "        output.clear_output()\n",
    "        \n",
    "        with output:\n",
    "            # Check if file is uploaded\n",
    "            if not file_upload.value:\n",
    "                print(\"Please upload a video first\")\n",
    "                return\n",
    "            \n",
    "            # Check if model is selected\n",
    "            if not model_select.value:\n",
    "                print(\"Please select a model\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                # Get uploaded file\n",
    "                uploaded_file = list(file_upload.value.values())[0]\n",
    "                content = uploaded_file['content']\n",
    "                \n",
    "                # Save video to temporary file\n",
    "                import tempfile\n",
    "                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')\n",
    "                temp_file.write(content)\n",
    "                temp_file.close()\n",
    "                \n",
    "                video_path = temp_file.name\n",
    "                print(f\"Processing video: {uploaded_file['metadata']['name']}\")\n",
    "                \n",
    "                # Open video\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                \n",
    "                # Check if video opened successfully\n",
    "                if not cap.isOpened():\n",
    "                    print(\"Error opening video file\")\n",
    "                    return\n",
    "                \n",
    "                # Get video properties\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "                \n",
    "                print(f\"Video properties: {frame_count} frames, {fps} fps\")\n",
    "                \n",
    "                # Calculate frames to sample\n",
    "                step = sample_rate.value\n",
    "                \n",
    "                # Initialize variables for results\n",
    "                frame_indices = []\n",
    "                predictions = []\n",
    "                probabilities = []\n",
    "                \n",
    "                # Process video\n",
    "                with tqdm(total=frame_count//step) as pbar:\n",
    "                    frame_idx = 0\n",
    "                    \n",
    "                    while cap.isOpened():\n",
    "                        ret, frame = cap.read()\n",
    "                        \n",
    "                        if not ret:\n",
    "                            break\n",
    "                        \n",
    "                        if frame_idx % step == 0:\n",
    "                            # Convert frame to RGB\n",
    "                            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                            \n",
    "                            # Preprocess frame\n",
    "                            faces = preprocess_image(frame_rgb, face_detector)\n",
    "                            \n",
    "                            if faces:\n",
    "                                # Use the first face\n",
    "                                face = faces[0]\n",
    "                                \n",
    "                                # Get selected model\n",
    "                                if model_select.value == 'ensemble':\n",
    "                                    selected_models = models\n",
    "                                else:\n",
    "                                    selected_models = {model_select.value: models[model_select.value]}\n",
    "                                \n",
    "                                # Detect deepfake\n",
    "                                results = detect_deepfake(face['tensor'], selected_models, device)\n",
    "                                \n",
    "                                # Store results\n",
    "                                if 'ensemble' in results:\n",
    "                                    pred = results['ensemble']['prediction']\n",
    "                                    prob = results['ensemble']['probability']\n",
    "                                else:\n",
    "                                    model_name = list(results.keys())[0]\n",
    "                                    pred = results[model_name]['prediction']\n",
    "                                    prob = results[model_name]['probability']\n",
    "                                \n",
    "                                frame_indices.append(frame_idx)\n",
    "                                predictions.append(pred)\n",
    "                                probabilities.append(prob)\n",
    "                            \n",
    "                            pbar.update(1)\n",
    "                        \n",
    "                        frame_idx += 1\n",
    "                \n",
    "                # Release video\n",
    "                cap.release()\n",
    "                \n",
    "                # Remove temporary file\n",
    "                os.unlink(video_path)\n",
    "                \n",
    "                # Visualize results\n",
    "                if frame_indices:\n",
    "                    # Convert probabilities for plotting (0 = Real, 1 = Fake)\n",
    "                    prob_values = [p if pred == 'Fake' else 1-p for p, pred in zip(probabilities, predictions)]\n",
    "                    \n",
    "                    # Create figure\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    \n",
    "                    # Plot timeline with colored points\n",
    "                    colors = ['green' if pred == 'Real' else 'red' for pred in predictions]\n",
    "                    plt.scatter(frame_indices, prob_values, c=colors, alpha=0.7)\n",
    "                    \n",
    "                    # Connect points with line\n",
    "                    plt.plot(frame_indices, prob_values, 'k--', alpha=0.3)\n",
    "                    \n",
    "                    # Add threshold line\n",
    "                    plt.axhline(y=0.5, color='blue', linestyle='--', alpha=0.5)\n",
    "                    \n",
    "                    # Add labels\n",
    "                    plt.xlabel('Frame Number')\n",
    "                    plt.ylabel('Fake Probability')\n",
    "                    plt.title('Deepfake Detection Results Across Video Frames')\n",
    "                    \n",
    "                    # Add legend\n",
    "                    from matplotlib.lines import Line2D\n",
    "                    legend_elements = [\n",
    "                        Line2D([0], [0], marker='o', color='w', markerfacecolor='green', label='Real', markersize=10),\n",
    "                        Line2D([0], [0], marker='o', color='w', markerfacecolor='red', label='Fake', markersize=10)\n",
    "                    ]\n",
    "                    plt.legend(handles=legend_elements)\n",
    "                    \n",
    "                    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Calculate summary statistics\n",
    "                    real_count = predictions.count('Real')\n",
    "                    fake_count = predictions.count('Fake')\n",
    "                    total_count = len(predictions)\n",
    "                    \n",
    "                    real_percent = real_count / total_count * 100\n",
    "                    fake_percent = fake_count / total_count * 100\n",
    "                    \n",
    "                    print(\"\\nVIDEO ANALYSIS SUMMARY:\")\n",
    "                    print(f\"Processed {total_count} frames from the video\")\n",
    "                    print(f\"• Real frames: {real_count} ({real_percent:.1f}%)\")\n",
    "                    print(f\"• Fake frames: {fake_count} ({fake_percent:.1f}%)\")\n",
    "                    \n",
    "                    # Overall verdict\n",
    "                    if fake_percent > 70:\n",
    "                        verdict = \"FAKE\"\n",
    "                        confidence = \"high\"\n",
    "                    elif fake_percent > 40:\n",
    "                        verdict = \"FAKE\"\n",
    "                        confidence = \"moderate\"\n",
    "                    elif fake_percent > 20:\n",
    "                        verdict = \"SUSPICIOUS\"\n",
    "                        confidence = \"low\"\n",
    "                    else:\n",
    "                        verdict = \"REAL\"\n",
    "                        confidence = \"high\" if real_percent > 80 else \"moderate\"\n",
    "                    \n",
    "                    print(f\"\\nOverall verdict: Video is likely {verdict} with {confidence} confidence\")\n",
    "                else:\n",
    "                    print(\"No faces detected in the sampled frames\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    # Register callback\n",
    "    run_button.on_click(on_run_button_clicked)\n",
    "    \n",
    "    # Arrange widgets\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([file_upload]), \n",
    "        widgets.HBox([model_select, sample_rate]),\n",
    "        widgets.HBox([run_button]),\n",
    "        output\n",
    "    ])\n",
    "    \n",
    "    return ui{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Deepfake Detection Demo\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides a demonstration of the deepfake detection system. It allows you to:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Load pretrained deepfake detection models\\n\",\n",
    "    \"2. Analyze single images for deepfake detection\\n\",\n",
    "    \"3. Process videos frame-by-frame for deepfake detection\\n\",\n",
    "    \"4. Visualize model decisions and explain results\\n\",\n",
    "    \"5. Test with your own images or sample images\\n\",\n",
    "    \"\\n\",\n",
    "    \"The demo uses the transformer-based deepfake detection models (ViT, DeiT, and Swin) and includes visualization tools to help understand the decision-making process.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import necessary libraries\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"import argparse\\n\",\n",
    "    \"import ipywidgets as widgets\\n\",\n",
    "    \"from IPython.display import display, clear_output, HTML\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add parent directory to path for importing project modules\\n\",\n",
    "    \"sys.path.append(os.path.abspath('..'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import project modules\\n\",\n",
    "    \"from models.vit.model import ViT\\n\",\n",
    "    \"from models.deit.model import DeiT\\n\",\n",
    "    \"from models.swin.model import SwinTransformer\\n\",\n",
    "    \"from models.model_zoo.model_factory import create_model\\n\",\n",
    "    \"from data.preprocessing.face_extraction import setup_face_detector, extract_faces\\n\",\n",
    "    \"from data.preprocessing.normalization import normalize_face\\n\",\n",
    "    \"from evaluation.visualization.attention_maps import visualize_attention_maps\\n\",\n",
    "    \"from evaluation.visualization.grad_cam import visualize_grad_cam\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set device\\n\",\n",
    "    \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"print(f\\\"Using device: {device}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Pretrained Models\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, let's load the pretrained models for deepfake detection.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Configure paths - update these to your checkpoint paths\\n\",\n",
    "    \"CHECKPOINT_DIR = \\\"../trained_models\\\"\\n\",\n",
    "    \"VIT_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"vit_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"DEIT_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"deit_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"SWIN_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \\\"swin_celebdf/checkpoints/best.pth\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check if checkpoints exist\\n\",\n",
    "    \"vit_exists = os.path.exists(VIT_CHECKPOINT)\\n\",\n",
    "    \"deit_exists = os.path.exists(DEIT_CHECKPOINT)\\n\",\n",
    "    \"swin_exists = os.path.exists(SWIN_CHECKPOINT)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"ViT checkpoint exists: {vit_exists}\\\")\\n\",\n",
    "    \"print(f\\\"DeiT checkpoint exists: {deit_exists}\\\")\\n\",\n",
    "    \"print(f\\\"Swin checkpoint exists: {swin_exists}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def load_checkpoint(model, checkpoint_path, device):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load model from checkpoint\\\"\\\"\\\"\\n\",\n",
    "    \"    if not os.path.exists(checkpoint_path):\\n\",\n",
    "    \"        print(f\\\"Checkpoint not found at {checkpoint_path}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        checkpoint = torch.load(checkpoint_path, map_location=device)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Different checkpoint formats\\n\",\n",
    "    \"        if 'model' in checkpoint:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint['model'])\\n\",\n",
    "    \"        elif 'model_state_dict' in checkpoint:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint['model_state_dict'])\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            model.load_state_dict(checkpoint)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        model = model.to(device)\\n\",\n",
    "    \"        model.eval()  # Set to evaluation mode\\n\",\n",
    "    \"        print(f\\\"Model loaded successfully from {checkpoint_path}\\\")\\n\",\n",
    "    \"        return model\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error loading checkpoint: {e}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Initialize models\\n\",\n",
    "    \"models = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load ViT model\\n\",\n",
    "    \"if vit_exists:\\n\",\n",
    "    \"    vit_model = ViT(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=16,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=768,\\n\",\n",
    "    \"        depth=12,\\n\",\n",
    "    \"        num_heads=12\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    vit_model = load_checkpoint(vit_model, VIT_CHECKPOINT, device)\\n\",\n",
    "    \"    if vit_model is not None:\\n\",\n",
    "    \"        models['vit'] = vit_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load DeiT model\\n\",\n",
    "    \"if deit_exists:\\n\",\n",
    "    \"    deit_model = DeiT(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=16,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=768,\\n\",\n",
    "    \"        depth=12,\\n\",\n",
    "    \"        num_heads=12,\\n\",\n",
    "    \"        distillation=True\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    deit_model = load_checkpoint(deit_model, DEIT_CHECKPOINT, device)\\n\",\n",
    "    \"    if deit_model is not None:\\n\",\n",
    "    \"        models['deit'] = deit_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load Swin model\\n\",\n",
    "    \"if swin_exists:\\n\",\n",
    "    \"    swin_model = SwinTransformer(\\n\",\n",
    "    \"        img_size=224,\\n\",\n",
    "    \"        patch_size=4,\\n\",\n",
    "    \"        in_channels=3,\\n\",\n",
    "    \"        num_classes=1,\\n\",\n",
    "    \"        embed_dim=96,\\n\",\n",
    "    \"        depths=[2, 2, 6, 2],\\n\",\n",
    "    \"        num_heads=[3, 6, 12, 24],\\n\",\n",
    "    \"        window_size=7\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    swin_model = load_checkpoint(swin_model, SWIN_CHECKPOINT, device)\\n\",\n",
    "    \"    if swin_model is not None:\\n\",\n",
    "    \"        models['swin'] = swin_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Loaded {len(models)} models: {list(models.keys())}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Set up Face Detection\\n\",\n",
    "    \"\\n\",\n",
    "    \"We need to set up face detection to extract faces from images and videos.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Set up face detector\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    face_detector = setup_face_detector(device='cpu')\\n\",\n",
    "    \"    print(\\\"Face detector set up successfully.\\\")\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    print(f\\\"Error setting up face detector: {e}\\\")\\n\",\n",
    "    \"    print(\\\"Please install the required dependencies: pip install facenet-pytorch opencv-python\\\")\\n\",\n",
    "    \"    face_detector = None\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Image Preprocessing Functions\\n\",\n",
    "    \"\\n\",\n",
    "    \"Define functions for preprocessing images for the deepfake detection models.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def preprocess_image(image_path, face_detector=None, target_size=224):\\n\",\n",
    "    \"    \\\"\\\"\\\"Preprocess an image for deepfake detection\\\"\\\"\\\"\\n\",\n",
    "    \"    # Load image\\n\",\n",
    "    \"    if isinstance(image_path, str):\\n\",\n",
    "    \"        # Load from file\\n\",\n",
    "    \"        if not os.path.exists(image_path):\\n\",\n",
    "    \"            print(f\\\"Image not found at {image_path}\\\")\\n\",\n",
    "    \"            return None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        img = cv2.imread(image_path)\\n\",\n",
    "    \"        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # Assume numpy array\\n\",\n",
    "    \"        img = image_path\\n\",\n",
    "    \"        if img.shape[2] == 3 and img.dtype == np.uint8:\\n\",\n",
    "    \"            # Likely BGR format from OpenCV\\n\",\n",
    "    \"            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Extract face if detector is provided\\n\",\n",
    "    \"    if face_detector is not None:\\n\",\n",
    "    \"        faces = extract_faces(img, face_detector)\\n\",\n",
    "    \"        if not faces:\\n\",\n",
    "    \"            print(\\\"No faces detected in the image\\\")\\n\",\n",
    "    \"            # Just use the whole image if no faces detected\\n\",\n",
    "    \"            faces = [img]\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # Use the whole image if no detector\\n\",\n",
    "    \"        faces = [img]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Process each face\\n\",\n",
    "    \"    processed_faces = []\\n\",\n",
    "    \"    for face in faces:\\n\",\n",
    "    \"        # Resize to target size\\n\",\n",
    "    \"        face_resized = cv2.resize(face, (target_size, target_size))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert to float and normalize\\n\",\n",
    "    \"        face_float = face_resized.astype(np.float32) / 255.0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Normalize with ImageNet mean and std\\n\",\n",
    "    \"        face_normalized = (face_float - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert to tensor and add batch dimension (C, H, W)\\n\",\n",
    "    \"        face_tensor = torch.from_numpy(face_normalized.transpose(2, 0, 1)).float()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        processed_faces.append({\\n\",\n",
    "    \"            'original': face,\\n\",\n",
    "    \"            'resized': face_resized,\\n\",\n",
    "    \"            'tensor': face_tensor\\n\",\n",
    "    \"        })\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return processed_faces\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Deepfake Detection Functions\\n\",\n",
    "    \"\\n\",\n",
    "    \"Define functions for performing deepfake detection on images.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def detect_deepfake(face_tensor, models, device):\\n\",\n",
    "    \"    \\\"\\\"\\\"Detect deepfake in a face image\\\"\\\"\\\"\\n\",\n",
    "    \"    # Check if we have models\\n\",\n",
    "    \"    if not models:\\n\",\n",
    "    \"        print(\\\"No models available for detection\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Move tensor to device\\n\",\n",
    "    \"    face_tensor = face_tensor.to(device).unsqueeze(0)  # Add batch dimension\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get predictions from each model\\n\",\n",
    "    \"    results = {}\\n\",\n",
    "    \"    for name, model in models.items():\\n\",\n",
    "    \"        with torch.no_grad():\\n\",\n",
    "    \"            # Forward pass\\n\",\n",
    "    \"            output = model(face_tensor)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Convert to probability\\n\",\n",
    "    \"            prob = torch.sigmoid(output).item()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Store result\\n\",\n",
    "    \"            results[name] = {\\n\",\n",
    "    \"                'probability': prob,\\n\",\n",
    "    \"                'prediction': 'Fake' if prob > 0.5 else 'Real',\\n\",\n",
    "    \"                'confidence': max(prob, 1 - prob)\\n\",\n",
    "    \"            }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate ensemble prediction (simple averaging)\\n\",\n",
    "    \"    if len(results) > 1:\\n\",\n",
    "    \"        ensemble_prob = np.mean([r['probability'] for r in results.values()])\\n\",\n",
    "    \"        results['ensemble'] = {\\n\",\n",
    "    \"            'probability': ensemble_prob,\\n\",\n",
    "    \"            'prediction': 'Fake' if ensemble_prob > 0.5 else 'Real',\\n\",\n",
    "    \"            'confidence': max(ensemble_prob, 1 - ensemble_prob)\\n\",\n",
    "    \"        }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return results\\n\",\n",
    "    \"\\n\",\n",
    "    \"def visualize_detection_results(face_dict, results):\\n\",\n",
    "    \"    \\\"\\\"\\\"Visualize deepfake detection results\\\"\\\"\\\"\\n\",\n",
    "    \"    # Check inputs\\n\",\n",
    "    \"    if face_dict is None or results is None:\\n\",\n",
    "    \"        print(\\\"No face or results to visualize\\\")\\n\",\n",
    "    \"        return\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Set up figure\\n\",\n",
    "    \"    fig = plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot original face\\n\",\n",
    "    \"    plt.subplot(1, 2, 1)\\n\",\n",
    "    \"    plt.imshow(face_dict['original'])\\n\",\n",
    "    \"    plt.title('Input Face')\\n\",\n",
    "    \"    plt.axis('off')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot detection results\\n\",\n",
    "    \"    plt.subplot(1, 2, 2)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create bar chart of fakeness probabilities\\n\",\n",
    "    \"    models = list(results.keys())\\n\",\n",
    "    \"    probs = [results[m]['probability'] for m in models]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Choose color based on prediction (green for real, red for fake)\\n\",\n",
    "    \"    colors = ['green' if results[m]['prediction'] == 'Real' else 'red' for m in models]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    bars = plt.barh(models, probs, color=colors)\\n\",\n",
    "    \"    plt.xlim(0, 1)\\n\",\n",
    "    \"    plt.xlabel('Probability of Fake')\\n\",\n",
    "    \"    plt.axvline(x=0.5, color='black', linestyle='--')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Add value labels\\n\",\n",
    "    \"    for i, bar in enumerate(bars):\\n\",\n",
    "    \"        plt.text(probs[i] + 0.01, bar.get_y() + bar.get_height()/2, \\n\",\n",
    "    \"                f\\\"{probs[i]:.2f} ({results[models[i]]['prediction']})\\\", \\n\",\n",
    "    \"                va='center')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.grid(True, linestyle='--', alpha=0.7)\\n\",\n",
    "    \"    plt.title('Deepfake Detection Results')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Print summary\\n\",\n",
    "    \"    print(\\\"\\\\nDetection Summary:\\\")\\n\",\n",
    "    \"    for model, result in results.items():\\n\",\n",
    "    \"        print(f\\\"• {model}: {result['prediction']} with {result['confidence']*100:.1f}% confidence\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Overall verdict (ensemble or only model)\\n\",\n",
    "    \"    if 'ensemble' in results:\\n\",\n",
    "    \"        verdict = results['ensemble']['prediction']\\n\",\n",
    "    \"        confidence = results['ensemble']['confidence']\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        # Use the only model available\\n\",\n",
    "    \"        model = list(results.keys())[0]\\n\",\n",
    "    \"        verdict = results[model]['prediction']\\n\",\n",
    "    \"        confidence = results[model]['confidence']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Print overall verdict with confidence level text\\n\",\n",
    "    \"    confidence_level = \\\"high\\\" if confidence > 0.8 else \\\"moderate\\\" if confidence > 0.6 else \\\"low\\\"\\n\",\n",
    "    \"    print(f\\\"\\\\nOverall verdict: Image is {verdict} with {confidence_level} confidence ({confidence*100:.1f}%)\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return fig\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Explanation Functions\\n\",\n",
    "    \"\\n\",\n",
    "    \"Define functions for explaining model decisions.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def find_target_layer(model, model_type):\\n\",\n",
    "    \"    \\\"\\\"\\\"Find the target layer for Grad-CAM based on model type\\\"\\\"\\\"\\n\",\n",
    "    \"    if model_type == 'vit':\\n\",\n",
    "    \"        # For ViT, we use the output of the last transformer block\\n\",\n",
    "    \"        return model.blocks[-1]\\n\",\n",
    "    \"    elif model_type == 'deit':\\n\",\n",
    "    \"        # For DeiT, we use the output of the last transformer block\\n\",\n",
    "    \"        return model.blocks[-1]\\n\",\n",
    "    \"    elif model_type == 'swin':\\n\",\n",
    "    \"        # For Swin, we use the output of the last layer\\n\",\n",
    "    \"        return model.layers[-1]\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        raise ValueError(f\\\"Unknown model type: {model_type}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"class GradCAM:\\n\",\n",
    "    \"    \\\"\\\"\\\"Grad-CAM implementation for transformer models\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __init__(self, model, target_layer):\\n\",\n",
    "    \"        self.model = model\\n\",\n",
    "    \"        self.target_layer = target_layer\\n\",\n",
    "    \"        self.gradients = None\\n\",\n",
    "    \"        self.activations = None\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Register hooks\\n\",\n",
    "    \"        self.register_hooks()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def register_hooks(self):\\n\",\n",
    "    \"        def forward_hook(module, input, output):\\n\",\n",
    "    \"            self.activations = output.detach()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        def backward_hook(module, grad_input, grad_output):\\n\",\n",
    "    \"            self.gradients = grad_output[0].detach()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Register hooks\\n\",\n",
    "    \"        self.target_layer.register_forward_hook(forward_hook)\\n\",\n",
    "    \"        self.target_layer.register_backward_hook(backward_hook)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __call__(self, x, class_idx=None):\\n\",\n",
    "    \"        # Forward pass\\n\",\n",
    "    \"        b, c, h, w = x.size()\\n\",\n",
    "    \"        logits = self.model(x)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # If class_idx is None, use the model's prediction\\n\",\n",
    "    \"        if class_idx is None:\\n\",\n",
    "    \"            if logits.dim() > 1 and logits.shape[1] > 1:\\n\",\n",
    "    \"                class_idx = torch.argmax(logits, dim=1).item()\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                class_idx = (logits > 0.5).long().item()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Backward pass\\n\",\n",
    "    \"        self.model.zero_grad()\\n\",\n",
    "    \"        if logits.dim() > 1 and logits.shape[1] > 1:\\n\",\n",
    "    \"            target = torch.zeros_like(logits)\\n\",\n",
    "    \"            target[0, class_idx] = 1\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            target = torch.ones_like(logits) if class_idx == 1 else torch.zeros_like(logits)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        logits.backward(gradient=target, retain_graph=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Special handling for transformer models\\n\",\n",
    "    \"        if hasattr(self.model, 'blocks'):\\n\",\n",
    "    \"            # For ViT/DeiT, reshape activations and gradients to match image size\\n\",\n",
    "    \"            if self.activations.dim() == 3:  # [B, L, D]\\n\",\n",
    "    \"                # Skip class token\\n\",\n",
    "    \"                activations = self.activations[:, 1:, :]\\n\",\n",
    "    \"                gradients = self.gradients[:, 1:, :]\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Calculate patch size\\n\",\n",
    "    \"                patch_size = int(np.sqrt(w // int(np.sqrt(activations.shape[1]))))\\n\",\n",
    "    \"                num_patches = int(np.sqrt(activations.shape[1]))\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Reshape to [B, H, W, D]\\n\",\n",
    "    \"                activations = activations.reshape(b, num_patches, num_patches, -1)\\n\",\n",
    "    \"                gradients = gradients.reshape(b, num_patches, num_patches, -1)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Get weights (average over patch dimensions)\\n\",\n",
    "    \"                weights = gradients.mean(dim=(1, 2)).unsqueeze(-1).unsqueeze(-1)\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Compute weighted activation map\\n\",\n",
    "    \"                cam = (weights * activations).sum(dim=3)[0].detach().cpu().numpy()\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Resize to original image size\\n\",\n",
    "    \"                cam = cv2.resize(cam, (w, h))\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                # Fallback for unsupported format\\n\",\n",
    "    \"                cam = np.zeros((h, w), dtype=np.float32)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            # Traditional CNN approach\\n\",\n",
    "    \"            weights = self.gradients.mean(dim=(2, 3))[0]\\n\",\n",
    "    \"            activations = self.activations[0]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Compute weighted activation map\\n\",\n",
    "    \"            cam = torch.zeros(activations.shape[1:], dtype=torch.float32, device=x.device)\\n\",\n",
    "    \"            for i, w in enumerate(weights):\\n\",\n",
    "    \"                cam += w * activations[i]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            cam = cam.detach().cpu().numpy()\\n\",\n",
    "    \"            cam = cv2.resize(cam, (w, h))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Apply ReLU and normalize\\n\",\n",
    "    \"        cam = np.maximum(cam, 0)\\n\",\n",
    "    \"        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return cam\\n\",\n",
    "    \"\\n\",\n",
    "    \"def get_attention_maps(model, img_tensor, model_type):\\n\",\n",
    "    \"    \\\"\\\"\\\"Get attention maps for visualization\\\"\\\"\\\"\\n\",\n",
    "    \"    # Check if model type is supported\\n\",\n",
    "    \"    if model_type not in ['vit', 'deit']:\\n\",\n",
    "    \"        print(f\\\"Attention map visualization not supported for {model_type}\\\")\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Move image to device\\n\",\n",
    "    \"    img_tensor = img_tensor.to(device).unsqueeze(0)  # Add batch dimension\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Initialize list to store attention maps\\n\",\n",
    "    \"    attention_maps = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Hook function to extract attention maps\\n\",\n",
    "    \"    def attention_hook(module, input, output):\\n\",\n",
    "    \"        attention_maps.append(output.detach().cpu())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Register hooks for attention layers\\n\",\n",
    "    \"    hooks = []\\n\",\n",
    "    \"    for block in model.blocks:\\n\",\n",
    "    \"        hooks.append(block.attn.register_forward_hook(attention_hook))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Forward pass\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        model(img_tensor)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Remove hooks\\n\",\n",
    "    \"    for hook in hooks:\\n\",\n",
    "    \"        hook.remove()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return attention_maps\\n\",\n",
    "    \"\\n\",\n",
    "    \"def explain_prediction(face_dict, model_name, model, device):\\n\",\n",
    "    \"    \\\"\\\"\\\"Explain model prediction using visualization techniques\\\"\\\"\\\"\\n\",\n",
    "    \"    # Get face tensor\\n\",\n",
    "    \"    face_tensor = face_dict['tensor'].to(device).unsqueeze(0)  # Add batch dimension\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get prediction\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        output = model(face_tensor)\\n\",\n",
    "    \"        prob = torch.sigmoid(output).item()\\n\",\n",
    "    \"        pred = 'Fake' if prob > 0.5 else 'Real'\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Set up figure\\n\",\n",
    "    \"    fig = plt.figure(figsize=(15, 10))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. Original image\\n\",\n",
    "    \"    plt.subplot(2, 3, 1)\\n\",\n",
    "    \"    plt.imshow(face_dict['original'])\\n\",\n",
    "    \"    plt.title(f'Original Image\\\\nPrediction: {pred} ({prob:.2f})')\\n\",\n",
    "    \"    plt.axis('off')\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 2. Grad-CAM visualization\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        target_layer = find_target_layer(model, model_name)\\n\",\n",
    "    \"        grad_cam = GradCAM(model, target_layer)\\n\",\n",
    "    \"        cam = grad_cam(face_tensor)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert to heatmap\\n\",\n",
    "    \"        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\\n\",\n",
    "    \"        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Resize to match image size\\n\",\n",
    "    \"        heatmap = cv2.resize(heatmap, (face_dict['original'].shape[1], face_dict['original'].shape[0]))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Overlay heatmap on image\\n\",\n",
    "    \"        superimposed = heatmap * 0.4 + face_dict['original'] * 0.6\\n\",\n",
    "    \"        superimposed = np.clip(superimposed, 0, 255).astype(np.uint8)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.subplot(2, 3, 2)\\n\",\n",
    "    \"        plt.imshow(heatmap)\\n\",\n",
    "    \"        plt.title('Grad-CAM Heatmap')\\n\",\n",
    "    \"        plt.axis('off')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.subplot(2, 3, 3)\\n\",\n",
    "    \"        plt.imshow(superimposed)\\n\",\n",
    "    \"        plt.title('Grad-CAM Overlay')\\n\",\n",
    "    \"        plt.axis('off')\\n\",\n",
    "    \"    except Exception as e:\\n\",\n",
    "    \"        print(f\\\"Error generating Grad-CAM: {e}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 3. Attention map visualization (for ViT/DeiT)\\n\",\n",
    "    \"    if model_name in ['vit', 'deit']:\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            attention_maps = get_attention_maps(model, face_dict['tensor'], model_name)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if attention_maps and len(attention_maps) > 0:\\n\",\n",
    "    \"                # Use the last attention map\\n\",\n",
    "    \"                attn_map = attention_maps[-1][0]  # First batch\\n\",\n",
    "    \"                \\n\",\n",
    "    \"                # Average over heads\\n\",\n",
    "    \"                avg_attn = attn_{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Deepfake Detection Demo\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides a demonstration of the deepfake detection system. It allows you to:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Load pretrained deepfake detection models\\n\",\n",
    "    \"2. Analyze single images for deepfake detection\\n\","
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
