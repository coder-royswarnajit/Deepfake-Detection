{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f290c8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Deepfake Detection Results Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides tools for analyzing the results of deepfake detection models. It includes:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Loading and visualizing evaluation metrics\\n\",\n",
    "    \"2. Comparative analysis of different models\\n\",\n",
    "    \"3. Error analysis and identification of challenging cases\\n\",\n",
    "    \"4. Cross-dataset generalization analysis\\n\",\n",
    "    \"5. ROC curves and precision-recall analysis\\n\",\n",
    "    \"6. Ensembling and fusion performance assessment\\n\",\n",
    "    \"\\n\",\n",
    "    \"These analyses help understand the strengths and weaknesses of different approaches and guide future improvements.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import necessary libraries\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import yaml\\n\",\n",
    "    \"import glob\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"from sklearn.metrics import roc_curve, precision_recall_curve, auc\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add parent directory to path to enable imports from project\\n\",\n",
    "    \"sys.path.append(os.path.abspath('..'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plot style\\n\",\n",
    "    \"plt.style.use('fivethirtyeight')\\n\",\n",
    "    \"sns.set(style=\\\"whitegrid\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Evaluation Results\\n\",\n",
    "    \"\\n\",\n",
    "    \"Load evaluation results from the evaluation output directory.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Set the evaluation results directory - update to your local path\\n\",\n",
    "    \"EVAL_DIR = \\\"../evaluation_results\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check if directory exists\\n\",\n",
    "    \"if not os.path.exists(EVAL_DIR):\\n\",\n",
    "    \"    print(f\\\"Evaluation directory {EVAL_DIR} does not exist. Please update the path.\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Found evaluation directory at {EVAL_DIR}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def load_evaluation_results(eval_dir):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load evaluation results from directory\\\"\\\"\\\"\\n\",\n",
    "    \"    if not os.path.exists(eval_dir):\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    results = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Find all model directories\\n\",\n",
    "    \"    model_dirs = [d for d in os.listdir(eval_dir) if os.path.isdir(os.path.join(eval_dir, d))]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for model_name in model_dirs:\\n\",\n",
    "    \"        model_path = os.path.join(eval_dir, model_name)\\n\",\n",
    "    \"        results_file = os.path.join(model_path, \\\"all_results.json\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if os.path.exists(results_file):\\n\",\n",
    "    \"            with open(results_file, 'r') as f:\\n\",\n",
    "    \"                model_results = json.load(f)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Store results\\n\",\n",
    "    \"            results[model_name] = model_results\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return results\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load results\\n\",\n",
    "    \"evaluation_results = load_evaluation_results(EVAL_DIR)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if evaluation_results is None:\\n\",\n",
    "    \"    print(\\\"No evaluation results found.\\\")\\n\",\n",
    "    \"elif not evaluation_results:\\n\",\n",
    "    \"    print(\\\"No models found in evaluation results.\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Loaded evaluation results for {len(evaluation_results)} models:\\\")\\n\",\n",
    "    \"    for model_name in evaluation_results.keys():\\n\",\n",
    "    \"        print(f\\\"  - {model_name}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def load_raw_predictions(eval_dir):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load raw predictions for detailed analysis\\\"\\\"\\\"\\n\",\n",
    "    \"    if not os.path.exists(eval_dir):\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    predictions = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Find all model directories\\n\",\n",
    "    \"    model_dirs = [d for d in os.listdir(eval_dir) if os.path.isdir(os.path.join(eval_dir, d))]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for model_name in model_dirs:\\n\",\n",
    "    \"        model_path = os.path.join(eval_dir, model_name)\\n\",\n",
    "    \"        model_predictions = {}\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Find dataset directories\\n\",\n",
    "    \"        dataset_dirs = [d for d in os.listdir(model_path) \\n\",\n",
    "    \"                       if os.path.isdir(os.path.join(model_path, d))]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for dataset_name in dataset_dirs:\\n\",\n",
    "    \"            dataset_path = os.path.join(model_path, dataset_name)\\n\",\n",
    "    \"            pred_file = os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212ecf8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 5. Cross-Dataset Generalization Analysis\n",
    "\n",
    "def analyze_cross_dataset_generalization(error_df):\n",
    "    \\\"\\\"\\\"Analyze how well models generalize across datasets\\\"\\\"\\\"\n",
    "    if error_df is None or error_df.empty:\n",
    "        print(\"No error data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    # Check if we have multiple datasets\n",
    "    if len(error_df['dataset'].unique()) < 2:\n",
    "        print(\"Need at least 2 datasets for cross-dataset analysis.\")\n",
    "        return\n",
    "    \n",
    "    # Create a list to store generalization data\n",
    "    generalization_data = []\n",
    "    \n",
    "    # For each model\n",
    "    for model in error_df['model'].unique():\n",
    "        model_data = error_df[error_df['model'] == model]\n",
    "        \n",
    "        # For each pair of datasets\n",
    "        for train_dataset in model_data['dataset'].unique():\n",
    "            train_perf = model_data[model_data['dataset'] == train_dataset]\n",
    "            \n",
    "            for test_dataset in model_data['dataset'].unique():\n",
    "                if train_dataset != test_dataset:\n",
    "                    test_perf = model_data[model_data['dataset'] == test_dataset]\n",
    "                    \n",
    "                    # Skip if missing data\n",
    "                    if train_perf.empty or test_perf.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate performance drops\n",
    "                    accuracy_drop = train_perf['accuracy'].values[0] - test_perf['accuracy'].values[0]\n",
    "                    real_acc_drop = train_perf['real_accuracy'].values[0] - test_perf['real_accuracy'].values[0]\n",
    "                    fake_acc_drop = train_perf['fake_accuracy'].values[0] - test_perf['fake_accuracy'].values[0]\n",
    "                    \n",
    "                    # Store results\n",
    "                    generalization_data.append({\n",
    "                        'model': model,\n",
    "                        'train_dataset': train_dataset,\n",
    "                        'test_dataset': test_dataset,\n",
    "                        'train_accuracy': train_perf['accuracy'].values[0],\n",
    "                        'test_accuracy': test_perf['accuracy'].values[0],\n",
    "                        'accuracy_drop': accuracy_drop,\n",
    "                        'real_acc_drop': real_acc_drop,\n",
    "                        'fake_acc_drop': fake_acc_drop\n",
    "                    })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if generalization_data:\n",
    "        gen_df = pd.DataFrame(generalization_data)\n",
    "        return gen_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "def visualize_cross_dataset_generalization(gen_df):\n",
    "    \\\"\\\"\\\"Visualize cross-dataset generalization results\\\"\\\"\\\"\n",
    "    if gen_df is None or gen_df.empty:\n",
    "        print(\"No generalization data to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Accuracy drop across datasets\n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='model', y='accuracy_drop', hue='test_dataset', data=gen_df, ax=ax1)\n",
    "    ax1.set_title('Accuracy Drop when Testing on Different Datasets')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Accuracy Drop')\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    ax1.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    # 2. Per-class accuracy drop\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    \n",
    "    # Melt the DataFrame\n",
    "    plot_df = pd.melt(gen_df, \n",
    "                      id_vars=['model', 'test_dataset'], \n",
    "                      value_vars=['real_acc_drop', 'fake_acc_drop'],\n",
    "                      var_name='Class', value_name='Accuracy Drop')\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='model', y='Accuracy Drop', hue='Class', data=plot_df, ax=ax2)\n",
    "    ax2.set_title('Per-class Accuracy Drop')\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('Accuracy Drop')\n",
    "    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    ax2.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    # 3. Train vs test accuracy scatter plot\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    \n",
    "    # Plot\n",
    "    sns.scatterplot(x='train_accuracy', y='test_accuracy', hue='model', \n",
    "                   style='test_dataset', s=100, data=gen_df, ax=ax3)\n",
    "    \n",
    "    # Plot diagonal line (x=y)\n",
    "    min_val = min(gen_df['train_accuracy'].min(), gen_df['test_accuracy'].min())\n",
    "    max_val = max(gen_df['train_accuracy'].max(), gen_df['test_accuracy'].max())\n",
    "    ax3.plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "    \n",
    "    ax3.set_title('Train vs Test Accuracy')\n",
    "    ax3.set_xlabel('Train Accuracy')\n",
    "    ax3.set_ylabel('Test Accuracy')\n",
    "    ax3.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 4. Generalization ranking\n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "    \n",
    "    # Calculate average accuracy drop for each model\n",
    "    model_drops = gen_df.groupby('model')['accuracy_drop'].mean().reset_index()\n",
    "    model_drops = model_drops.sort_values('accuracy_drop')\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='model', y='accuracy_drop', data=model_drops, ax=ax4)\n",
    "    ax4.set_title('Average Accuracy Drop (Lower is Better)')\n",
    "    ax4.set_xlabel('Model')\n",
    "    ax4.set_ylabel('Average Accuracy Drop')\n",
    "    ax4.set_xticklabels(ax4.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    ax4.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    # Add overall title\n",
    "    plt.suptitle('Cross-Dataset Generalization Analysis', fontsize=16)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    return model_drops\n",
    "\n",
    "# Analyze cross-dataset generalization\n",
    "if 'error_df' in locals() and error_df is not None and not error_df.empty:\n",
    "    gen_df = analyze_cross_dataset_generalization(error_df)## 4. Error Analysis\n",
    "\n",
    "def analyze_errors(raw_predictions):\n",
    "    \\\"\\\"\\\"Analyze error patterns in model predictions\\\"\\\"\\\"\n",
    "    if raw_predictions is None or not raw_predictions:\n",
    "        print(\"No predictions to analyze.\")\n",
    "        return\n",
    "    \n",
    "    # Create a DataFrame to store error analysis results\n",
    "    error_analysis = []\n",
    "    \n",
    "    for model_name, model_preds in raw_predictions.items():\n",
    "        for dataset_name, data in model_preds.items():\n",
    "            # Check if we have the necessary data\n",
    "            if 'labels' in data and 'predictions' in data:\n",
    "                labels = data['labels']\n",
    "                predictions = data['predictions']\n",
    "                \n",
    "                # Calculate error statistics\n",
    "                total = len(labels)\n",
    "                correct = (predictions == labels).sum()\n",
    "                incorrect = total - correct\n",
    "                accuracy = correct / total\n",
    "                \n",
    "                # Per-class statistics\n",
    "                real_samples = (labels == 0).sum()\n",
    "                fake_samples = (labels == 1).sum()\n",
    "                \n",
    "                real_correct = ((predictions == 0) & (labels == 0)).sum()\n",
    "                fake_correct = ((predictions == 1) & (labels == 1)).sum()\n",
    "                \n",
    "                real_to_fake = ((predictions == 1) & (labels == 0)).sum()\n",
    "                fake_to_real = ((predictions == 0) & (labels == 1)).sum()\n",
    "                \n",
    "                real_accuracy = real_correct / real_samples if real_samples > 0 else 0\n",
    "                fake_accuracy = fake_correct / fake_samples if fake_samples > 0 else 0\n",
    "                \n",
    "                # Store results\n",
    "                error_analysis.append({\n",
    "                    'model': model_name,\n",
    "                    'dataset': dataset_name,\n",
    "                    'total_samples': total,\n",
    "                    'accuracy': accuracy,\n",
    "                    'correct': correct,\n",
    "                    'incorrect': incorrect,\n",
    "                    'real_samples': real_samples,\n",
    "                    'fake_samples': fake_samples,\n",
    "                    'real_accuracy': real_accuracy,\n",
    "                    'fake_accuracy': fake_accuracy,\n",
    "                    'real_to_fake': real_to_fake,\n",
    "                    'fake_to_real': fake_to_real\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if error_analysis:\n",
    "        error_df = pd.DataFrame(error_analysis)\n",
    "        return error_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "def visualize_error_patterns(error_df):\n",
    "    \\\"\\\"\\\"Visualize error patterns from error analysis\\\"\\\"\\\"\n",
    "    if error_df is None or error_df.empty:\n",
    "        print(\"No error data to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Overall accuracy vs per-class accuracy\n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    \n",
    "    # Melt the DataFrame to get it in the right format for seaborn\n",
    "    plot_df = pd.melt(error_df, \n",
    "                      id_vars=['model', 'dataset'], \n",
    "                      value_vars=['accuracy', 'real_accuracy', 'fake_accuracy'],\n",
    "                      var_name='Metric', value_name='Value')\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='model', y='Value', hue='Metric', data=plot_df, ax=ax1)\n",
    "    ax1.set_title('Overall vs Per-Class Accuracy')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 2. Error distribution\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    \n",
    "    # Melt the DataFrame\n",
    "    plot_df = pd.melt(error_df, \n",
    "                      id_vars=['model', 'dataset'], \n",
    "                      value_vars=['real_to_fake', 'fake_to_real'],\n",
    "                      var_name='Error Type', value_name='Count')\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='model', y='Count', hue='Error Type', data=plot_df, ax=ax2)\n",
    "    ax2.set_title('Error Distribution')\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('Error Count')\n",
    "    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 3. Class balance\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    \n",
    "    # Calculate class balance\n",
    "    error_df['real_percent'] = error_df['real_samples'] / error_df['total_samples'] * 100\n",
    "    error_df['fake_percent'] = 100 - error_df['real_percent']\n",
    "    \n",
    "    # Melt the DataFrame\n",
    "    plot_df = pd.melt(error_df, \n",
    "                      id_vars=['dataset'], \n",
    "                      value_vars=['real_percent', 'fake_percent'],\n",
    "                      var_name='Class', value_name='Percentage')\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='dataset', y='Percentage', hue='Class', data=plot_df, ax=ax3)\n",
    "    ax3.set_title('Class Balance')\n",
    "    ax3.set_xlabel('Dataset')\n",
    "    ax3.set_ylabel('Percentage')\n",
    "    \n",
    "    # 4. Error rate vs class balance\n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "    \n",
    "    # Calculate error rates\n",
    "    error_df['real_error_rate'] = error_df['real_to_fake'] / error_df['real_samples']\n",
    "    error_df['fake_error_rate'] = error_df['fake_to_real'] / error_df['fake_samples']\n",
    "    \n",
    "    # Melt the DataFrame\n",
    "    plot_df = pd.melt(error_df, \n",
    "                      id_vars=['model', 'dataset', 'real_percent', 'fake_percent'], \n",
    "                      value_vars=['real_error_rate', 'fake_error_rate'],\n",
    "                      var_name='Error Type', value_name='Error Rate')\n",
    "    \n",
    "    # Add class percentage information\n",
    "    plot_df['class_percent'] = plot_df.apply(\n",
    "        lambda row: row['real_percent'] if row['Error Type'] == 'real_error_rate' else row['fake_percent'], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Plot\n",
    "    for model in plot_df['model'].unique():\n",
    "        model_df = plot_df[plot_df['model'] == model]\n",
    "        ax4.scatter(model_df['class_percent'], model_df['Error Rate'], label=model)\n",
    "    \n",
    "    ax4.set_title('Error Rate vs Class Percentage')\n",
    "    ax4.set_xlabel('Class Percentage')\n",
    "    ax4.set_ylabel('Error Rate')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add overall title\n",
    "    plt.suptitle('Error Analysis', fontsize=16)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Analyze errors\n",
    "if 'raw_predictions' in locals() and raw_predictions:\n",
    "    error_df = analyze_errors(raw_predictions)\n",
    "    \n",
    "    if error_df is not None:\n",
    "        print(\"\\nError Analysis:\")\n",
    "        display(error_df)\n",
    "        \n",
    "        # Visualize error patterns\n",
    "        visualize_error_patterns(error_df)\n",
    "    else:\n",
    "        print(\"Could not perform error analysis.\")\n",
    "else:\n",
    "    print(\"No raw predictions available for error analysis.\")## 3. Visualize ROC Curves and PR Curves\n",
    "\n",
    "def plot_roc_curves(raw_predictions, dataset_name=None):\n",
    "    \\\"\\\"\\\"Plot ROC curves for all models on a specific dataset\\\"\\\"\\\"\n",
    "    if raw_predictions is None or not raw_predictions:\n",
    "        print(\"No predictions to plot.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Keep track of datasets plotted\n",
    "    datasets_plotted = set()\n",
    "    \n",
    "    # Plot ROC curve for each model\n",
    "    for model_name, model_preds in raw_predictions.items():\n",
    "        # If dataset_name is specified, only plot for that dataset\n",
    "        if dataset_name:\n",
    "            if dataset_name in model_preds:\n",
    "                data = model_preds[dataset_name]\n",
    "                \n",
    "                # Check if we have the necessary data\n",
    "                if 'labels' in data and 'probabilities' in data and data['probabilities'] is not None:\n",
    "                    # Compute ROC curve\n",
    "                    fpr, tpr, _ = roc_curve(data['labels'], data['probabilities'])\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    \n",
    "                    # Plot ROC curve\n",
    "                    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "                    datasets_plotted.add(dataset_name)\n",
    "        else:\n",
    "            # Plot for all datasets\n",
    "            for ds_name, data in model_preds.items():\n",
    "                # Check if we have the necessary data\n",
    "                if 'labels' in data and 'probabilities' in data and data['probabilities'] is not None:\n",
    "                    # Compute ROC curve\n",
    "                    fpr, tpr, _ = roc_curve(data['labels'], data['probabilities'])\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    \n",
    "                    # Plot ROC curve\n",
    "                    plt.plot(fpr, tpr, lw=2, label=f'{model_name} - {ds_name} (AUC = {roc_auc:.3f})')\n",
    "                    datasets_plotted.add(ds_name)\n",
    "    \n",
    "    # Plot random classifier\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    \n",
    "    # Set title and labels\n",
    "    if dataset_name:\n",
    "        plt.title(f'ROC Curves - {dataset_name}' if dataset_name in datasets_plotted else 'ROC Curves')\n",
    "    else:\n",
    "        plt.title('ROC Curves')\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_pr_curves(raw_predictions, dataset_name=None):\n",
    "    \\\"\\\"\\\"Plot Precision-Recall curves for all models on a specific dataset\\\"\\\"\\\"\n",
    "    if raw_predictions is None or not raw_predictions:\n",
    "        print(\"No predictions to plot.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Keep track of datasets plotted\n",
    "    datasets_plotted = set()\n",
    "    \n",
    "    # Plot PR curve for each model\n",
    "    for model_name, model_preds in raw_predictions.items():\n",
    "        # If dataset_name is specified, only plot for that dataset\n",
    "        if dataset_name:\n",
    "            if dataset_name in model_preds:\n",
    "                data = model_preds[dataset_name]\n",
    "                \n",
    "                # Check if we have the necessary data\n",
    "                if 'labels' in data and 'probabilities' in data and data['probabilities'] is not None:\n",
    "                    # Compute PR curve\n",
    "                    precision, recall, _ = precision_recall_curve(data['labels'], data['probabilities'])\n",
    "                    pr_auc = auc(recall, precision)\n",
    "                    \n",
    "                    # Plot PR curve\n",
    "                    plt.plot(recall, precision, lw=2, label=f'{model_name} (AUC = {pr_auc:.3f})')\n",
    "                    datasets_plotted.add(dataset_name)\n",
    "        else:\n",
    "            # Plot for all datasets\n",
    "            for ds_name, data in model_preds.items():\n",
    "                # Check if we have the necessary data\n",
    "                if 'labels' in data and 'probabilities' in data and data['probabilities'] is not None:\n",
    "                    # Compute PR curve\n",
    "                    precision, recall, _ = precision_recall_curve(data['labels'], data['probabilities'])\n",
    "                    pr_auc = auc(recall, precision)\n",
    "                    \n",
    "                    # Plot PR curve\n",
    "                    plt.plot(recall, precision, lw=2, label=f'{model_name} - {ds_name} (AUC = {pr_auc:.3f})')\n",
    "                    datasets_plotted.add(ds_name)\n",
    "    \n",
    "    # Set title and labels\n",
    "    if dataset_name:\n",
    "        plt.title(f'Precision-Recall Curves - {dataset_name}' if dataset_name in datasets_plotted else 'Precision-Recall Curves')\n",
    "    else:\n",
    "        plt.title('Precision-Recall Curves')\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC and PR curves\n",
    "if 'raw_predictions' in locals() and raw_predictions:\n",
    "    # Get list of available datasets\n",
    "    available_datasets = set()\n",
    "    for model_preds in raw_predictions.values():\n",
    "        available_datasets.update(model_preds.keys())\n",
    "    \n",
    "    print(f\"Available datasets: {', '.join(available_datasets)}\")\n",
    "    \n",
    "    # Plot curves for each dataset\n",
    "    for dataset in available_datasets:\n",
    "        print(f\"\\nROC and PR curves for {dataset} dataset:\")\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        plot_roc_curves(raw_predictions, dataset)\n",
    "        \n",
    "        # Plot PR curve\n",
    "        plot_pr_curves(raw_predictions, dataset)\n",
    "else:\n",
    "    print(\"No raw predictions available for plotting curves.\")def rank_models(performance_df, metrics=None):\n",
    "    \\\"\\\"\\\"Rank models based on performance metrics\\\"\\\"\\\"\n",
    "    if performance_df is None or performance_df.empty:\n",
    "        print(\"No performance data to rank.\")\n",
    "        return\n",
    "    \n",
    "    if metrics is None:\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']\n",
    "        if 'eer' in performance_df.columns:\n",
    "            metrics.append('eer')\n",
    "    \n",
    "    # Ensure all metrics are in DataFrame\n",
    "    metrics = [m for m in metrics if m in performance_df.columns]\n",
    "    \n",
    "    if not metrics:\n",
    "        print(\"No valid metrics to rank.\")\n",
    "        return\n",
    "    \n",
    "    # Create a copy of DataFrame\n",
    "    df = performance_df.copy()\n",
    "    \n",
    "    # Create rank columns for each metric\n",
    "    for metric in metrics:\n",
    "        # For EER, lower is better\n",
    "        if metric == 'eer':\n",
    "            df[f'{metric}_rank'] = df.groupby('dataset')[metric].rank(ascending=True)\n",
    "        else:\n",
    "            df[f'{metric}_rank'] = df.groupby('dataset')[metric].rank(ascending=False)\n",
    "    \n",
    "    # Calculate average rank across metrics\n",
    "    rank_cols = [f'{metric}_rank' for metric in metrics]\n",
    "    df['avg_rank'] = df[rank_cols].mean(axis=1)\n",
    "    \n",
    "    # Sort by average rank\n",
    "    df = df.sort_values(['dataset', 'avg_rank'])\n",
    "    \n",
    "    # Select columns to display\n",
    "    display_cols = ['model', 'dataset'] + metrics + ['avg_rank']\n",
    "    \n",
    "    # Reset index for better display\n",
    "    df_display = df[display_cols].reset_index(drop=True)\n",
    "    \n",
    "    return df_display\n",
    "\n",
    "# Rank models\n",
    "if 'performance_df' in locals() and performance_df is not None and not performance_df.empty:\n",
    "    ranked_df = rank_models(performance_df)\n",
    "    \n",
    "    if ranked_df is not None:\n",
    "        print(\"\\nModel Rankings:\")\n",
    "        display(ranked_df)\n",
    "    else:\n",
    "        print(\"Could not rank models.\")\n",
    "else:\n",
    "    print(\"No performance data to rank.\")def visualize_performance_metrics(performance_df):\n",
    "    \\\"\\\"\\\"Visualize performance metrics across models and datasets\\\"\\\"\\\"\n",
    "    if performance_df is None or performance_df.empty:\n",
    "        print(\"No performance data to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Define metrics to plot\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']\n",
    "    if 'eer' in performance_df.columns:\n",
    "        metrics.append('eer')\n",
    "    \n",
    "    # Number of rows and columns\n",
    "    n_metrics = len(metrics)\n",
    "    n_rows = (n_metrics + 1) // 2\n",
    "    n_cols = 2\n",
    "    \n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(metrics):\n",
    "        if metric in performance_df.columns:\n",
    "            ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "            \n",
    "            # Create bar plot\n",
    "            sns.barplot(x='model', y=metric, hue='dataset', data=performance_df, ax=ax)\n",
    "            \n",
    "            # Set title and labels\n",
    "            ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel('Model')\n",
    "            ax.set_ylabel(metric)\n",
    "            \n",
    "            # Rotate x-labels for better readability\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            # Set y-axis limits\n",
    "            if metric != 'eer':  # For EER, lower is better\n",
    "                ax.set_ylim(0.5, 1.05)\n",
    "            else:\n",
    "                ax.set_ylim(0, 0.5)\n",
    "            \n",
    "            # Add legend\n",
    "            ax.legend(title='Dataset')\n",
    "    \n",
    "    # Add overall title\n",
    "    plt.suptitle('Model Performance Comparison', fontsize=16)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Visualize performance metrics\n",
    "if 'performance_df' in locals() and performance_df is not None and not performance_df.empty:\n",
    "    visualize_performance_metrics(performance_df)\n",
    "else:\n",
    "    print(\"No performance data to visualize.\")## 2. Analyze Model Performance Metrics\n",
    "\n",
    "def create_performance_summary(evaluation_results):\n",
    "    \\\"\\\"\\\"Create a summary table of model performance metrics\\\"\\\"\\\"\n",
    "    if not evaluation_results:\n",
    "        return None\n",
    "    \n",
    "    # Create a list to store performance data\n",
    "    performance_data = []\n",
    "    \n",
    "    for model_name, model_results in evaluation_results.items():\n",
    "        for dataset_name, dataset_results in model_results.items():\n",
    "            # Extract metrics\n",
    "            metrics = {\n",
    "                'model': model_name,\n",
    "                'dataset': dataset_name,\n",
    "                'accuracy': dataset_results.get('accuracy', None),\n",
    "                'precision': dataset_results.get('precision', None),\n",
    "                'recall': dataset_results.get('recall', None),\n",
    "                'f1_score': dataset_results.get('f1_score', None),\n",
    "                'auc': dataset_results.get('auc', None),\n",
    "                'eer': dataset_results.get('eer', None)\n",
    "            }\n",
    "            \n",
    "            performance_data.append(metrics)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if performance_data:\n",
    "        df = pd.DataFrame(performance_data)\n",
    "        \n",
    "        # Ensure all metric columns are float\n",
    "        for col in ['accuracy', 'precision', 'recall', 'f1_score', 'auc', 'eer']:\n",
    "            if col in df:\n",
    "                df[col] = df[col].astype(float)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Create performance summary\n",
    "if evaluation_results:\n",
    "    performance_df = create_performance_summary(evaluation_results)\n",
    "    \n",
    "    if performance_df is not None:\n",
    "        print(\"Performance Summary:\")\n",
    "        display(performance_df)\n",
    "    else:\n",
    "        print(\"Could not create performance summary.\")\n",
    "else:\n",
    "    print(\"No evaluation results to analyze.\"){\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Deepfake Detection Results Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides tools for analyzing the results of deepfake detection models. It includes:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Loading and visualizing evaluation metrics\\n\",\n",
    "    \"2. Comparative analysis of different models\\n\",\n",
    "    \"3. Error analysis and identification of challenging cases\\n\",\n",
    "    \"4. Cross-dataset generalization analysis\\n\",\n",
    "    \"5. ROC curves and precision-recall analysis\\n\",\n",
    "    \"6. Ensembling and fusion performance assessment\\n\",\n",
    "    \"\\n\",\n",
    "    \"These analyses help understand the strengths and weaknesses of different approaches and guide future improvements.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import necessary libraries\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import yaml\\n\",\n",
    "    \"import glob\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"from sklearn.metrics import roc_curve, precision_recall_curve, auc\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add parent directory to path to enable imports from project\\n\",\n",
    "    \"sys.path.append(os.path.abspath('..'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plot style\\n\",\n",
    "    \"plt.style.use('fivethirtyeight')\\n\",\n",
    "    \"sns.set(style=\\\"whitegrid\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Evaluation Results\\n\",\n",
    "    \"\\n\",\n",
    "    \"Load evaluation results from the evaluation output directory.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Set the evaluation results directory - update to your local path\\n\",\n",
    "    \"EVAL_DIR = \\\"../evaluation_results\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check if directory exists\\n\",\n",
    "    \"if not os.path.exists(EVAL_DIR):\\n\",\n",
    "    \"    print(f\\\"Evaluation directory {EVAL_DIR} does not exist. Please update the path.\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Found evaluation directory at {EVAL_DIR}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def load_evaluation_results(eval_dir):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load evaluation results from directory\\\"\\\"\\\"\\n\",\n",
    "    \"    if not os.path.exists(eval_dir):\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    results = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Find all model directories\\n\",\n",
    "    \"    model_dirs = [d for d in os.listdir(eval_dir) if os.path.isdir(os.path.join(eval_dir, d))]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for model_name in model_dirs:\\n\",\n",
    "    \"        model_path = os.path.join(eval_dir, model_name)\\n\",\n",
    "    \"        results_file = os.path.join(model_path, \\\"all_results.json\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if os.path.exists(results_file):\\n\",\n",
    "    \"            with open(results_file, 'r') as f:\\n\",\n",
    "    \"                model_results = json.load(f)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Store results\\n\",\n",
    "    \"            results[model_name] = model_results\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return results\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load results\\n\",\n",
    "    \"evaluation_results = load_evaluation_results(EVAL_DIR)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if evaluation_results is None:\\n\",\n",
    "    \"    print(\\\"No evaluation results found.\\\")\\n\",\n",
    "    \"elif not evaluation_results:\\n\",\n",
    "    \"    print(\\\"No models found in evaluation results.\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Loaded evaluation results for {len(evaluation_results)} models:\\\")\\n\",\n",
    "    \"    for model_name in evaluation_results.keys():\\n\",\n",
    "    \"        print(f\\\"  - {model_name}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def load_raw_predictions(eval_dir):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load raw predictions for detailed analysis\\\"\\\"\\\"\\n\",\n",
    "    \"    if not os.path.exists(eval_dir):\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    predictions = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Find all model directories\\n\",\n",
    "    \"    model_dirs = [d for d in os.listdir(eval_dir) if os.path.isdir(os.path.join(eval_dir, d))]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for model_name in model_dirs:\\n\",\n",
    "    \"        model_path = os.path.join(eval_dir, model_name)\\n\",\n",
    "    \"        model_predictions = {}\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Find dataset directories\\n\",\n",
    "    \"        dataset_dirs = [d for d in os.listdir(model_path) \\n\",\n",
    "    \"                       if os.path.isdir(os.path.join(model_path, d))]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for dataset_name in dataset_dirs:\\n\",\n",
    "    \"            dataset_path = os.path.join(model_path, dataset_name)\\n\",\n",
    "    \"            pred_file = os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff5f478",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 8. Summary and Recommendations\n",
    "\n",
    "def generate_summary_and_recommendations():\n",
    "    \\\"\\\"\\\"Generate a summary of findings and recommendations\\\"\\\"\\\"\n",
    "    # Summary\n",
    "    print(\"## Summary of Findings\\n\")\n",
    "    \n",
    "    # Check which analyses were performed\n",
    "    has_performance = 'performance_df' in globals() and performance_df is not None\n",
    "    has_ranking = 'ranked_df' in globals() and ranked_df is not None\n",
    "    has_errors = 'error_df' in globals() and error_df is not None\n",
    "    has_generalization = 'gen_df' in globals() and gen_df is not None\n",
    "    has_confidence = 'conf_df' in globals() and conf_df is not None\n",
    "    has_complementarity = 'comp_df' in globals() and comp_df is not None\n",
    "    \n",
    "    # 1. Overall Performance\n",
    "    print(\"### 1. Overall Performance\\n\")\n",
    "    if has_performance:\n",
    "        best_model = performance_df.loc[performance_df['accuracy'].idxmax()]\n",
    "        print(f\"- The best overall performing model is **{best_model['model']}** on the {best_model['dataset']} dataset, with an accuracy of {best_model['accuracy']:.4f}.\")\n",
    "        \n",
    "        # Dataset differences\n",
    "        datasets = performance_df['dataset'].unique()\n",
    "        if len(datasets) > 1:\n",
    "            print(\"- Performance across datasets:\")\n",
    "            for dataset in datasets:\n",
    "                dataset_avg = performance_df[performance_df['dataset'] == dataset]['accuracy'].mean()\n",
    "                print(f\"  - {dataset}: Average accuracy of {dataset_avg:.4f}\")\n",
    "    else:\n",
    "        print(\"- No performance data available.\")\n",
    "    \n",
    "    # 2. Model Ranking\n",
    "    print(\"\\n### 2. Model Ranking\\n\")\n",
    "    if has_ranking:\n",
    "        print(\"- Models ranked by overall performance:\")\n",
    "        for dataset in ranked_df['dataset'].unique():\n",
    "            dataset_ranks = ranked_df[ranked_df['dataset'] == dataset].sort_values('avg_rank')\n",
    "            print(f\"  - **{dataset}**: {', '.join(dataset_ranks['model'])}\")\n",
    "    else:\n",
    "        print(\"- No ranking data available.\")\n",
    "    \n",
    "    # 3. Error Analysis\n",
    "    print(\"\\n### 3. Error Analysis\\n\")\n",
    "    if has_errors:\n",
    "        # Find common error patterns\n",
    "        has_real_fake_imbalance = error_df['real_accuracy'].mean() - error_df['fake_accuracy'].mean()\n",
    "        \n",
    "        if abs(has_real_fake_imbalance) > 0.05:\n",
    "            if has_real_fake_imbalance > 0:\n",
    "                print(f\"- Models generally perform better on **real** samples (by {has_real_fake_imbalance:.4f} on average).\")\n",
    "            else:\n",
    "                print(f\"- Models generally perform better on **fake** samples (by {-has_real_fake_imbalance:.4f} on average).\")\n",
    "        else:\n",
    "            print(\"- Models show relatively balanced performance between real and fake samples.\")\n",
    "        \n",
    "        # Look for dataset-specific patterns\n",
    "        for dataset in error_df['dataset'].unique():\n",
    "            dataset_error = error_df[error_df['dataset'] == dataset]\n",
    "            real_fake_diff = dataset_error['real_accuracy'].mean() - dataset_error['fake_accuracy'].mean()\n",
    "            \n",
    "            if abs(real_fake_diff) > 0.1:\n",
    "                print(f\"  - On {dataset}, there's a significant imbalance: \" +\n",
    "                     f\"{'real' if real_fake_diff > 0 else 'fake'} samples are easier to classify \" +\n",
    "                     f\"(by {abs(real_fake_diff):.4f} accuracy difference).\")\n",
    "    else:\n",
    "        print(\"- No error analysis data available.\")\n",
    "    \n",
    "    # 4. Cross-Dataset Generalization\n",
    "    print(\"\\n### 4. Cross-Dataset Generalization\\n\")\n",
    "    if has_generalization:\n",
    "        # Sort models by generalization performance\n",
    "        model_drops = gen_df.groupby('model')['accuracy_drop'].mean().reset_index()\n",
    "        model_drops = model_drops.sort_values('accuracy_drop')\n",
    "        \n",
    "        print(f\"- **{model_drops.iloc[0]['model']}** shows the best cross-dataset generalization with an average accuracy drop of {model_drops.iloc[0]['accuracy_drop']:.4f}.\")\n",
    "        \n",
    "        # Look for challenging pairs\n",
    "        worst_pair = gen_df.sort_values('accuracy_drop', ascending=False).iloc[0]\n",
    "        print(f\"- The most challenging dataset transfer is from **{worst_pair['train_dataset']}** to **{worst_pair['test_dataset']}**, with an accuracy drop of {worst_pair['accuracy_drop']:.4f}.\")\n",
    "    else:\n",
    "        print(\"- No cross-dataset generalization data available.\")\n",
    "    \n",
    "    # 5. Confidence Analysis\n",
    "    print(\"\\n### 5. Confidence Analysis\\n\")\n",
    "    if has_confidence:\n",
    "        # Get models sorted by calibration error\n",
    "        calibration_errors = []\n",
    "        for model_name in conf_df['model'].unique():\n",
    "            model_data = conf_df[conf_df['model'] == model_name]\n",
    "            \n",
    "            # Calculate weighted average for each bin\n",
    "            weighted_data = model_data.groupby('bin_confidence').apply(\n",
    "                lambda x: pd.Series({\n",
    "                    'accuracy': np.average(x['bin_accuracy'], weights=x['bin_samples']),\n",
    "                    'samples': x['bin_samples'].sum()\n",
    "                })\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Calculate expected accuracy for each bin (assuming perfect calibration)\n",
    "            weighted_data['expected_accuracy'] = 0.5 + weighted_data['bin_confidence'] / 2\n",
    "            \n",
    "            # Calculate calibration error\n",
    "            error = np.average(\n",
    "                np.abs(weighted_data['accuracy'] - weighted_data['expected_accuracy']),\n",
    "                weights=weighted_data['samples']\n",
    "            )\n",
    "            \n",
    "            calibration_errors.append({\n",
    "                'model': model_name,\n",
    "                'calibration_error': error\n",
    "            })\n",
    "        \n",
    "        # Create DataFrame and sort\n",
    "        ce_df = pd.DataFrame(calibration_errors).sort_values('calibration_error')\n",
    "        \n",
    "        print(f\"- **{ce_df.iloc[0]['model']}** has the best calibration with an error of {ce_df.iloc[0]['calibration_error']:.4f}.\")\n",
    "        print(f\"- **{ce_df.iloc[-1]['model']}** has the worst calibration with an error of {ce_df.iloc[-1]['calibration_error']:.4f}.\")\n",
    "        \n",
    "        # Look for overall calibration trends\n",
    "        avg_error = ce_df['calibration_error'].mean()\n",
    "        if avg_error < 0.05:\n",
    "            print(\"- Models generally show good calibration between confidence and accuracy.\")\n",
    "        elif avg_error < 0.1:\n",
    "            print(\"- Models show moderate calibration issues between confidence and accuracy.\")\n",
    "        else:\n",
    "            print(\"- Models show significant calibration issues between confidence and accuracy.\")\n",
    "    else:\n",
    "        print(\"- No confidence analysis data available.\")\n",
    "    \n",
    "    # 6. Model Complementarity\n",
    "    print(\"\\n### 6. Model Complementarity\\n\")\n",
    "    if has_complementarity:\n",
    "        # Sort by complementarity\n",
    "        top_pair = comp_df.sort_values('complementarity', ascending=False).iloc[0]\n",
    "        print(f\"- The most complementary model pair is **{top_pair['model1']}** and **{top_pair['model2']}** on {top_pair['dataset']} dataset, with a complementarity score of {top_pair['complementarity']:.4f}.\")\n",
    "        \n",
    "        # Look for ensemble improvements\n",
    "        top_ensemble = comp_df.sort_values('ensemble_improvement', ascending=False).iloc[0]\n",
    "        print(f\"- The best ensemble improvement is achieved by combining **{top_ensemble['model1']}** and **{top_ensemble['model2']}** on {top_ensemble['dataset']} dataset, with an accuracy improvement of {top_ensemble['ensemble_improvement']:.4f}.\")\n",
    "        \n",
    "        # Oracle performance\n",
    "        avg_oracle = comp_df['oracle_acc'].mean()\n",
    "        avg_ensemble = comp_df['ensemble_acc'].mean()\n",
    "        print(f\"- On average, ensembles achieve {avg_ensemble:.4f} accuracy, while the theoretical maximum (oracle) is {avg_oracle:.4f}, showing room for improvement of {avg_oracle - avg_ensemble:.4f}.\")\n",
    "    else:\n",
    "        print(\"- No model complementarity data available.\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n## Recommendations\\n\")\n",
    "    \n",
    "    # 1. Model Selection\n",
    "    print(\"### 1. Model Selection\\n\")\n",
    "    if has_ranking:\n",
    "        best_model = ranked_df.sort_values('avg_rank').iloc[0]['model']\n",
    "        print(f\"- **Primary Model**: Use **{best_model}** as the primary model for most scenarios, as it shows the best overall performance.\")\n",
    "    \n",
    "    if has_generalization:\n",
    "        best_gen_model = model_drops.iloc[0]['model']\n",
    "        print(f\"- **Cross-Dataset Scenarios**: Use **{best_gen_model}** for scenarios where the test data may differ significantly from the training data, as it shows the best generalization capabilities.\")\n",
    "    \n",
    "    if has_confidence and 'ce_df' in locals():\n",
    "        best_calibrated = ce_df.iloc[0]['model']\n",
    "        print(f\"- **Confidence-Critical Applications**: Use **{best_calibrated}** for applications where accurate confidence estimation is crucial, as it shows the best calibration.\")\n",
    "    \n",
    "    # 2. Ensemble Strategy\n",
    "    print(\"\\n### 2. Ensemble Strategy\\n\")\n",
    "    if has_complementarity:\n",
    "        # Get top complementary pairs\n",
    "        top_pairs = comp_df.sort_values('complementarity', ascending=False).head(3)\n",
    "        \n",
    "        print(\"- **Recommended Ensembles**:\")\n",
    "        for i, row in top_pairs.iterrows():\n",
    "            print(f\"  - Combine **{row['model1']}** and **{row['model2']}** for {row['dataset']} dataset (complementarity: {row['complementarity']:.4f}, ensemble accuracy: {row['ensemble_acc']:.4f}).\")\n",
    "    else:\n",
    "        print(\"- No specific ensemble recommendations can be made without complementarity analysis.\")\n",
    "    \n",
    "    # 3. Future Improvements\n",
    "    print(\"\\n### 3. Future Improvements\\n\")\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    if has_errors:\n",
    "        # Check for class imbalance issues\n",
    "        has_real_fake_imbalance = abs(error_df['real_accuracy'].mean() - error_df['fake_accuracy'].mean()) > 0.05\n",
    "        if has_real_fake_imbalance:\n",
    "            weaker_class = \"real\" if error_df['real_accuracy'].mean() < error_df['fake_accuracy'].mean() else \"fake\"\n",
    "            recommendations.append(f\"- **Balanced Training**: Focus on improving performance on **{weaker_class}** samples, which are more challenging across models.\")\n",
    "    \n",
    "    if has_generalization:\n",
    "        if gen_df['accuracy_drop'].mean() > 0.05:\n",
    "            recommendations.append(\"- **Domain Adaptation**: Implement domain adaptation techniques to improve cross-dataset generalization.\")\n",
    "    \n",
    "    if has_confidence:\n",
    "        if 'ce_df' in locals() and ce_df['calibration_error'].mean() > 0.05:\n",
    "            recommendations.append(\"- **Calibration**: Apply post-hoc calibration methods (like Platt scaling or temperature scaling) to improve confidence calibration.\")\n",
    "    \n",
    "    if has_complementarity:\n",
    "        if comp_df['oracle_acc'].mean() - comp_df['ensemble_acc'].mean() > 0.05:\n",
    "            recommendations.append(\"- **Advanced Ensembling**: Explore more sophisticated ensemble methods (like stacking or feature-level fusion) to better leverage model complementarity.\")\n",
    "    \n",
    "    # If no specific recommendations, add general ones\n",
    "    if not recommendations:\n",
    "        recommendations.append(\"- **Data Augmentation**: Expand the training dataset with more diverse examples to improve robustness.\")\n",
    "        recommendations.append(\"- **Model Architecture**: Experiment with different model architectures or hybrids to capture different aspects of deepfakes.\")\n",
    "        recommendations.append(\"- **Feature Engineering**: Investigate what features are most predictive of deepfakes and enhance those aspects in the models.\")\n",
    "    \n",
    "    # Print all recommendations\n",
    "    for recommendation in recommendations:\n",
    "        print(recommendation)\n",
    "    \n",
    "    # 4. Deployment Considerations\n",
    "    print(\"\\n### 4. Deployment Considerations\\n\")\n",
    "    print(\"- **Threshold Tuning**: Adjust decision thresholds based on the specific application needs (higher precision vs. higher recall).\")\n",
    "    print(\"- **Monitoring**: Implement monitoring for detecting performance degradation as new deepfake techniques emerge.\")\n",
    "    print(\"- **Human Oversight**: For high-stakes decisions, maintain human oversight to verify model predictions, especially for cases with moderate confidence.\")\n",
    "    print(\"- **Explainability**: Include visualizations like attention maps or Grad-CAM to help users understand model decisions.\")\n",
    "\n",
    "# Generate summary and recommendations\n",
    "print(\"\\n\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"                      SUMMARY AND RECOMMENDATIONS                      \")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n\")\n",
    "\n",
    "generate_summary_and_recommendations()\n",
    "\n",
    "print(\"\\n\\nResults Analysis Notebook Complete!\")\n",
    "## 7. Model Complementarity Analysis\n",
    "\n",
    "def analyze_model_complementarity(raw_predictions):\n",
    "    \\\"\\\"\\\"Analyze how models complement each other\\\"\\\"\\\"\n",
    "    if raw_predictions is None or not raw_predictions:\n",
    "        print(\"No predictions to analyze.\")\n",
    "        return\n",
    "    \n",
    "    # Check if we have at least two models\n",
    "    if len(raw_predictions) < 2:\n",
    "        print(\"Need at least 2 models for complementarity analysis.\")\n",
    "        return\n",
    "    \n",
    "    # Create a list of (model_name, dataset_name) pairs\n",
    "    model_dataset_pairs = []\n",
    "    for model_name, model_preds in raw_predictions.items():\n",
    "        for dataset_name in model_preds.keys():\n",
    "            model_dataset_pairs.append((model_name, dataset_name))\n",
    "    \n",
    "    # Create a list to store complementarity data\n",
    "    complementarity_data = []\n",
    "    \n",
    "    # For each pair of models on the same dataset\n",
    "    for i, (model1, dataset1) in enumerate(model_dataset_pairs):\n",
    "        for j, (model2, dataset2) in enumerate(model_dataset_pairs[i+1:], i+1):\n",
    "            # Skip if datasets are different\n",
    "            if dataset1 != dataset2:\n",
    "                continue\n",
    "            \n",
    "            # Get predictions\n",
    "            data1 = raw_predictions[model1][dataset1]\n",
    "            data2 = raw_predictions[model2][dataset2]\n",
    "            \n",
    "            # Check if we have the necessary data\n",
    "            if ('labels' in data1 and 'predictions' in data1 and\n",
    "                'labels' in data2 and 'predictions' in data2):\n",
    "                \n",
    "                labels = data1['labels']\n",
    "                preds1 = data1['predictions']\n",
    "                preds2 = data2['predictions']\n",
    "                \n",
    "                # Calculate complementarity metrics\n",
    "                both_correct = np.sum((preds1 == labels) & (preds2 == labels))\n",
    "                model1_only = np.sum((preds1 == labels) & (preds2 != labels))\n",
    "                model2_only = np.sum((preds1 != labels) & (preds2 == labels))\n",
    "                both_wrong = np.sum((preds1 != labels) & (preds2 != labels))\n",
    "                \n",
    "                # Calculate total samples\n",
    "                total = len(labels)\n",
    "                \n",
    "                # Calculate individual accuracies\n",
    "                acc1 = np.sum(preds1 == labels) / total\n",
    "                acc2 = np.sum(preds2 == labels) / total\n",
    "                \n",
    "                # Calculate ensemble accuracy (majority voting)\n",
    "                ensemble_preds = (preds1 + preds2 > 0.5).astype(int)\n",
    "                ensemble_acc = np.sum(ensemble_preds == labels) / total\n",
    "                \n",
    "                # Calculate oracle accuracy (best possible combination)\n",
    "                oracle_preds = ((preds1 == labels) | (preds2 == labels)).astype(int)\n",
    "                oracle_acc = np.sum(oracle_preds == labels) / total\n",
    "                \n",
    "                # Store results\n",
    "                complementarity_data.append({\n",
    "                    'model1': model1,\n",
    "                    'model2': model2,\n",
    "                    'dataset': dataset1,\n",
    "                    'total_samples': total,\n",
    "                    'both_correct': both_correct,\n",
    "                    'model1_only': model1_only,\n",
    "                    'model2_only': model2_only,\n",
    "                    'both_wrong': both_wrong,\n",
    "                    'model1_acc': acc1,\n",
    "                    'model2_acc': acc2,\n",
    "                    'ensemble_acc': ensemble_acc,\n",
    "                    'oracle_acc': oracle_acc,\n",
    "                    'complementarity': (model1_only + model2_only) / total\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if complementarity_data:\n",
    "        comp_df = pd.DataFrame(complementarity_data)\n",
    "        return comp_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "def visualize_model_complementarity(comp_df):\n",
    "    \\\"\\\"\\\"Visualize model complementarity results\\\"\\\"\\\"\n",
    "    if comp_df is None or comp_df.empty:\n",
    "        print(\"No complementarity data to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Confusion matrix-like visualization of model agreements\n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    \n",
    "    # For each model pair\n",
    "    for i, row in comp_df.iterrows():\n",
    "        # Prepare data for confusion matrix\n",
    "        cm_data = np.array([\n",
    "            [row['both_correct'], row['model1_only']],\n",
    "            [row['model2_only'], row['both_wrong']]\n",
    "        ])\n",
    "        \n",
    "        # Normalize by total samples\n",
    "        cm_data = cm_data / row['total_samples']\n",
    "        \n",
    "        # Create a separate axis for each pair\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        sns.heatmap(cm_data, annot=True, fmt='.2%', cmap='Blues',\n",
    "                  xticklabels=['Correct', 'Wrong'], yticklabels=['Correct', 'Wrong'])\n",
    "        \n",
    "        plt.title(f\"{row['model1']} vs {row['model2']} - {row['dataset']}\")\n",
    "        plt.xlabel(f\"{row['model1']} Predictions\")\n",
    "        plt.ylabel(f\"{row['model2']} Predictions\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. Complementarity vs individual accuracy\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    \n",
    "    # Calculate average accuracy for each model pair\n",
    "    comp_df['avg_acc'] = (comp_df['model1_acc'] + comp_df['model2_acc']) / 2\n",
    "    \n",
    "    # Plot\n",
    "    sns.scatterplot(x='avg_acc', y='complementarity', hue='dataset', \n",
    "                   size='total_samples', sizes=(50, 200), data=comp_df, ax=ax2)\n",
    "    \n",
    "    ax2.set_title('Complementarity vs Average Accuracy')\n",
    "    ax2.set_xlabel('Average Accuracy')\n",
    "    ax2.set_ylabel('Complementarity')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 3. Accuracy improvement from ensemble\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    \n",
    "    # Calculate accuracy improvements\n",
    "    comp_df['ensemble_improvement'] = comp_df['ensemble_acc'] - comp_df['avg_acc']\n",
    "    comp_df['oracle_improvement'] = comp_df['oracle_acc'] - comp_df['avg_acc']\n",
    "    \n",
    "    # Sort by ensemble improvement\n",
    "    plot_df = comp_df.sort_values('ensemble_improvement', ascending=False)\n",
    "    \n",
    "    # Create labels for model pairs\n",
    "    plot_df['model_pair'] = plot_df.apply(lambda row: f\"{row['model1']}\\n+\\n{row['model2']}\", axis=1)\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='model_pair', y='ensemble_improvement', data=plot_df, ax=ax3)\n",
    "    \n",
    "    ax3.set_title('Accuracy Improvement from Ensemble')\n",
    "    ax3.set_xlabel('Model Pair')\n",
    "    ax3.set_ylabel('Accuracy Improvement')\n",
    "    ax3.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    # 4. Oracle vs ensemble performance\n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "    \n",
    "    # Melt the DataFrame\n",
    "    plot_df = pd.melt(comp_df, \n",
    "                      id_vars=['model1', 'model2', 'dataset'], \n",
    "                      value_vars=['model1_acc', 'model2_acc', 'ensemble_acc', 'oracle_acc'],\n",
    "                      var_name='Method', value_name='Accuracy')\n",
    "    \n",
    "    # Plot\n",
    "    sns.boxplot(x='Method', y='Accuracy', data=plot_df, ax=ax4)\n",
    "    \n",
    "    ax4.set_title('Performance Comparison')\n",
    "    ax4.set_xlabel('Method')\n",
    "    ax4.set_ylabel('Accuracy')\n",
    "    \n",
    "    # Add overall title\n",
    "    plt.suptitle('Model Complementarity Analysis', fontsize=16)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    # Return model pairs ranked by complementarity\n",
    "    return comp_df.sort_values('complementarity', ascending=False)\n",
    "\n",
    "# Analyze model complementarity\n",
    "if 'raw_predictions' in locals() and raw_predictions and len(raw_predictions) >= 2:\n",
    "    comp_df = analyze_model_complementarity(raw_predictions)\n",
    "    \n",
    "    if comp_df is not None:\n",
    "        print(\"\\nModel Complementarity Analysis:\")\n",
    "        display(comp_df)\n",
    "        \n",
    "        # Visualize complementarity\n",
    "        ranked_pairs = visualize_model_complementarity(comp_df)\n",
    "        if ranked_pairs is not None:\n",
    "            print(\"\\nModel Pairs Ranked by Complementarity (Higher is Better):\")\n",
    "            display(ranked_pairs[['model1', 'model2', 'dataset', 'complementarity', 'ensemble_acc', 'oracle_acc']])\n",
    "    else:\n",
    "        print(\"Could not perform complementarity analysis.\")\n",
    "else:\n",
    "    print(\"Need at least 2 models with predictions for complementarity analysis.\")## 6. Confidence Analysis\n",
    "\n",
    "def analyze_prediction_confidence(raw_predictions):\n",
    "    \\\"\\\"\\\"Analyze prediction confidence and its relationship with accuracy\\\"\\\"\\\"\n",
    "    if raw_predictions is None or not raw_predictions:\n",
    "        print(\"No predictions to analyze.\")\n",
    "        return\n",
    "    \n",
    "    # Create a list to store confidence data\n",
    "    confidence_data = []\n",
    "    \n",
    "    for model_name, model_preds in raw_predictions.items():\n",
    "        for dataset_name, data in model_preds.items():\n",
    "            # Check if we have the necessary data\n",
    "            if ('labels' in data and 'predictions' in data and \n",
    "                'probabilities' in data and data['probabilities'] is not None):\n",
    "                \n",
    "                labels = data['labels']\n",
    "                predictions = data['predictions']\n",
    "                probabilities = data['probabilities']\n",
    "                \n",
    "                # Ensure probabilities are for the positive class\n",
    "                if probabilities.ndim > 1 and probabilities.shape[1] > 1:\n",
    "                    probabilities = probabilities[:, 1]\n",
    "                \n",
    "                # Convert to confidence scores (distance from decision boundary)\n",
    "                confidence = np.abs(probabilities - 0.5) * 2  # Scale to [0, 1]\n",
    "                \n",
    "                # Bin confidence scores\n",
    "                n_bins = 10\n",
    "                bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "                bin_indices = np.digitize(confidence, bin_edges) - 1\n",
    "                \n",
    "                # Calculate accuracy for each bin\n",
    "                for bin_idx in range(n_bins):\n",
    "                    bin_mask = bin_indices == bin_idx\n",
    "                    if np.sum(bin_mask) > 0:\n",
    "                        bin_acc = np.mean(predictions[bin_mask] == labels[bin_mask])\n",
    "                        bin_conf = (bin_edges[bin_idx] + bin_edges[bin_idx + 1]) / 2\n",
    "                        \n",
    "                        confidence_data.append({\n",
    "                            'model': model_name,\n",
    "                            'dataset': dataset_name,\n",
    "                            'bin_confidence': bin_conf,\n",
    "                            'bin_accuracy': bin_acc,\n",
    "                            'bin_samples': np.sum(bin_mask)\n",
    "                        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if confidence_data:\n",
    "        conf_df = pd.DataFrame(confidence_data)\n",
    "        return conf_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "def visualize_confidence_analysis(conf_df):\n",
    "    \\\"\\\"\\\"Visualize confidence analysis results\\\"\\\"\\\"\n",
    "    if conf_df is None or conf_df.empty:\n",
    "        print(\"No confidence data to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # 1. Confidence vs. Accuracy\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    \n",
    "    # Plot confidence vs. accuracy for each model\n",
    "    for model_name in conf_df['model'].unique():\n",
    "        model_data = conf_df[conf_df['model'] == model_name]\n",
    "        \n",
    "        # Calculate weighted average for each bin\n",
    "        weighted_data = model_data.groupby('bin_confidence').apply(\n",
    "            lambda x: pd.Series({\n",
    "                'accuracy': np.average(x['bin_accuracy'], weights=x['bin_samples']),\n",
    "                'samples': x['bin_samples'].sum()\n",
    "            })\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Plot with point size proportional to number of samples\n",
    "        ax1.scatter(weighted_data['bin_confidence'], weighted_data['accuracy'], \n",
    "                   s=weighted_data['samples'] / 10, alpha=0.7, label=model_name)\n",
    "        \n",
    "        # Add trend line\n",
    "        ax1.plot(weighted_data['bin_confidence'], weighted_data['accuracy'], alpha=0.5)\n",
    "    \n",
    "    # Plot perfect calibration line\n",
    "    ax1.plot([0, 1], [0.5, 1], 'k--', alpha=0.5, label='Perfect Calibration')\n",
    "    \n",
    "    ax1.set_title('Confidence vs. Accuracy')\n",
    "    ax1.set_xlabel('Confidence')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0.5, 1.05)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Model Calibration Comparison\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    \n",
    "    # Calculate calibration error for each model\n",
    "    calibration_errors = []\n",
    "    \n",
    "    for model_name in conf_df['model'].unique():\n",
    "        model_data = conf_df[conf_df['model'] == model_name]\n",
    "        \n",
    "        # Calculate weighted average for each bin\n",
    "        weighted_data = model_data.groupby('bin_confidence').apply(\n",
    "            lambda x: pd.Series({\n",
    "                'accuracy': np.average(x['bin_accuracy'], weights=x['bin_samples']),\n",
    "                'samples': x['bin_samples'].sum()\n",
    "            })\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Calculate expected accuracy for each bin (assuming perfect calibration)\n",
    "        weighted_data['expected_accuracy'] = 0.5 + weighted_data['bin_confidence'] / 2\n",
    "        \n",
    "        # Calculate calibration error\n",
    "        error = np.average(\n",
    "            np.abs(weighted_data['accuracy'] - weighted_data['expected_accuracy']),\n",
    "            weights=weighted_data['samples']\n",
    "        )\n",
    "        \n",
    "        calibration_errors.append({\n",
    "            'model': model_name,\n",
    "            'calibration_error': error\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and plot\n",
    "    ce_df = pd.DataFrame(calibration_errors)\n",
    "    ce_df = ce_df.sort_values('calibration_error')\n",
    "    \n",
    "    sns.barplot(x='model', y='calibration_error', data=ce_df, ax=ax2)\n",
    "    ax2.set_title('Model Calibration Error (Lower is Better)')\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('Calibration Error')\n",
    "    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add overall title\n",
    "    plt.suptitle('Prediction Confidence Analysis', fontsize=16)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    return ce_df\n",
    "\n",
    "# Analyze prediction confidence\n",
    "if 'raw_predictions' in locals() and raw_predictions:\n",
    "    conf_df = analyze_prediction_confidence(raw_predictions)\n",
    "    \n",
    "    if conf_df is not None:\n",
    "        print(\"\\nConfidence Analysis:\")\n",
    "        display(conf_df)\n",
    "        \n",
    "        # Visualize confidence analysis\n",
    "        ce_df = visualize_confidence_analysis(conf_df)\n",
    "        if ce_df is not None:\n",
    "            print(\"\\nModels Ranked by Calibration Error (Lower is Better):\")\n",
    "            display(ce_df)\n",
    "    else:\n",
    "        print(\"Could not perform confidence analysis.\")\n",
    "else:\n",
    "    print(\"No raw predictions available for confidence analysis.\")## 5. Cross-Dataset Generalization Analysis\n",
    "\n",
    "def analyze_cross_dataset_generalization(error_df):\n",
    "    \\\"\\\"\\\"Analyze how well models generalize across datasets\\\"\\\"\\\"\n",
    "    if error_df is None or error_df.empty:\n",
    "        print(\"No error data to analyze.\")\n",
    "        return\n",
    "    \n",
    "    # Check if we have multiple datasets\n",
    "    if len(error_df['dataset'].unique()) < 2:\n",
    "        print(\"Need at least 2 datasets for cross-dataset analysis.\")\n",
    "        return\n",
    "    \n",
    "    # Create a list to store generalization data\n",
    "    generalization_data = []\n",
    "    \n",
    "    # For each model\n",
    "    for model in error_df['model'].unique():\n",
    "        model_data = error_df[error_df['model'] == model]\n",
    "        \n",
    "        # For each pair of datasets\n",
    "        for train_dataset in model_data['dataset'].unique():\n",
    "            train_perf = model_data[model_data['dataset'] == train_dataset]\n",
    "            \n",
    "            for test_dataset in model_data['dataset'].unique():\n",
    "                if train_dataset != test_dataset:\n",
    "                    test_perf = model_data[model_data['dataset'] == test_dataset]\n",
    "                    \n",
    "                    # Skip if missing data\n",
    "                    if train_perf.empty or test_perf.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate performance drops\n",
    "                    accuracy_drop = train_perf['accuracy'].values[0] - test_perf['accuracy'].values[0]\n",
    "                    real_acc_drop = train_perf['real_accuracy'].values[0] - test_perf['real_accuracy'].values[0]\n",
    "                    fake_acc_drop = train_perf['fake_accuracy'].values[0] - test_perf['fake_accuracy'].values[0]\n",
    "                    \n",
    "                    # Store results\n",
    "                    generalization_data.append({\n",
    "                        'model': model,\n",
    "                        'train_dataset': train_dataset,\n",
    "                        'test_dataset': test_dataset,\n",
    "                        'train_accuracy': train_perf['accuracy'].values[0],\n",
    "                        'test_accuracy': test_perf['accuracy'].values[0],\n",
    "                        'accuracy_drop': accuracy_drop,\n",
    "                        'real_acc_drop': real_acc_drop,\n",
    "                        'fake_acc_drop': fake_acc_drop\n",
    "                    })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if generalization_data:\n",
    "        gen_df = pd.DataFrame(generalization_data)\n",
    "        return gen_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "def visualize_cross_dataset_generalization(gen_df):\n",
    "    \\\"\\\"\\\"Visualize cross-dataset generalization results\\\"\\\"\\\"\n",
    "    if gen_df is None or gen_df.empty:\n",
    "        print(\"No generalization data to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Accuracy drop across datasets\n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='model', y='accuracy_drop', hue='test_dataset', data=gen_df, ax=ax1)\n",
    "    ax1.set_title('Accuracy Drop when Testing on Different Datasets')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Accuracy Drop')\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    ax1.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    # 2. Per-class accuracy drop\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    \n",
    "    # Melt the DataFrame\n",
    "    plot_df = pd.melt(gen_df, \n",
    "                      id_vars=['model', 'test_dataset'], \n",
    "                      value_vars=['real_acc_drop', 'fake_acc_drop'],\n",
    "                      var_name='Class', value_name='Accuracy Drop')\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='model', y='Accuracy Drop', hue='Class', data=plot_df, ax=ax2)\n",
    "    ax2.set_title('Per-class Accuracy Drop')\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('Accuracy Drop')\n",
    "    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    ax2.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    # 3. Train vs test accuracy scatter plot\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    \n",
    "    # Plot\n",
    "    sns.scatterplot(x='train_accuracy', y='test_accuracy', hue='model', \n",
    "                   style='test_dataset', s=100, data=gen_df, ax=ax3)\n",
    "    \n",
    "    # Plot diagonal line (x=y)\n",
    "    min_val = min(gen_df['train_accuracy'].min(), gen_df['test_accuracy'].min())\n",
    "    max_val = max(gen_df['train_accuracy'].max(), gen_df['test_accuracy'].max())\n",
    "    ax3.plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "    \n",
    "    ax3.set_title('Train vs Test Accuracy')\n",
    "    ax3.set_xlabel('Train Accuracy')\n",
    "    ax3.set_ylabel('Test Accuracy')\n",
    "    ax3.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 4. Generalization ranking\n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "    \n",
    "    # Calculate average accuracy drop for each model\n",
    "    model_drops = gen_df.groupby('model')['accuracy_drop'].mean().reset_index()\n",
    "    model_drops = model_drops.sort_values('accuracy_drop')\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='model', y='accuracy_drop', data=model_drops, ax=ax4)\n",
    "    ax4.set_title('Average Accuracy Drop (Lower is Better)')\n",
    "    ax4.set_xlabel('Model')\n",
    "    ax4.set_ylabel('Average Accuracy Drop')\n",
    "    ax4.set_xticklabels(ax4.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    ax4.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "    # Add overall title\n",
    "    plt.suptitle('Cross-Dataset Generalization Analysis', fontsize=16)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "    return model_drops\n",
    "\n",
    "# Analyze cross-dataset generalization\n",
    "if 'error_df' in locals() and error_df is not None and not error_df.empty:\n",
    "    gen_df = analyze_cross_dataset_generalization(error_df)\n",
    "    \n",
    "    if gen_df is not None:\n",
    "        print(\"\\nCross-Dataset Generalization Analysis:\")\n",
    "        display(gen_df)\n",
    "        \n",
    "        # Visualize generalization results\n",
    "        model_ranks = visualize_cross_dataset_generalization(gen_df)\n",
    "        if model_ranks is not None:\n",
    "            print(\"\\nModels Ranked by Generalization Performance (Lower Drop is Better):\")\n",
    "            display(model_ranks)\n",
    "    else:\n",
    "        print(\"Could not perform cross-dataset generalization analysis.\")\n",
    "else:\n",
    "    print(\"No error data available for cross-dataset generalization analysis.\")## 4. Error Analysis\n",
    "\n",
    "def analyze_errors(raw_predictions):\n",
    "    \\\"\\\"\\\"Analyze error patterns in model predictions\\\"\\\"\\\"\n",
    "    if raw_predictions is None or not raw_predictions:\n",
    "        print(\"No predictions to analyze.\")\n",
    "        return\n",
    "    \n",
    "    # Create a DataFrame to store error analysis results\n",
    "    error_analysis = []\n",
    "    \n",
    "    for model_name, model_preds in raw_predictions.items():\n",
    "        for dataset_name, data in model_preds.items():\n",
    "            # Check if we have the necessary data\n",
    "            if 'labels' in data and 'predictions' in data:\n",
    "                labels = data['labels']\n",
    "                predictions = data['predictions']\n",
    "                \n",
    "                # Calculate error statistics\n",
    "                total = len(labels)\n",
    "                correct = (predictions == labels).sum()\n",
    "                incorrect = total - correct\n",
    "                accuracy = correct / total\n",
    "                \n",
    "                # Per-class statistics\n",
    "                real_samples = (labels == 0).sum()\n",
    "                fake_samples = (labels == 1).sum()\n",
    "                \n",
    "                real_correct = ((predictions == 0) & (labels == 0)).sum()\n",
    "                fake_correct = ((predictions == 1) & (labels == 1)).sum()\n",
    "                \n",
    "                real_to_fake = ((predictions == 1) & (labels == 0)).sum()\n",
    "                fake_to_real = ((predictions == 0) & (labels == 1)).sum()\n",
    "                \n",
    "                real_accuracy = real_correct / real_samples if real_samples > 0 else 0\n",
    "                fake_accuracy = fake_correct / fake_samples if fake_samples > 0 else 0\n",
    "                \n",
    "                # Store results\n",
    "                error_analysis.append({\n",
    "                    'model': model_name,\n",
    "                    'dataset': dataset_name,\n",
    "                    'total_samples': total,\n",
    "                    'accuracy': accuracy,\n",
    "                    'correct': correct,\n",
    "                    'incorrect': incorrect,\n",
    "                    'real_samples': real_samples,\n",
    "                    'fake_samples': fake_samples,\n",
    "                    'real_accuracy': real_accuracy,\n",
    "                    'fake_accuracy': fake_accuracy,\n",
    "                    'real_to_fake': real_to_fake,\n",
    "                    'fake_to_real': fake_to_real\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if error_analysis:\n",
    "        error_df = pd.DataFrame(error_analysis)\n",
    "        return error_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "def visualize_error_patterns(error_df):\n",
    "    \\\"\\\"\\\"Visualize error patterns from error analysis\\\"\\\"\\\"\n",
    "    if error_df is None or error_df.empty:\n",
    "        print(\"No error data to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Overall accuracy vs per-class accuracy\n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    \n",
    "    # Melt the DataFrame to get it in the right format for seaborn\n",
    "    plot_df = pd.melt(error_df, \n",
    "                      id_vars=['model', 'dataset'], \n",
    "                      value_vars=['accuracy', 'real_accuracy', 'fake_accuracy'],\n",
    "                      var_name='Metric', value_name='Value')\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='model', y='Value', hue='Metric', data=plot_df, ax=ax1)\n",
    "    ax1.set_title('Overall vs Per-Class Accuracy')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 2. Error distribution\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    \n",
    "    # Melt the DataFrame\n",
    "    plot_df = pd.melt(error_df, \n",
    "                      id_vars=['model', 'dataset'], \n",
    "                      value_vars=['real_to_fake', 'fake_to_real'],\n",
    "                      var_name='Error Type', value_name='Count')\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='model', y='Count', hue='Error Type', data=plot_df, ax=ax2)\n",
    "    ax2.set_title('Error Distribution')\n",
    "    ax2.set_xlabel('Model')\n",
    "    ax2.set_ylabel('Error Count')\n",
    "    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # 3. Class balance\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    \n",
    "    # Calculate class balance\n",
    "    error_df['real_percent'] = error_df['real_samples'] / error_df['total_samples'] * 100\n",
    "    error_df['fake_percent'] = 100 - error_df['real_percent']\n",
    "    \n",
    "    # Melt the DataFrame\n",
    "    plot_df = pd.melt(error_df, \n",
    "                      id_vars=['dataset'], \n",
    "                      value_vars=['real_percent', 'fake_percent'],\n",
    "                      var_name='Class', value_name='Percentage')\n",
    "    \n",
    "    # Plot\n",
    "    sns.barplot(x='dataset', y='Percentage', hue='Class', data=plot_df, ax=ax3)\n",
    "    ax3.set_title('Class Balance')\n",
    "    ax3.set_xlabel('Dataset')\n",
    "    ax3.set_ylabel('Percentage')\n",
    "    \n",
    "    # 4. Error rate vs class balance\n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "    \n",
    "    # Calculate error rates\n",
    "    error_df['real_error_rate'] = error_df['real_to_fake'] / error_df['real_samples']\n",
    "    error_df['fake_error_rate'] = error_df['fake_to_real'] / error_df['fake_samples']\n",
    "    \n",
    "    # Melt the DataFrame\n",
    "    plot_df = pd.melt(error_df, \n",
    "                      id_vars=['model', 'dataset', 'real_percent', 'fake_percent'], \n",
    "                      value_vars=['real_error_rate', 'fake_error_rate'],\n",
    "                      var_name='Error Type', value_name='Error Rate')\n",
    "    \n",
    "    # Add class percentage information\n",
    "    plot_df['class_percent'] = plot_df.apply(\n",
    "        lambda row: row['real_percent'] if row['Error Type'] == 'real_error_rate' else row['fake_percent'], \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Plot\n",
    "    for model in plot_df['model'].unique():\n",
    "        model_df = plot_df[plot_df['model'] == model]\n",
    "        ax4.scatter(model_df['class_percent'], model_df['Error Rate'], label=model)\n",
    "    \n",
    "    ax4.set_title('Error Rate vs Class Percentage')\n",
    "    ax4.set_xlabel('Class Percentage')\n",
    "    ax4.set_ylabel('Error Rate')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add overall title\n",
    "    plt.suptitle('Error Analysis', fontsize=16)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Analyze errors\n",
    "if 'raw_predictions' in locals() and raw_predictions:\n",
    "    error_df = analyze_errors(raw_predictions)\n",
    "    \n",
    "    if error_df is not None:\n",
    "        print(\"\\nError Analysis:\")\n",
    "        display(error_df)\n",
    "        \n",
    "        # Visualize error patterns\n",
    "        visualize_error_patterns(error_df)\n",
    "    else:\n",
    "        print(\"Could not perform error analysis.\")\n",
    "else:\n",
    "    print(\"No raw predictions available for error analysis.\")## 3. Visualize ROC Curves and PR Curves\n",
    "\n",
    "def plot_roc_curves(raw_predictions, dataset_name=None):\n",
    "    \\\"\\\"\\\"Plot ROC curves for all models on a specific dataset\\\"\\\"\\\"\n",
    "    if raw_predictions is None or not raw_predictions:\n",
    "        print(\"No predictions to plot.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Keep track of datasets plotted\n",
    "    datasets_plotted = set()\n",
    "    \n",
    "    # Plot ROC curve for each model\n",
    "    for model_name, model_preds in raw_predictions.items():\n",
    "        # If dataset_name is specified, only plot for that dataset\n",
    "        if dataset_name:\n",
    "            if dataset_name in model_preds:\n",
    "                data = model_preds[dataset_name]\n",
    "                \n",
    "                # Check if we have the necessary data\n",
    "                if 'labels' in data and 'probabilities' in data and data['probabilities'] is not None:\n",
    "                    # Compute ROC curve\n",
    "                    fpr, tpr, _ = roc_curve(data['labels'], data['probabilities'])\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    \n",
    "                    # Plot ROC curve\n",
    "                    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "                    datasets_plotted.add(dataset_name)\n",
    "        else:\n",
    "            # Plot for all datasets\n",
    "            for ds_name, data in model_preds.items():\n",
    "                # Check if we have the necessary data\n",
    "                if 'labels' in data and 'probabilities' in data and data['probabilities'] is not None:\n",
    "                    # Compute ROC curve\n",
    "                    fpr, tpr, _ = roc_curve(data['labels'], data['probabilities'])\n",
    "                    roc_auc = auc(fpr, tpr)\n",
    "                    \n",
    "                    # Plot ROC curve\n",
    "                    plt.plot(fpr, tpr, lw=2, label=f'{model_name} - {ds_name} (AUC = {roc_auc:.3f})')\n",
    "                    datasets_plotted.add(ds_name)\n",
    "    \n",
    "    # Plot random classifier\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    \n",
    "    # Set title and labels\n",
    "    if dataset_name:\n",
    "        plt.title(f'ROC Curves - {dataset_name}' if dataset_name in datasets_plotted else 'ROC Curves')\n",
    "    else:\n",
    "        plt.title('ROC Curves')\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_pr_curves(raw_predictions, dataset_name=None):\n",
    "    \\\"\\\"\\\"Plot Precision-Recall curves for all models on a specific dataset\\\"\\\"\\\"\n",
    "    if raw_predictions is None or not raw_predictions:\n",
    "        print(\"No predictions to plot.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Keep track of datasets plotted\n",
    "    datasets_plotted = set()\n",
    "    \n",
    "    # Plot PR curve for each model\n",
    "    for model_name, model_preds in raw_predictions.items():\n",
    "        # If dataset_name is specified, only plot for that dataset\n",
    "        if dataset_name:\n",
    "            if dataset_name in model_preds:\n",
    "                data = model_preds[dataset_name]\n",
    "                \n",
    "                # Check if we have the necessary data\n",
    "                if 'labels' in data and 'probabilities' in data and data['probabilities'] is not None:\n",
    "                    # Compute PR curve\n",
    "                    precision, recall, _ = precision_recall_curve(data['labels'], data['probabilities'])\n",
    "                    pr_auc = auc(recall, precision)\n",
    "                    \n",
    "                    # Plot PR curve\n",
    "                    plt.plot(recall, precision, lw=2, label=f'{model_name} (AUC = {pr_auc:.3f})')\n",
    "                    datasets_plotted.add(dataset_name)\n",
    "        else:\n",
    "            # Plot for all datasets\n",
    "            for ds_name, data in model_preds.items():\n",
    "                # Check if we have the necessary data\n",
    "                if 'labels' in data and 'probabilities' in data and data['probabilities'] is not None:\n",
    "                    # Compute PR curve\n",
    "                    precision, recall, _ = precision_recall_curve(data['labels'], data['probabilities'])\n",
    "                    pr_auc = auc(recall, precision)\n",
    "                    \n",
    "                    # Plot PR curve\n",
    "                    plt.plot(recall, precision, lw=2, label=f'{model_name} - {ds_name} (AUC = {pr_auc:.3f})')\n",
    "                    datasets_plotted.add(ds_name)\n",
    "    \n",
    "    # Set title and labels\n",
    "    if dataset_name:\n",
    "        plt.title(f'Precision-Recall Curves - {dataset_name}' if dataset_name in datasets_plotted else 'Precision-Recall Curves')\n",
    "    else:\n",
    "        plt.title('Precision-Recall Curves')\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC and PR curves\n",
    "if 'raw_predictions' in locals() and raw_predictions:\n",
    "    # Get list of available datasets\n",
    "    available_datasets = set()\n",
    "    for model_preds in raw_predictions.values():\n",
    "        available_datasets.update(model_preds.keys())\n",
    "    \n",
    "    print(f\"Available datasets: {', '.join(available_datasets)}\")\n",
    "    \n",
    "    # Plot curves for each dataset\n",
    "    for dataset in available_datasets:\n",
    "        print(f\"\\nROC and PR curves for {dataset} dataset:\")\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        plot_roc_curves(raw_predictions, dataset)\n",
    "        \n",
    "        # Plot PR curve\n",
    "        plot_pr_curves(raw_predictions, dataset)\n",
    "else:\n",
    "    print(\"No raw predictions available for plotting curves.\")def rank_models(performance_df, metrics=None):\n",
    "    \\\"\\\"\\\"Rank models based on performance metrics\\\"\\\"\\\"\n",
    "    if performance_df is None or performance_df.empty:\n",
    "        print(\"No performance data to rank.\")\n",
    "        return\n",
    "    \n",
    "    if metrics is None:\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']\n",
    "        if 'eer' in performance_df.columns:\n",
    "            metrics.append('eer')\n",
    "    \n",
    "    # Ensure all metrics are in DataFrame\n",
    "    metrics = [m for m in metrics if m in performance_df.columns]\n",
    "    \n",
    "    if not metrics:\n",
    "        print(\"No valid metrics to rank.\")\n",
    "        return\n",
    "    \n",
    "    # Create a copy of DataFrame\n",
    "    df = performance_df.copy()\n",
    "    \n",
    "    # Create rank columns for each metric\n",
    "    for metric in metrics:\n",
    "        # For EER, lower is better\n",
    "        if metric == 'eer':\n",
    "            df[f'{metric}_rank'] = df.groupby('dataset')[metric].rank(ascending=True)\n",
    "        else:\n",
    "            df[f'{metric}_rank'] = df.groupby('dataset')[metric].rank(ascending=False)\n",
    "    \n",
    "    # Calculate average rank across metrics\n",
    "    rank_cols = [f'{metric}_rank' for metric in metrics]\n",
    "    df['avg_rank'] = df[rank_cols].mean(axis=1)\n",
    "    \n",
    "    # Sort by average rank\n",
    "    df = df.sort_values(['dataset', 'avg_rank'])\n",
    "    \n",
    "    # Select columns to display\n",
    "    display_cols = ['model', 'dataset'] + metrics + ['avg_rank']\n",
    "    \n",
    "    # Reset index for better display\n",
    "    df_display = df[display_cols].reset_index(drop=True)\n",
    "    \n",
    "    return df_display\n",
    "\n",
    "# Rank models\n",
    "if 'performance_df' in locals() and performance_df is not None and not performance_df.empty:\n",
    "    ranked_df = rank_models(performance_df)\n",
    "    \n",
    "    if ranked_df is not None:\n",
    "        print(\"\\nModel Rankings:\")\n",
    "        display(ranked_df)\n",
    "    else:\n",
    "        print(\"Could not rank models.\")\n",
    "else:\n",
    "    print(\"No performance data to rank.\")def visualize_performance_metrics(performance_df):\n",
    "    \\\"\\\"\\\"Visualize performance metrics across models and datasets\\\"\\\"\\\"\n",
    "    if performance_df is None or performance_df.empty:\n",
    "        print(\"No performance data to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Set up the figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Define metrics to plot\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']\n",
    "    if 'eer' in performance_df.columns:\n",
    "        metrics.append('eer')\n",
    "    \n",
    "    # Number of rows and columns\n",
    "    n_metrics = len(metrics)\n",
    "    n_rows = (n_metrics + 1) // 2\n",
    "    n_cols = 2\n",
    "    \n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(metrics):\n",
    "        if metric in performance_df.columns:\n",
    "            ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "            \n",
    "            # Create bar plot\n",
    "            sns.barplot(x='model', y=metric, hue='dataset', data=performance_df, ax=ax)\n",
    "            \n",
    "            # Set title and labels\n",
    "            ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "            ax.set_xlabel('Model')\n",
    "            ax.set_ylabel(metric)\n",
    "            \n",
    "            # Rotate x-labels for better readability\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "            \n",
    "            # Set y-axis limits\n",
    "            if metric != 'eer':  # For EER, lower is better\n",
    "                ax.set_ylim(0.5, 1.05)\n",
    "            else:\n",
    "                ax.set_ylim(0, 0.5)\n",
    "            \n",
    "            # Add legend\n",
    "            ax.legend(title='Dataset')\n",
    "    \n",
    "    # Add overall title\n",
    "    plt.suptitle('Model Performance Comparison', fontsize=16)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Visualize performance metrics\n",
    "if 'performance_df' in locals() and performance_df is not None and not performance_df.empty:\n",
    "    visualize_performance_metrics(performance_df)\n",
    "else:\n",
    "    print(\"No performance data to visualize.\")## 2. Analyze Model Performance Metrics\n",
    "\n",
    "def create_performance_summary(evaluation_results):\n",
    "    \\\"\\\"\\\"Create a summary table of model performance metrics\\\"\\\"\\\"\n",
    "    if not evaluation_results:\n",
    "        return None\n",
    "    \n",
    "    # Create a list to store performance data\n",
    "    performance_data = []\n",
    "    \n",
    "    for model_name, model_results in evaluation_results.items():\n",
    "        for dataset_name, dataset_results in model_results.items():\n",
    "            # Extract metrics\n",
    "            metrics = {\n",
    "                'model': model_name,\n",
    "                'dataset': dataset_name,\n",
    "                'accuracy': dataset_results.get('accuracy', None),\n",
    "                'precision': dataset_results.get('precision', None),\n",
    "                'recall': dataset_results.get('recall', None),\n",
    "                'f1_score': dataset_results.get('f1_score', None),\n",
    "                'auc': dataset_results.get('auc', None),\n",
    "                'eer': dataset_results.get('eer', None)\n",
    "            }\n",
    "            \n",
    "            performance_data.append(metrics)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if performance_data:\n",
    "        df = pd.DataFrame(performance_data)\n",
    "        \n",
    "        # Ensure all metric columns are float\n",
    "        for col in ['accuracy', 'precision', 'recall', 'f1_score', 'auc', 'eer']:\n",
    "            if col in df:\n",
    "                df[col] = df[col].astype(float)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Create performance summary\n",
    "if evaluation_results:\n",
    "    performance_df = create_performance_summary(evaluation_results)\n",
    "    \n",
    "    if performance_df is not None:\n",
    "        print(\"Performance Summary:\")\n",
    "        display(performance_df)\n",
    "    else:\n",
    "        print(\"Could not create performance summary.\")\n",
    "else:\n",
    "    print(\"No evaluation results to analyze.\"){\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Deepfake Detection Results Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook provides tools for analyzing the results of deepfake detection models. It includes:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Loading and visualizing evaluation metrics\\n\",\n",
    "    \"2. Comparative analysis of different models\\n\",\n",
    "    \"3. Error analysis and identification of challenging cases\\n\",\n",
    "    \"4. Cross-dataset generalization analysis\\n\",\n",
    "    \"5. ROC curves and precision-recall analysis\\n\",\n",
    "    \"6. Ensembling and fusion performance assessment\\n\",\n",
    "    \"\\n\",\n",
    "    \"These analyses help understand the strengths and weaknesses of different approaches and guide future improvements.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import necessary libraries\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"import yaml\\n\",\n",
    "    \"import glob\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"from sklearn.metrics import roc_curve, precision_recall_curve, auc\\n\",\n",
    "    \"from tqdm.notebook import tqdm\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add parent directory to path to enable imports from project\\n\",\n",
    "    \"sys.path.append(os.path.abspath('..'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plot style\\n\",\n",
    "    \"plt.style.use('fivethirtyeight')\\n\",\n",
    "    \"sns.set(style=\\\"whitegrid\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Evaluation Results\\n\",\n",
    "    \"\\n\",\n",
    "    \"Load evaluation results from the evaluation output directory.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Set the evaluation results directory - update to your local path\\n\",\n",
    "    \"EVAL_DIR = \\\"../evaluation_results\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check if directory exists\\n\",\n",
    "    \"if not os.path.exists(EVAL_DIR):\\n\",\n",
    "    \"    print(f\\\"Evaluation directory {EVAL_DIR} does not exist. Please update the path.\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Found evaluation directory at {EVAL_DIR}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def load_evaluation_results(eval_dir):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load evaluation results from directory\\\"\\\"\\\"\\n\",\n",
    "    \"    if not os.path.exists(eval_dir):\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    results = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Find all model directories\\n\",\n",
    "    \"    model_dirs = [d for d in os.listdir(eval_dir) if os.path.isdir(os.path.join(eval_dir, d))]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for model_name in model_dirs:\\n\",\n",
    "    \"        model_path = os.path.join(eval_dir, model_name)\\n\",\n",
    "    \"        results_file = os.path.join(model_path, \\\"all_results.json\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if os.path.exists(results_file):\\n\",\n",
    "    \"            with open(results_file, 'r') as f:\\n\",\n",
    "    \"                model_results = json.load(f)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Store results\\n\",\n",
    "    \"            results[model_name] = model_results\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return results\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load results\\n\",\n",
    "    \"evaluation_results = load_evaluation_results(EVAL_DIR)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if evaluation_results is None:\\n\",\n",
    "    \"    print(\\\"No evaluation results found.\\\")\\n\",\n",
    "    \"elif not evaluation_results:\\n\",\n",
    "    \"    print(\\\"No models found in evaluation results.\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Loaded evaluation results for {len(evaluation_results)} models:\\\")\\n\",\n",
    "    \"    for model_name in evaluation_results.keys():\\n\",\n",
    "    \"        print(f\\\"  - {model_name}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"def load_raw_predictions(eval_dir):\\n\",\n",
    "    \"    \\\"\\\"\\\"Load raw predictions for detailed analysis\\\"\\\"\\\"\\n\",\n",
    "    \"    if not os.path.exists(eval_dir):\\n\",\n",
    "    \"        return None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    predictions = {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Find all model directories\\n\",\n",
    "    \"    model_dirs = [d for d in os.listdir(eval_dir) if os.path.isdir(os.path.join(eval_dir, d))]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for model_name in model_dirs:\\n\",\n",
    "    \"        model_path = os.path.join(eval_dir, model_name)\\n\",\n",
    "    \"        model_predictions = {}\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Find dataset directories\\n\",\n",
    "    \"        dataset_dirs = [d for d in os.listdir(model_path) \\n\",\n",
    "    \"                       if os.path.isdir(os.path.join(model_path, d))]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for dataset_name in dataset_dirs:\\n\",\n",
    "    \"            dataset_path = os.path.join(model_path, dataset_name)\\n\",\n",
    "    \"            pred_file = os"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
